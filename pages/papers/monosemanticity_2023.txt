Transformer Circuits Thread
Towards Monosemanticity: Decomposing Language Models With Dictionary Learning
Using a sparse autoencoder, we extract a large number of interpretable features from a one-layer transformer.
Browse A/1 Features →
Browse All Features →

Authors
Trenton Bricken*, Adly Templeton*, Joshua Batson*, Brian Chen*, Adam Jermyn*, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah
Affiliations
Anthropic
Published
Oct 4, 2023
* Core Contributor; Correspondence to colah@anthropic.com; Author contributions statement below.
Contents
Problem Setup
Features as a Decomposition
What makes a good decomposition?
Why not use architectural approaches?
Using Sparse Autoencoders To Find Good Decompositions
Sparse Autoencoder Setup
The (one-layer) model we're studying
Notation for Features
Detailed Investigations of Individual Features
Arabic Script Feature
DNA Feature
base64 Feature
Hebrew Feature
Global Analysis
How Interpretable is the Typical Feature?
How much of the model does our interpretation explain?
Do features tell us about the model or the data?
Phenomenology
Feature Motifs
Feature Splitting
Universality
"Finite State Automata"
Related Work
Discussion
Comments & Replications
Neel Nanda
Author Contributions Statement
Acknowledgments
Citation Information
Feature Proxies
Feature Interpretability Rubric
Feature Density Histograms
Transformer Training and Preprocessing
Advice for Training Sparse Autoencoders
Automated Interpretability

Mechanistic interpretability seeks to understand neural networks by breaking them into components that are more easily understood than the whole. By understanding the function of each component, and how they interact, we hope to be able to reason about the behavior of the entire network. The first step in that program is to identify the correct components to analyze.

Unfortunately, the most natural computational unit of the neural network – the neuron itself – turns out not to be a natural unit for human understanding. This is because many neurons are polysemantic: they respond to mixtures of seemingly unrelated inputs. In the vision model Inception v1, a single neuron responds to faces of cats and fronts of cars 
[1]
. In a small language model we discuss in this paper, a single neuron responds to a mixture of academic citations, English dialogue, HTTP requests, and Korean text. Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons.

One potential cause of polysemanticity is superposition 
[2, 3, 4, 5]
, a hypothesized phenomenon where a neural network represents more independent "features" of the data than it has neurons by assigning each feature its own linear combination of neurons. If we view each feature as a vector over the neurons, then the set of features form an overcomplete linear basis for the activations of the network neurons. In our previous paper on Toy Models of Superposition 
[5]
, we showed that superposition can arise naturally during the course of neural network training if the set of features useful to a model are sparse in the training data. As in compressed sensing, sparsity allows a model to disambiguate which combination of features produced any given activation vector. 1

In Toy Models of Superposition, we described three strategies to finding a sparse and interpretable set of features if they are indeed hidden by superposition: (1) creating models without superposition, perhaps by encouraging activation sparsity; (2) using dictionary learning to find an overcomplete feature basis in a model exhibiting superposition; and (3) hybrid approaches relying on a combination of the two. Since the publication of that work, we've explored all three approaches. We eventually developed counterexamples which persuaded us that the sparse architectural approach (approach 1) was insufficient to prevent polysemanticity, and that standard dictionary learning methods (approach 2) had significant issues with overfitting.

In this paper, we use a weak dictionary learning algorithm called a sparse autoencoder to generate learned features from a trained model that offer a more monosemantic unit of analysis than the model's neurons themselves. Our approach here builds on a significant amount of prior work, especially in using dictionary learning and related methods on neural network activations (e.g. 
[6, 2, 7, 8, 9, 10]
), and a more general allied literature on disentanglement. We also note interim reports 
[11, 12, 13, 14, 15, 16]
 which independently investigated the sparse autoencoder approach in response to Toy Models, culminating in the recent manuscript of Cunningham et al. 
[17]
.

The goal of this paper is to provide a detailed demonstration of a sparse autoencoder compellingly succeeding at the goals of extracting interpretable features from superposition and enabling basic circuit analysis. Concretely, we take a one-layer transformer with a 512-neuron MLP layer, and decompose the MLP activations into relatively interpretable features by training sparse autoencoders on MLP activations from 8 billion data points, with expansion factors ranging from 1× (512 features) to 256× (131,072 features). We focus our detailed interpretability analyses on the 4,096 features learned in one run we call A/1.

This report has four major sections. In Problem Setup, we provide motivation for our approach and describe the transformers and sparse autoencoders we train. In Detailed Investigations of Individual Features, we offer an existence proof – we make the case that several features we find are functionally specific causal units which don't correspond to neurons. In Global Analysis, we argue that the typical feature is interpretable and that they explain a non-trivial portion of the MLP layer. Finally, in Phenomenology we describe several properties of our features, including feature-splitting, universality, and how they can form "finite state automata"-like systems implementing interesting behaviors.

We also provide three comprehensive visualizations of features. First, for all features from 90 learned dictionaries we present activating dataset examples and downstream logit effects. We recommend the reader begin with the visualization of A/1. Second, we provide a data-oriented view, showing all features active on each token of 25 texts. Finally, we coembed all 4,096 features from A/1 and all 512 features from A/0 into the plane using UMAP to allow for interactive exploration of the space of features:

Cluster
Feature
search labels
Cluster #49
A/0/307
This feature fires for references to citations in scientific papers. It attends to the formatting of citations such as brackets, @ symbols, names of authors, dates and labels.
A/0/311
This feature fires for reference citations in academic papers, specifically when it sees the [@ symbol that is commonly used.
A/1/776
Years in some citation notation
A/1/1538
Citations in a [@author] or [@authoryear] format
A/1/1875
Markdown Citation (Predict year)
A/1/2252
" [@"
A/1/2237
[Ultralow density cluster]
Cluster #42
A/0/126
This feature seems to fire on section headings, specifically the word "sec" within Markdown section tags like {#sec:foo}.
A/1/357
"ref" in [context]
A/1/1469
"s"/"sec" after "{#", section reference in some markup
A/1/3841
"Sec"
A/1/3898
Section number in {#SecX}
A/1/4083
" {#"
A/1/2129
"." in [context]
A/1/553
"](#" in [context]
Cluster #43
A/0/8
This feature attends to text formatting markups such as references, figure captions, and table captions. It looks specifically for the "ref-type" attribute in markup tags.
A/0/398
This feature attends to references to figures and tables.
A/0/454
This feature fires on reference/bibliographic citations in LaTeX documents. It attends to the braces, "ref" tag, and "type" attribute that are commonly used to define citations.
A/1/35
"){"
A/1/366
"type"
A/1/945
"ref" in [context]
A/1/1895
"-" in [context]
A/1/2176
"fig"
Cluster #46
A/0/425
This feature attends to highlighting bibliography and reference markers in the text such as [1], (Table 1), [@B23], etc.
A/1/703
This feature attends to bibliographic citations with brackets and @CIT numbers.
A/1/864
" \\[[@"
A/1/868
"[@" in bibliography citations
A/1/2217
".]("
A/1/2777
" ([@" in [context]
A/1/3120
This feature attends to bibliographic citations or references, firing most strongly for formatted citations with brackets, citation keys like [b23] and full references citing authors, titles and dates.
A/1/3184
Introducing an acaemic reference format
A/1/3698
"](#" in [context]
A/1/51
[Ultralow density cluster]
Cluster #48
A/0/363
The feature attends to in-text citations with URL and DOI links.
A/0/366
This feature fires on citations and reference markers such as [ ], ( ), etc.
A/0/396
This feature fires for in-text references, citation links, and reference brackets/notations in scholarly articles, research papers, and academic journals.
A/1/90
"1"
A/1/204
"CR"
A/1/220
This feature attends to formats of references and citations.
A/1/729
Digits in bibliographies?
A/1/1015
Reference IDs in parentheses in [](){} citation format
A/1/1264
Mostly letter fragments of IDs in parentheses in [](){} citation format
A/1/1316
A specific form of medical journal document reference, syntax '![](journalname12345-1234){...', specifically the journalid12345-1234 part
A/1/1440
"g" in [context]
A/1/1445
"ijms"
A/1/2200
The feature fires on citation indices in scientific papers, such as [B13-ijms-20-04353-f001].
A/1/2280
"bib" in [context]
A/1/2385
"bib" in some bibliography format ending in "bib-XXXX" where X are digits
A/1/2496
"B" in [context]
A/1/2559
[Ultralow density cluster]
A/1/2696
Numbers/digits in hyphenated IDs in square brackets, likely [@ ... ] citation format
A/1/2881
"B" in [context]
A/1/2988
Digits after upper case letters
A/1/2996
This feature attends to inline citations like [R9] or [@ref9] in scientific papers.
A/1/3143
This feature seems to fire on citations and references in brackets and parentheses with numbers, IDs and abbreviations.
A/1/3163
[Ultralow density cluster]
A/1/3164
Numbers after "pone.", for citation IDs
A/1/3547
Beginnings of hash locations in Markdown URL targets?
A/1/3949
"pone"
A/1/4062
Post-"#" numeric ID fragments in parentheses in [](){} citation format
A/0/270
This feature fires on reference links to articles, papers, and datasets in academic writing. These references links are typically in a specific format like [text](link) or with abbreviated journal titles and citation numbers like [B14-sensors-19-00441].
Cluster #36
A/0/277
The feature fires when it sees short HTML/XML tags such as <ref-type="fig">
A/0/427
This feature appears to fire on hyphens between numbers, especially years.
A/1/367
"=\"" in [context]
A/1/747
"-" in [context]
A/1/1220
"-" in [context]
A/1/1782
"-" in [context]
A/1/2324
"=\"" in [context]
A/1/2876
"-" in [context]
A/1/3122
"-" for dashes within journal citations?
A/1/774
[Ultralow density cluster]
A/1/3838
" @" in [context]
A/1/1415
"),", sometimes with more leading punctuation, in scientific contexts
A/1/3877
"," in [context]
Cluster #124
A/1/1147
"," in [context]
A/1/2930
"," in [context]
A/1/3857
"," in [context]
Cluster #127
A/0/359
This feature fires when it sees the punctuation mark comma ","
A/1/178
"," in mechanical engineering?
A/1/713
"," after "However" or "by contrast" type phrases
A/1/1204
This feature attends to commas used as enumerators in scientific texts.
A/1/1374
"," in lists of chemicals
A/1/1565
"],"
A/1/1801
"," between demographic factors
A/1/2284
"," in [context]
A/1/2835
"," in [context]
A/1/3482
";" in [context]
A/1/3921
";" in [context]
Cluster #37
A/0/289
This feature attends to parentheses and other bracket notations.
A/1/1785
This feature attends to closing parentheses.
A/1/1885
")" in biomedical studies
A/1/3672
. The "]" at the end of each reference code
A/1/715
"," after an adverb indicating a conclusion from something previous, e.g. "Finally"
A/0/231
The feature attends to the word "in" followed by a comma and appearing at the beginning of a sentence.
Cluster #142
A/0/430
This feature fires when a comma is present.
A/1/1284
"," in [context]
A/1/1479
"," in [context]
A/1/2409
"," in [context]
A/1/3342
"," in lists, mostly of short nouns/phrases
A/1/3411
"," in [context]
Cluster #10
A/0/128
This feature appears to attend to proper nouns and named entities in text, especially names of people, organizations, locations, and creative works.
A/1/626
[Ultralow density cluster]
A/1/1366
Sports, esp. Title Case names of competitions
A/1/1860
[Ultralow density cluster]
A/1/2731
" the" in sports (football?)
A/1/2784
American sports concepts: offensive/defensive lines and defensive/tackle, right-handed bat, three-pointer, starter, drills
A/1/3316
Professional sports
A/1/3336
1- to 2-digit numbers in sports contexts, esp. scores
A/1/3856
" with" in sports contexts
A/1/993
"," in [context]
A/1/1814
"," in [context]
A/1/757
Commas after conjunctions at starts of sentences or connecting clauses
Cluster #143
A/1/586
Commas in discussions of military battles
A/1/658
This feature fires when there is a comma used in the middle or end of a sentence.
A/1/1327
This feature appears to fire when there is a comma followed by a word starting with a vowel.
A/1/1389
Comma after an "if" clause
A/1/3754
"," in [context]
A/1/3160
"," in [context]
Cluster #6
A/0/75
This feature fires when it encounters the word "and" or the word "or" being used as conjunctions.
A/0/214
The feature fires when it sees the word 'and'.
A/0/403
The feature fires for sentences that have a comma separating two phrases, clauses or lists, or the words "or", "and", "but".
A/1/33
" and" in [context]
A/1/56
Code: " and"/" or" in comments about data
A/1/87
" and" after country names and other geographical locations
A/1/92
" and" in [context]
A/1/310
"and" conjoining family members
A/1/324
" and" after a full (first+last) name
A/1/337
This feature attends to coordinating conjunctions such as "and" and "or".
A/1/391
" and" in U.S. history or geography
A/1/406
" and" in [context]
A/1/524
" or" joining number words or similar, e.g. "one or two" or "one or more"
A/1/603
"and" in chemistry text
A/1/783
[Ultralow density cluster]
A/1/802
" or" in [context]
A/1/822
[Ultralow density cluster]
A/1/843
" and" in recipes
A/1/1000
" and" and other conjunctions in computing text
A/1/1074
" and" in [context]
A/1/1137
" and" shortly after EOT
A/1/1211
" and" in [context]
A/1/1249
" and" scientific
A/1/1274
" and" in [context]
A/1/1349
" and" in [context]
A/1/1386
" and" in [context]
A/1/1431
" or" in [context]
A/1/1495
" and" in [context]
A/1/1851
" and" in [context]
A/1/1882
" and" in [context]
A/1/1934
" and" after an emotion, predicting another emotion
A/1/1949
" and" in [context]
A/1/2058
" &" in [context]
A/1/2117
"and"
A/1/2208
" and"/" or", after a number and time interval (e.g. "six months") and predicting another time interval
A/1/2251
" and" in [context]
A/1/2272
" and" and conjunctions in demography or population surveys
A/1/2356
" and" in [context]
A/1/2419
" and" after groups of people, predicting other groups of people
A/1/2428
" and" in [context]
A/1/2431
" and" in [context]
A/1/2520
" and" in [context]
A/1/2585
" and" in [context]
A/1/2597
" and" in [context]
A/1/2612
" and" in [context]
A/1/2623
" and" in [context]
A/1/2679
" and" in [context]
A/1/2747
"and" in context of color descriptions of objects
A/1/2849
" or" in [context]
A/1/2860
The feature fires when words like "or", "and", "to", or punctuation like "," appear.
A/1/2884
" and" in [context]
A/1/3042
" and" in [context]
A/1/3106
" and" in [context]
A/1/3142
'or' in 'and/or'
A/1/3196
" and" in [context]
A/1/3217
" and" in [context]
A/1/3220
" and"/" or" after an abbreviation, predicting another abbreviation
A/1/3232
" and"/" or" largely in all-lowercase contexts like chat logs
A/1/3244
" and" in [context]
A/1/3454
" and" in [context]
A/1/3486
" and"/" or" after and predicting adjectives like "economic" or "medical"
A/1/3618
" and" in [context]
A/1/3671
[Ultralow density cluster]
A/1/3757
" and" in [context]
A/1/3790
This feature fires for sentences that contain the word "and".
A/1/3875
"and" conjunction predicting a past-tense verb
A/1/3906
" and" in [context]
A/1/3983
" and" and sometimes " or" in medical/anatomical contexts
A/1/2611
"," in [context]
Cluster #147
A/0/310
This feature fires when there is a comma followed by a space.
A/1/2367
"," in [context]
A/1/3376
"," in [context]
A/1/3595
This feature seems to attend to punctuation, specifically commas. It fires highest for commas and has a secondary activation for other types of punctuation like exclamation points and semicolons.
A/1/3634
"," in [context]
A/1/3995
"," in [context]
A/1/3506
Commas near beginning of quoted English text
A/1/3966
Adjective describing a famous person (occupation or nationality)
Cluster #146
A/0/504
This feature seems to fire when there is a comma followed by the word "and".
A/1/814
"," in [context]
A/1/1059
"," in [context]
A/1/1116
',' comma in archaic english
A/1/1988
"," in [context]
A/1/2925
Female - she / her / herself ... ?
A/1/2390
";" in [context]
A/1/1928
"," in [context]
A/1/382
"," in sports reporting, particularly after "season" / units of time
A/1/3897
"," in [context]
Cluster #105
A/0/123
This feature fires when a name is followed by a comma.
A/1/739
Comma after a work of art (performance, album, book)
A/1/1081
"," separating full names in lists
A/1/1238
Commas in lists of countries
A/1/1631
"," in [context]
A/1/1828
"," after scientific institutions
A/1/2007
[Ultralow density cluster]
A/1/2397
"," in [context]
A/1/2964
Commas in European history
A/1/3459
[Ultralow density cluster]
A/1/1365
The feature fires when it sees quotation marks with text followed by punctuation such as a comma or period.
A/1/657
"," in [context]
Cluster #84
A/1/331
"," in legal contexts?
A/1/673
"," after/between years
A/1/1864
"," in [context]
Cluster #80
A/0/6
The feature fires when it sees punctuation like commas, semicolons and periods, especially when they follow a word that ends with a lowercase letter.
A/1/1066
"," in [context]
A/1/1152
"," in [context]
A/1/3394
This feature fires when it sees a semicolon (;) followed by a parenthetical citation with a 4 digit date.
A/1/3697
"," in [context]
A/1/1692
"," in [context]
A/1/2893
Commas in thousands-place separators (patent numbers?)
Cluster #57
A/1/1406
".," in [context]
A/1/2622
"," after physicist last names, predicting " Phys"
A/1/2767
Commas in citations
Cluster #73
A/0/240
This feature attends to numbers from 1 to 99,999 preceded or followed by commas.
A/1/32
"," after small numbers in sequences
A/1/2014
Commas separating numbers (with spaces)
A/1/2209
"," in numbers
Cluster #83
A/1/546
Commas, often in book titles or preceding them to delimit the author
A/1/2690
"]" in [context]
A/1/3653
")" in legal documents
A/1/4051
Korean: ?
Cluster #7
A/0/48
This feature fires when it sees Greek text, specifically Ancient Greek.
A/1/572
Greek: ?
A/1/634
Greek: "ι"?
A/1/1586
Khmer, Amharic, (Greek?): ?
A/1/2599
Greek: Capital letters?
A/1/3453
Greek: nu, chi
A/1/3831
Greek: Vowel, especially omicron and upsilon
Cluster #13
A/0/66
This feature fires when it sees foreign characters such as accented characters, Cyrillic and Greek characters.
A/0/79
This feature seems to fire on mathematical fractions, degrees symbols, and other symbols commonly used in math and science writing.
A/0/80
This feature fires on Greek text.
A/0/101
This feature seems to fire for characters and scripts that are not Latin-based. Some examples where it has high activation include Georgian, Japanese, Chinese, Korean, Greek, Cyrillic, and Thai characters.
A/0/290
This feature attends to non-Latin alphabet languages such as Thai, Tamil, Georgian, Hindi, Japanese, Arabic, etc.
A/1/0
Japanese, Chinese, other Asian scripts: Interwoven in English
A/1/78
Japanese: "\xe9"?
A/1/281
Greek?: "\\xe1"
A/1/422
Thai: Unicode prefix "\xe0\xb8", after vowel
A/1/426
Start of Unicode character esp. predicting \x95 in U+2215 HORIZONTAL BAR
A/1/432
"\xc3" in UTF-8 as latin1 mojibake, esp. of Russian
A/1/558
"\\xce" in [context]
A/1/589
"\\xe0\\xa4"
A/1/619
"\\xe1\\xbb"
A/1/641
Japanese: beginnings of characters?
A/1/795
Arabic: Unicode start "\xd9"
A/1/870
Japanese: ?
A/1/903
"\\xce" in [context]
A/1/934
Japanese: "\xe8\xa6"?
A/1/1009
" \\xce"
A/1/1016
Hebrew, Arabic, Farsi: Unicode block "\xd7"?
A/1/1096
[Ultralow density cluster]
A/1/1163
Telugu?: ?
A/1/1193
Japanese: "\\xe3\\x82"?
A/1/1232
Math (Incomplete Unicode)
A/1/1235
\xc2 in UTF-8 as latin1 mojibake of Korean?
A/1/1345
"\xc3" in UTF-8 as latin1 mojibake of Arabic?
A/1/1369
Bengali: Unicode sequence start
A/1/1499
"\\xc4" in [context]
A/1/1724
Georgian and Korean?: "\xe1\x83"
A/1/1820
South Asian scripts: Unicode start
A/1/1904
Japanese: Katakana "extend the previous vowel sound for longer" partial-uniocde character
A/1/1940
Japanese: beginnings of kanji
A/1/2012
"\\xcf" in [context]
A/1/2152
Japanese: Beginnings of kanji
A/1/2399
Japanese, Chinese: "\xe6\xa5"?
A/1/2453
Japanese: "\xe9\x9a"?
A/1/2465
Armenian: "\xd5"
A/1/2466
\xc2 as UTF-8 as latin1 mojibake of Chinese?
A/1/2472
Vietnamese (Thai?): "\xe1\xba"
A/1/2556
Japanese, Chinese, other Asian scripts: Unicode Start \xe6\xb0?
A/1/2563
Thai: Unicode prefix "\xe0\xb9"
A/1/2608
"\\xce" in [context]
A/1/2617
Chinese and Japanese: beginnings of characters
A/1/2618
Japanese: Hiragana unicode character byte-prefix (predicting the postfix)
A/1/2636
\xc2 in UTF-8 as latin1 mojibake of Korean?
A/1/2725
\xc2 in UTF-8 as latin1 mojibake of Chinese?
A/1/2736
Japanese and Chinese: "\xe5\x88"
A/1/2745
\xc2 in UTF-8 as latin1 mojibake of Chinese?
A/1/2782
Japanese: "\\xe3\\x83"
A/1/2798
\xc2 (Unicode starter of Unicode continuation) in UTF-8 as latin1 mojibake of Russian?
A/1/2852
" \\xcf"
A/1/2919
" \\xe0\\xa4"
A/1/2957
Unicode start codes for combining accents
A/1/3006
Emoji? / mid-Unicode bytes
A/1/3133
Hebrew: "\xd6"?
A/1/3150
"\\xc5"
A/1/3345
Gujarati: ?
A/1/3368
Japanese and other Asian languages: "\xe9\x80"?
A/1/3399
Arabic: Unicode start "\xd8"?
A/1/3468
Japanese, Chinese, and Korean: "\\xe7"?
A/1/3561
Thai: unicode prefix "\xe0\xb8"
A/1/3687
Georgian (Unicode Start)
A/1/3692
Hebrew: Unicode Block " \xd7"?
A/1/3726
"\xc2" in UTF-8 as latin1 mojibake of Chinese?
A/1/3748
" \xcc" in mojibake?
A/1/3785
Japanese: "\xe6\x8a"?
A/1/3815
Chinese and Japanese: Beginnings of characters, vaguely poetic?
A/1/3907
Arabic: Unicode start " \xd8"?
A/1/3908
" \xc3" in UTF-8 as latin1 mojibake of Korean? (beginnings of chat logs?)
A/1/3967
Japanese: "\xe7\x9b"?
A/1/4004
Japanese: "\xe8\xaa"?
A/1/4050
Greek: ?
A/1/522
Tamil: Unicode start
A/1/1280
Greek: ?
A/1/3119
Czech, Lithuanian, Tamil: Unicode code point start
Cluster #17
A/0/435
The feature fires when it sees text written in South Indian languages such as Tamil, Telugu, Malayalam, and Kannada.
A/1/416
Hebrew: Unicode end
A/1/466
Sinhala: ?
A/1/480
Armenian: Unicode end
A/1/580
Georgian: ?
A/1/994
Georgian: ?
A/1/1287
Bengali: Unicode end
A/1/1900
Punjabi: ?
A/1/2499
Tamil: Unicode end
A/1/2586
Hindi: ?
A/1/2629
Bengali, Thai, Tamil, Lao?: ?
A/1/2790
Devanagari (Hindi?): ?
A/1/2796
Devanagari, Tamil, Telugu, Malayalam, and Thai: Unicode end
Cluster #4
A/0/17
The feature fires on non-ascii and special characters.
A/1/873
UTF-8 as latin1 mojibake of Chinese?
A/1/980
Euro sign etc. in mojibake: UTF-8 as latin1 of quotes, U+201x
A/1/1049
UTF-8 as latin1 mojibake of Chinese?
A/1/1411
"ä" U+00E4 in UTF-8 as latin1 mojibake, esp. of Chinese
A/1/1517
"\x91" in UTF-8 as latin1 mojibake, esp. of Russian
A/1/1635
"\\x90" UTF-8 as latin1 mojibake of Russian?
A/1/2387
UTF-8 as latin1 mojibake of Korean?
A/1/3041
Particularly nasty mojibake (latin1, Windows-1252, Mac Roman; possibly iterated multiple times)
A/1/3440
"Ã" U+00C3 in two-character mojibake, broadly accented characters in European languages
A/1/3914
UTF-8 as latin1 mojibake of Chinese?
A/1/3961
Some Cyrillic script? Unicode starters in UTF-8 as latin1 mojibake, extra cursed by normalization
A/1/1691
Korean: " \\xed"/"\\xed"
Cluster #15
A/0/319
The feature attends to Korean text.
A/1/1184
Korean: ?
A/1/2323
Arabic, Korean, Chinese, Japanese, Russian, Norwegian, et al.: ?
A/1/2955
Korean: ?
Cluster #18
A/0/459
The feature appears to activate for text written in right-to-left languages such as Arabic, Persian, and Hebrew.
A/1/1271
Arabic script: Whitespace
A/1/1466
Arabic script (Urdu?): ?
A/1/1706
Thai: (Unicode Sequence Complete)
A/1/3134
Arabic script: ?
A/1/3450
Arabic script: ?
Cluster #20
A/1/207
Turkic family?: languages in Cyrillic script
A/1/1197
Russian: " \u0441"
A/1/1858
Russian: ?
A/1/2363
Russian: in (conjunction and verbal prefix)
A/1/2459
Russian: word-ends?
A/1/766
Russian: ?
Cluster #14
A/0/98
This feature attends to Russian words related to action verbs, especially those ending in -ть.
A/0/413
This feature attends to Russian grammatical features like verb conjugations and declensions.
A/1/3181
Russian: Prefix in verbs / nouns
A/1/3061
Russian: Beginnings of quotes
A/1/2664
Russian: " \u0443"
Cluster #19
A/1/24
Russian: "и" ?
A/1/1473
Russian: " Т" / " т"
A/1/1964
Russian: " \u0437"
A/1/2557
Russian?: The common "-ция" suffix indicates that the word is a noun
A/1/3073
Russian: л and к characters?
A/1/3415
Russian: Mostly letter "е"
A/1/3553
Russian: " \u0434"
A/1/4095
Russian: " п"
A/0/130
This feature fires when it sees contractions with "I'm" or "I am", especially at the beginning of sentences or quotes.
A/1/225
[Ultralow density cluster]
A/1/1002
" am" after "I", in context of subjective experience
A/1/983
" im"
Cluster #31
A/0/55
This feature fires when the words "was" or "were" have a high activation value.
A/0/164
This feature fires when the token "is" is present.
A/0/272
This feature fires when the phrase "be ___" (with some variation) is present.
A/1/4
" was", esp. after "he"/"she"/a name, predicting an adjective
A/1/111
" is" in [context]
A/1/253
" be" after auxiliary verbs like "must"/"may", sometimes also with "not"
A/1/517
" is" in [context]
A/1/797
" be" in [context]
A/1/885
" is" in [context]
A/1/1095
" is" in [context]
A/1/1378
The feature fires when the third-person singular present-tense "is" follows a noun phrase with an embedded clause, often signaling that a fact or argument is being stated.
A/1/1521
" been" in [context]
A/1/1750
" become"/" becoming"
A/1/2049
" is" in [context]
A/1/2076
This feature fires when a verb in the past tense follows "was" or "were".
A/1/2131
" was" in [context]
A/1/2188
" is" in [context]
A/1/2325
" is" in [context]
A/1/2481
"al" - logit nonsense
A/1/2755
" is" in [context]
A/1/3015
" is" in [context]
A/1/3063
" be" in [context]
A/1/3161
" are" in [context]
A/1/3535
" is" / other copulae, esp after <EOT>
A/1/3558
" be" in [context]
A/1/3910
" was" in [context]
A/1/3982
" being"
A/1/2560
" remain"
Cluster #32
A/0/295
This feature fires when the token "was" appears in the text.
A/1/1001
Copulas (mostly " is") in technical text
A/1/1146
" is" in [context]
A/1/1426
" is" in [context]
A/1/1463
" be" in [context]
A/1/1648
" is" in [context]
A/1/1787
" are" in [context]
A/1/2307
[Ultralow density cluster]
A/1/2341
" been" in scientific manuscripts (shown, demonstrated, proposed)
A/1/2539
" was"/" were" in biomedical studies
A/1/2542
" was"/" were" in medical contexts?
A/1/2844
" be" in [context]
A/1/3339
" is" in [context]
A/1/1223
"'m"
A/1/138
',"' or ",'" concluding what somebody said
A/1/2561
".\"" in [context]
A/1/2591
[Ultralow density cluster]
A/0/334
The feature fires for the period (.) character followed by quotation marks (").
Cluster #115
A/0/47
The feature detects the use of quotation marks as it activates whenever it encounters paired single or double quotation marks.
A/1/1203
".\"" in [context]
A/1/2346
".\"" in [context]
A/1/2816
Closing punctuation+quote esp in series of quotes with no unquoted text
A/1/3421
".\"" in [context]
A/1/2001
This feature fires when sentences end in question marks or exclamation points.
A/1/311
".”" (period + smart close quote)
Cluster #114
A/0/200
The feature seems to fire the most when a sentence ends with a period.
A/1/170
"." in sports discussion
A/1/1497
"." in [context]
A/1/2230
".'"
A/1/3932
"." in [context]
A/1/3968
"." in [context]
Cluster #0
A/0/25
A/0/89
A/0/120
A/0/124
A/0/208
A/0/299
A/0/309
A/0/336
A/0/467
A/1/31
[Ultralow density cluster]
A/1/37
[Ultralow density cluster]
A/1/57
[Ultralow density cluster]
A/1/79
[Ultralow density cluster]
A/1/82
[Ultralow density cluster]
A/1/95
[Ultralow density cluster]
A/1/122
[Ultralow density cluster]
A/1/131
[Ultralow density cluster]
A/1/135
[Ultralow density cluster]
A/1/168
[Ultralow density cluster]
A/1/188
[Ultralow density cluster]
A/1/190
[Ultralow density cluster]
A/1/191
[Ultralow density cluster]
A/1/214
[Ultralow density cluster]
A/1/217
[Ultralow density cluster]
A/1/223
[Ultralow density cluster]
A/1/227
[Ultralow density cluster]
A/1/238
[Ultralow density cluster]
A/1/240
[Ultralow density cluster]
A/1/242
[Ultralow density cluster]
A/1/250
[Ultralow density cluster]
A/1/260
[Ultralow density cluster]
A/1/266
[Ultralow density cluster]
A/1/286
[Ultralow density cluster]
A/1/288
[Ultralow density cluster]
A/1/298
[Ultralow density cluster]
A/1/299
[Ultralow density cluster]
A/1/309
[Ultralow density cluster]
A/1/312
[Ultralow density cluster]
A/1/318
[Ultralow density cluster]
A/1/325
[Ultralow density cluster]
A/1/354
[Ultralow density cluster]
A/1/361
[Ultralow density cluster]
A/1/368
[Ultralow density cluster]
A/1/383
[Ultralow density cluster]
A/1/387
[Ultralow density cluster]
A/1/401
[Ultralow density cluster]
A/1/414
[Ultralow density cluster]
A/1/428
[Ultralow density cluster]
A/1/433
[Ultralow density cluster]
A/1/440
[Ultralow density cluster]
A/1/446
[Ultralow density cluster]
A/1/448
[Ultralow density cluster]
A/1/459
[Ultralow density cluster]
A/1/468
[Ultralow density cluster]
A/1/490
[Ultralow density cluster]
A/1/504
[Ultralow density cluster]
A/1/518
[Ultralow density cluster]
A/1/562
[Ultralow density cluster]
A/1/568
[Ultralow density cluster]
A/1/574
[Ultralow density cluster]
A/1/581
[Ultralow density cluster]
A/1/602
[Ultralow density cluster]
A/1/637
[Ultralow density cluster]
A/1/659
[Ultralow density cluster]
A/1/670
[Ultralow density cluster]
A/1/695
[Ultralow density cluster]
A/1/710
[Ultralow density cluster]
A/1/719
[Ultralow density cluster]
A/1/742
[Ultralow density cluster]
A/1/779
[Ultralow density cluster]
A/1/793
[Ultralow density cluster]
A/1/801
[Ultralow density cluster]
A/1/886
[Ultralow density cluster]
A/1/892
[Ultralow density cluster]
A/1/926
[Ultralow density cluster]
A/1/950
[Ultralow density cluster]
A/1/966
[Ultralow density cluster]
A/1/977
[Ultralow density cluster]
A/1/987
[Ultralow density cluster]
A/1/992
[Ultralow density cluster]
A/1/1006
[Ultralow density cluster]
A/1/1011
[Ultralow density cluster]
A/1/1018
[Ultralow density cluster]
A/1/1048
[Ultralow density cluster]
A/1/1058
[Ultralow density cluster]
A/1/1071
[Ultralow density cluster]
A/1/1072
[Ultralow density cluster]
A/1/1097
[Ultralow density cluster]
A/1/1109
[Ultralow density cluster]
A/1/1118
[Ultralow density cluster]
A/1/1121
[Ultralow density cluster]
A/1/1151
[Ultralow density cluster]
A/1/1158
[Ultralow density cluster]
A/1/1161
[Ultralow density cluster]
A/1/1170
[Ultralow density cluster]
A/1/1217
[Ultralow density cluster]
A/1/1221
[Ultralow density cluster]
A/1/1228
[Ultralow density cluster]
A/1/1247
[Ultralow density cluster]
A/1/1252
[Ultralow density cluster]
A/1/1257
[Ultralow density cluster]
A/1/1265
[Ultralow density cluster]
A/1/1267
[Ultralow density cluster]
A/1/1289
[Ultralow density cluster]
A/1/1290
[Ultralow density cluster]
A/1/1294
[Ultralow density cluster]
A/1/1324
[Ultralow density cluster]
A/1/1331
[Ultralow density cluster]
A/1/1336
[Ultralow density cluster]
A/1/1341
[Ultralow density cluster]
A/1/1356
[Ultralow density cluster]
A/1/1370
[Ultralow density cluster]
A/1/1393
[Ultralow density cluster]
A/1/1402
[Ultralow density cluster]
A/1/1407
[Ultralow density cluster]
A/1/1421
[Ultralow density cluster]
A/1/1427
[Ultralow density cluster]
A/1/1468
[Ultralow density cluster]
A/1/1486
[Ultralow density cluster]
A/1/1492
[Ultralow density cluster]
A/1/1504
[Ultralow density cluster]
A/1/1516
[Ultralow density cluster]
A/1/1523
[Ultralow density cluster]
A/1/1525
[Ultralow density cluster]
A/1/1528
[Ultralow density cluster]
A/1/1530
[Ultralow density cluster]
A/1/1540
[Ultralow density cluster]
A/1/1569
[Ultralow density cluster]
A/1/1574
[Ultralow density cluster]
A/1/1585
[Ultralow density cluster]
A/1/1590
[Ultralow density cluster]
A/1/1594
[Ultralow density cluster]
A/1/1617
[Ultralow density cluster]
A/1/1646
[Ultralow density cluster]
A/1/1684
[Ultralow density cluster]
A/1/1702
[Ultralow density cluster]
A/1/1705
[Ultralow density cluster]
A/1/1710
[Ultralow density cluster]
A/1/1711
[Ultralow density cluster]
A/1/1715
[Ultralow density cluster]
A/1/1734
[Ultralow density cluster]
A/1/1744
[Ultralow density cluster]
A/1/1748
[Ultralow density cluster]
A/1/1756
[Ultralow density cluster]
A/1/1765
[Ultralow density cluster]
A/1/1767
[Ultralow density cluster]
A/1/1769
[Ultralow density cluster]
A/1/1792
[Ultralow density cluster]
A/1/1816
[Ultralow density cluster]
A/1/1817
[Ultralow density cluster]
A/1/1819
[Ultralow density cluster]
A/1/1831
[Ultralow density cluster]
A/1/1839
[Ultralow density cluster]
A/1/1854
[Ultralow density cluster]
A/1/1861
[Ultralow density cluster]
A/1/1869
[Ultralow density cluster]
A/1/1873
[Ultralow density cluster]
A/1/1884
[Ultralow density cluster]
A/1/1888
[Ultralow density cluster]
A/1/1897
[Ultralow density cluster]
A/1/1899
[Ultralow density cluster]
A/1/1946
[Ultralow density cluster]
A/1/1963
[Ultralow density cluster]
A/1/1966
[Ultralow density cluster]
A/1/1971
[Ultralow density cluster]
A/1/1976
[Ultralow density cluster]
A/1/1989
[Ultralow density cluster]
A/1/1994
[Ultralow density cluster]
A/1/2004
[Ultralow density cluster]
A/1/2010
[Ultralow density cluster]
A/1/2029
[Ultralow density cluster]
A/1/2043
[Ultralow density cluster]
A/1/2050
[Ultralow density cluster]
A/1/2080
[Ultralow density cluster]
A/1/2086
[Ultralow density cluster]
A/1/2093
[Ultralow density cluster]
A/1/2097
[Ultralow density cluster]
A/1/2110
[Ultralow density cluster]
A/1/2120
[Ultralow density cluster]
A/1/2128
[Ultralow density cluster]
A/1/2138
[Ultralow density cluster]
A/1/2218
[Ultralow density cluster]
A/1/2220
[Ultralow density cluster]
A/1/2223
[Ultralow density cluster]
A/1/2238
[Ultralow density cluster]
A/1/2246
[Ultralow density cluster]
A/1/2247
[Ultralow density cluster]
A/1/2257
[Ultralow density cluster]
A/1/2297
[Ultralow density cluster]
A/1/2305
[Ultralow density cluster]
A/1/2333
[Ultralow density cluster]
A/1/2355
[Ultralow density cluster]
A/1/2365
[Ultralow density cluster]
A/1/2388
[Ultralow density cluster]
A/1/2392
[Ultralow density cluster]
A/1/2408
[Ultralow density cluster]
A/1/2437
[Ultralow density cluster]
A/1/2441
[Ultralow density cluster]
A/1/2447
[Ultralow density cluster]
A/1/2470
[Ultralow density cluster]
A/1/2475
[Ultralow density cluster]
A/1/2480
[Ultralow density cluster]
A/1/2490
[Ultralow density cluster]
A/1/2511
[Ultralow density cluster]
A/1/2521
[Ultralow density cluster]
A/1/2537
[Ultralow density cluster]
A/1/2541
[Ultralow density cluster]
A/1/2544
[Ultralow density cluster]
A/1/2551
[Ultralow density cluster]
A/1/2564
[Ultralow density cluster]
A/1/2575
[Ultralow density cluster]
A/1/2581
[Ultralow density cluster]
A/1/2583
[Ultralow density cluster]
A/1/2619
[Ultralow density cluster]
A/1/2643
[Ultralow density cluster]
A/1/2648
[Ultralow density cluster]
A/1/2682
[Ultralow density cluster]
A/1/2685
[Ultralow density cluster]
A/1/2692
[Ultralow density cluster]
A/1/2714
[Ultralow density cluster]
A/1/2716
[Ultralow density cluster]
A/1/2717
[Ultralow density cluster]
A/1/2726
[Ultralow density cluster]
A/1/2733
[Ultralow density cluster]
A/1/2749
[Ultralow density cluster]
A/1/2771
[Ultralow density cluster]
A/1/2775
[Ultralow density cluster]
A/1/2833
[Ultralow density cluster]
A/1/2858
[Ultralow density cluster]
A/1/2869
[Ultralow density cluster]
A/1/2870
[Ultralow density cluster]
A/1/2891
[Ultralow density cluster]
A/1/2895
[Ultralow density cluster]
A/1/2896
[Ultralow density cluster]
A/1/2912
[Ultralow density cluster]
A/1/2923
[Ultralow density cluster]
A/1/2924
[Ultralow density cluster]
A/1/2935
[Ultralow density cluster]
A/1/2936
[Ultralow density cluster]
A/1/2938
[Ultralow density cluster]
A/1/2961
[Ultralow density cluster]
A/1/2962
[Ultralow density cluster]
A/1/2966
[Ultralow density cluster]
A/1/2980
[Ultralow density cluster]
A/1/2987
[Ultralow density cluster]
A/1/2991
[Ultralow density cluster]
A/1/3025
[Ultralow density cluster]
A/1/3051
[Ultralow density cluster]
A/1/3069
[Ultralow density cluster]
A/1/3079
[Ultralow density cluster]
A/1/3086
[Ultralow density cluster]
A/1/3092
[Ultralow density cluster]
A/1/3101
[Ultralow density cluster]
A/1/3103
[Ultralow density cluster]
A/1/3113
[Ultralow density cluster]
A/1/3117
[Ultralow density cluster]
A/1/3127
[Ultralow density cluster]
A/1/3148
[Ultralow density cluster]
A/1/3153
[Ultralow density cluster]
A/1/3168
[Ultralow density cluster]
A/1/3170
[Ultralow density cluster]
A/1/3171
[Ultralow density cluster]
A/1/3172
[Ultralow density cluster]
A/1/3187
[Ultralow density cluster]
A/1/3202
[Ultralow density cluster]
A/1/3221
[Ultralow density cluster]
A/1/3239
[Ultralow density cluster]
A/1/3251
[Ultralow density cluster]
A/1/3257
[Ultralow density cluster]
A/1/3266
[Ultralow density cluster]
A/1/3268
[Ultralow density cluster]
A/1/3273
This feature fires for words related to light, specifically luminescence.
A/1/3277
[Ultralow density cluster]
A/1/3285
[Ultralow density cluster]
A/1/3296
[Ultralow density cluster]
A/1/3300
[Ultralow density cluster]
A/1/3302
[Ultralow density cluster]
A/1/3312
[Ultralow density cluster]
A/1/3330
[Ultralow density cluster]
A/1/3360
[Ultralow density cluster]
A/1/3372
[Ultralow density cluster]
A/1/3377
[Ultralow density cluster]
A/1/3390
[Ultralow density cluster]
A/1/3391
[Ultralow density cluster]
A/1/3395
[Ultralow density cluster]
A/1/3398
[Ultralow density cluster]
A/1/3409
[Ultralow density cluster]
A/1/3431
[Ultralow density cluster]
A/1/3436
[Ultralow density cluster]
A/1/3473
[Ultralow density cluster]
A/1/3478
[Ultralow density cluster]
A/1/3480
[Ultralow density cluster]
A/1/3497
[Ultralow density cluster]
A/1/3531
[Ultralow density cluster]
A/1/3538
[Ultralow density cluster]
A/1/3541
[Ultralow density cluster]
A/1/3544
[Ultralow density cluster]
A/1/3566
[Ultralow density cluster]
A/1/3571
[Ultralow density cluster]
A/1/3575
[Ultralow density cluster]
A/1/3586
[Ultralow density cluster]
A/1/3597
[Ultralow density cluster]
A/1/3633
[Ultralow density cluster]
A/1/3640
[Ultralow density cluster]
A/1/3645
[Ultralow density cluster]
A/1/3654
[Ultralow density cluster]
A/1/3661
[Ultralow density cluster]
A/1/3662
[Ultralow density cluster]
A/1/3686
[Ultralow density cluster]
A/1/3695
[Ultralow density cluster]
A/1/3707
[Ultralow density cluster]
A/1/3714
[Ultralow density cluster]
A/1/3719
[Ultralow density cluster]
A/1/3722
[Ultralow density cluster]
A/1/3724
[Ultralow density cluster]
A/1/3727
[Ultralow density cluster]
A/1/3731
[Ultralow density cluster]
A/1/3735
[Ultralow density cluster]
A/1/3744
[Ultralow density cluster]
A/1/3764
[Ultralow density cluster]
A/1/3770
[Ultralow density cluster]
A/1/3773
[Ultralow density cluster]
A/1/3775
[Ultralow density cluster]
A/1/3781
[Ultralow density cluster]
A/1/3786
[Ultralow density cluster]
A/1/3795
[Ultralow density cluster]
A/1/3824
[Ultralow density cluster]
A/1/3834
[Ultralow density cluster]
A/1/3862
[Ultralow density cluster]
A/1/3867
[Ultralow density cluster]
A/1/3896
[Ultralow density cluster]
A/1/3930
[Ultralow density cluster]
A/1/3935
[Ultralow density cluster]
A/1/3938
[Ultralow density cluster]
A/1/3943
[Ultralow density cluster]
A/1/3946
[Ultralow density cluster]
A/1/3951
[Ultralow density cluster]
A/1/3959
[Ultralow density cluster]
A/1/3971
[Ultralow density cluster]
A/1/3996
[Ultralow density cluster]
A/1/4002
[Ultralow density cluster]
A/1/4009
[Ultralow density cluster]
A/1/4015
[Ultralow density cluster]
A/1/4017
[Ultralow density cluster]
A/1/4021
[Ultralow density cluster]
A/1/4029
[Ultralow density cluster]
A/1/4039
[Ultralow density cluster]
A/1/4057
[Ultralow density cluster]
Cluster #104
A/0/15
This feature fires when the current token is followed by a period.
A/1/248
"." in [context]
A/1/264
"." ending sentences in historical/chronological contexts
A/1/415
Period (in media review?)
A/1/537
"." ending a sentence in a news report about politics
A/1/614
"." in [context]
A/1/913
"." in [context]
A/1/1514
Periods and other sentence-concluding punctuation, esp. in video games?
A/1/1722
"." in [context]
A/1/1755
"." in [context]
A/1/1809
"." in [context]
A/1/2013
Periods, esp. shortly after EOT
A/1/2294
"." in [context]
A/1/2411
"." ending sentences in business/finance news contexts?
A/1/2495
")." as end of English language parenthetical
A/1/3294
"." in [context]
A/1/3515
"." in [context]
A/1/3636
"." in [context]
A/1/3958
"." in [context]
Cluster #103
A/0/135
The feature fires when it sees punctuation marks and symbols like periods, parentheses, brackets, dollar signs and mathematical symbols at the end of tokens.
A/0/498
This feature fires when it sees punctuation at the end of a sentence or phrase, particularly periods "."
A/1/513
"." ending a sentence about medical care or formal medical reports
A/1/531
"." ending a sentence in a scientific paper, mostly materials science
A/1/532
"." ending a sentence in a journal paper or formal publication about disease or immunology or similar
A/1/831
Periods at the end of sentences in papers on radio astronomy
A/1/1357
"]\\]."
A/1/2368
End parenthetical + new sentence in academic param
A/1/3924
"." in [context]
A/1/4024
This feature fires when a sentence ends with a period.
A/1/796
" ." used in repeated " . . ." style contexts
A/1/3810
"." in [context]
Cluster #76
A/0/424
This feature fires when it sees a sentence ending in a question mark.
A/1/587
"!"
A/1/2770
"?" in news?
A/0/285
This feature attends to sentences that end with a period followed by whitespace.
A/1/621
"?" in dialogue?
A/1/3668
"." in [context]
Cluster #67
A/1/979
" ;)"
A/1/2062
" :)"
A/1/2855
"." in [context]
Cluster #72
A/0/46
This feature seems to fire when it sees words in chat logs or transcripts that users would type when asking for technical help or troubleshooting issues.
A/1/277
[Ultralow density cluster]
A/1/1282
Ubuntu/linux conversations (irc chat?)
A/1/1902
" the" in installations, operating systems?
A/1/4079
Ubuntu/Linux/GNOME terms, esp. lowercase in chat
A/1/2641
(IRC?) usernames being mentioned/replied to chat logs, after > and before :
A/1/2035
Punctuation in paths or package names near "ubuntu"/"linux"/etc.
Cluster #71
A/0/151
This feature fires when it sees <> and : style chat messages, IRC style messages, and other messaging styles like that.
A/1/149
":" in ">"-based chat logs?
A/1/2616
">" in [context]
Cluster #82
A/0/474
This feature fires for instances of a single space in the token sequence.
A/1/1117
The first space in a double-space between sentences, in a legal context
A/1/3053
Newline+spaces near Old/poetic English?
A/1/3720
" " in [context]
A/1/3899
Line breaks followed by many spaces in legal contexts
A/1/4023
" " in [context]
A/1/50
"..." and some other long English punctuation
Cluster #101
A/0/437
This feature fires when it sees line breaks (like em-dashes, hyphens, vertical bars) used as separators, especially between sentences or clauses.
A/1/185
Closing parenthesis on years
A/1/598
"\u201d"
A/1/2548
"\"" in [context]
A/1/3559
"space em-dash" (em dashes typeset open) in English prose
A/1/3592
")" in [context]
A/1/2759
The feature attends to punctuation around proper nouns.
Cluster #66
A/0/367
This feature fires when it sees structured references (e.g., titles, sections, footnotes, page numbers) that are commonly seen in academic writing and publications.
A/1/960
Punctuation near "_"-surrounded numbers/variables
A/1/1916
Continue chains of commas (and maybe related punctuation)?
Cluster #62
A/0/11
This feature seems to fire on numerical decimal points, especially those with 2-3 digits before and after the decimal point.
A/1/116
Legal text "." and ")." ?
A/1/498
[Ultralow density cluster]
A/1/569
Decimal point in a number
A/1/1114
"." as a decimal separator
A/1/1236
" ." in [context]
A/1/1286
"." in [context]
A/1/1821
"+"
A/1/2106
"." in [context]
A/1/2684
"." in [context]
A/1/2768
"." among long strings of digits, often ones with multiple "."s
A/1/2889
"." between numbers near lots of spaces
A/1/3082
"." in [context]
A/1/3609
"." interspersed between more than two numbers (version numbers, IP addresses; but not decimal numbers)
A/1/2130
" ?"
Cluster #100
A/0/507
The feature fires on the colon (:) symbol.
A/1/694
":" in [context]
A/1/959
" -" in [context]
A/1/1004
: separator in times
A/1/1914
":" in [context]
A/1/2229
":" in [context]
A/1/2347
":" in [context]
A/1/2713
: to indicate someone is speaking in a script
A/1/4091
":" in citations of sources
Cluster #97
A/1/408
":" after "\n\nA" (in Q: and A: pairs?)
A/1/2144
":" in [context]
A/1/2186
":" in [context]
A/1/2908
" !"
A/0/402
The feature seems to fire when it encounters punctuation such as colons, parentheses, brackets, slashes, pipes, etc. Especially when they are at the end of words or lines.
A/1/96
":" esp. after "abstract" (in some paper metadata?)
A/1/2846
":" in [context]
A/1/3751
[Ultralow density cluster]
A/1/3669
Punctuation or non-content links in forum posts?
A/1/2817
Time specification
A/1/1013
" "/" " in [context]
A/1/1537
Long spans of whitespace in poorly-formatted documents, often near decimal points
Cluster #86
A/0/426
This feature attends to formatted tabular data such as titles, tables, lists, tables of contents, etc.
A/1/1299
Spaces near vertical bars (Markdown/ASCII tables?)
A/1/3826
Runs of periods
A/1/4090
" |" in Markdown tables?
A/1/2885
Emoji ?
Cluster #85
A/1/91
ASCII/Unicode box drawings
A/1/972
Long runs of spaces (in technical writing / math?)
A/1/1717
Math
A/1/2139
"-->" indicating the span of time between two listed timestamps (and occasionally captures hexadecimal base pairs separated by spaces?)
A/1/682
Pipe-separated text (esp. words)
A/1/1852
LaTeX "bf" and other macros after "\"
Cluster #45
A/0/95
The feature attends to hexadecimal numbers.
A/1/818
Comma-separated numbers
A/1/884
Digits after "0x" in "0x" hex literals
A/1/1399
" "/" "/" "/" "/" "/" " in [context]
A/1/2213
"," separating hexadecimal literals like "0xf1"
A/1/2430
" 0" beginning "0x" hexadecimal literals
A/1/3203
Hex numbers after "0x", usually in code defining ALLCAPS constants
A/1/2605
This feature fires on words that contain a dash "-", particularly scientific and biochemistry related terms.
A/1/3942
"-" in [context]
A/1/1272
"-" in [context]
Cluster #41
A/0/114
The feature fires when it encounters a hyphen followed by lowercase letters like in chemical compound names and hyphenated phrases.
A/1/99
Hyphen in Title-Case compounds
A/1/145
Hyphen after ALLCAPS abbreviation (SCPs, etc...)
A/1/2113
"-" connecting last names
A/1/2327
"-" in [context]
A/1/2569
"/" in [context]
Cluster #30
A/0/112
This feature fires on hyphens between numbers, especially when denoting age.
A/1/12
Hyphen, esp. after "Code § " and a number, denoting a legal code section
A/1/906
"/" for number/number in normal language text, often used for things like dates or simple fractions, but not in a math formula
A/1/1326
"-" in [context]
A/1/1678
"\u2013"
A/1/2115
"-" in identifiers/numbers/phone-numbers
A/1/2967
Seperator in dates
A/1/3712
^/^- in scientific notation
Cluster #40
A/0/243
This feature seems to be attending to hyphens followed by another word or number.
A/0/286
This feature fires on hyphenated words often indicating prefixes and suffixes.
A/1/246
"-" in hyphenated constructs esp. after "anti"
A/1/333
"-" after common compound-word-forming preposition: "in"/"on"/"over"/...
A/1/489
"-" in [context]
A/1/556
"-" in [context]
A/1/902
"-" in [context]
A/1/986
"-" in [context]
A/1/1435
"-" in [context]
A/1/1524
"-" in [context]
A/1/1887
"-" in [context]
A/1/3007
"-" after "well"
A/1/3110
"-" in [context]
A/1/3245
"-" in [context]
A/1/3271
"-" in [context]
A/1/3423
"-" in [context]
A/1/3474
"-" in "non-"
A/1/3643
"-" in [context]
A/1/3931
"-" in [context]
A/1/1043
"/" separating language concepts like "and/or" or "noun/othernoun"
A/1/2438
"-" in [context]
A/0/276
This feature attends to punctuation like hyphens, dashes, slashes, parentheses, quotes, apostrophes, etc.
A/1/1115
Hyphen used to join words
A/0/385
This feature seems to fire after underscore characters when they are being used to emphasize or highlight the words/phrases before and after them.
A/1/3679
"_" typically concluding italics for a title, e.g. in Markdown
Cluster #29
A/0/157
The feature fires when there are quotation marks or other types of quotes around words or phrases.
A/1/19
Various European languages: Opening quotes?
A/1/639
" \"" quoting some legal text
A/1/805
" \u201c" in [context]
A/1/927
" \"" for opening an airquote or small-phrased-quote
A/1/1129
Opening quotes, mostly non-ascii and stronger on double-quotes
A/1/2819
"\u2014", emdash
A/1/3649
Opening quotes, esp. " '" and esp. after a comma
A/1/4084
" _"
Cluster #28
A/0/401
The feature fires when the token directly after a period is a quotation mark.
A/1/1763
"\u201c"
A/1/2701
" \"" for someone speaking to follow a sentence of their behavior or previous speaking, as in a story
A/1/3123
" \"" to as part of rapid-fire back-and-forth conversation
A/1/3728
" \""as part of longer back-and-forth conversation
A/0/53
This feature fires strongly on quotation marks, especially double quotation marks “ “.
A/1/194
'"' after two newlines
Cluster #132
A/0/4
This feature fires on line breaks and paragraphs especially in legal or technical documents.
A/1/2235
Code?: Double newlines in legal text or code?
A/1/3350
Quadruple newlines in legal text
A/1/3767
Triple newlines, often after EOT
A/1/3463
Newlines in recipies
Cluster #163
A/0/354
This feature attends to line breaks (new lines) in text, particularly double line breaks.
A/1/1693
Newline before numbers in legal text?
A/1/2853
Newline in legal text
A/1/3462
Newline in legal judgment?
Cluster #130
A/1/804
Court filings
A/1/1198
Newline+Form Feed in legal text?
A/1/1582
"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n " in [context]
Cluster #133
A/0/429
This feature fires for line breaks such as carriage returns and newlines.
A/1/184
Code: "\r\n\r" (carriage-return, newline, carriage-return)
A/1/257
Carriage returns, usu. before newlines, in legal contexts
A/1/625
Code: Carriage returns, usu. before newlines, esp. in C or similar languages
A/1/2443
Carriage returns, usu. followed by newlines, close to EOT and/or in HTML
A/1/2934
Carriage return
A/1/3947
[Ultralow density cluster]
A/1/4068
Code: Carriage returns, usu. before newlines, in CSS
Cluster #167
A/0/100
This feature fires when there is a newline character "\n".
A/0/192
The feature fires when a new paragraph starts.
A/0/372
This feature fires for line breaks / new lines.
A/1/171
Newlines after proper nouns?
A/1/1598
Newline mid-sentence in art/fiction/nature?
A/1/1619
Newline shortly after EOT
A/1/2092
Newline in discussion of accounts/authentication
A/1/2259
Newline in economics?
A/1/2354
Newline after/between quotes/dialogue
A/1/2555
Newline in English prose (fiction? religion/philosophy?)
A/1/2615
Newlines in website updates, payment/subscription management
A/1/2727
Newlines in patents? (mechanical engineering?)
A/1/2734
Newline mid-sentence esp. in religious text
A/1/2760
Newlines before level-3 Markdown/ATX headers
A/1/2822
Newlines after punctuation that's not just a period
A/1/3141
Newline in news reports?
A/1/3155
Various European languages: Newline?
A/1/3236
Newline after sentence-ending period
A/1/3499
Newline in bibliography list?
A/1/3779
Newline after "was"/"were" mid-sentence
Cluster #181
A/0/106
This feature fires when there's a new line that follows an end punctuation mark (., !, ?, ", etc.).
A/1/148
Newline after another newline, predicting new sentence/item
A/1/2454
Newlines after "References"
A/1/948
Newline in chat logs? (IRC?)
Cluster #183
A/0/475
This feature fires on new line characters ("\n").
A/1/400
Newlines near numbers or number-heavy IDs
A/1/545
Newline in biology contexts and/or near repeated "-"/"=" as horizontal dividers
A/1/652
Newline after some curly-brace-based header and before "-"-based horizontal rules
A/1/824
Newlines after math expressions in LaTeX
A/1/863
" \n" after ######
A/1/2182
Newline in biology near section markers?
A/1/2738
Newline separating Wikipedia categories
Cluster #184
A/0/375
This feature fires for text containing numbers ending in question marks on new lines.
A/1/74
Newlines in wikitext tables?
A/1/591
Newline near "="-based horizontal rules
A/1/1261
"\n\n" after "add your own caption"
A/1/3222
Newline in LaTeX esp. after beginning a math environment
A/1/2813
BibTeX
A/1/651
Synthetic math: Newline before/after one-line numbers in synthetic math problems
A/1/3352
Short sequences of asterisks?
A/1/845
[Ultralow density cluster]
A/0/132
The feature attends to Wikipedia articles, firing most for category headings.
A/1/2861
Wikipedia categories
A/1/919
Quotes/newlines in .po/.pot files (https://www.drupal.org/community/contributor-guide/reference-information/localize-drupal-org/working-with-offline/po-and-pot-files)
A/1/2124
Category + :
A/1/1494
[Ultralow density cluster]
A/1/275
Code: "-" esp. in "UTF-8" and other Unicode encoding names
Cluster #172
A/0/364
This feature fires on blocks of legal or license text.
A/1/72
"," in several memorized license strings
A/1/643
" FITNESS" in code licenses (GPL?)
A/1/743
WHETHER in a contract
A/1/803
"Competing interests" phrases
A/1/832
" under" in software licenses
A/1/1437
Copyright dates (and sometimes copyright organizations)
A/1/1647
All rights reserved. (and some other source code licensing texts)
A/1/1834
" *" (and sometimes "//") for use in copyright license multiline c/c++ comments
A/1/2105
" the" in the GPL license?
A/1/2135
Lists of 'Category' in movie metadata?
A/1/2711
" COURT"
A/1/2720
BSD License
A/1/2985
" picked"
A/1/3005
" respect"
A/1/3037
[Ultralow density cluster]
A/1/3219
" License" in the GPL
A/1/3349
Funding acknowledgements
A/1/3691
Activates on "Copyright", predicts the copyright symbol and dates
A/1/3845
"Click here for additional data file" and supplemental data more generally
A/1/3029
[Ultralow density cluster]
A/1/1662
Code: XML namespace references
A/1/3263
Code: Error variables/handling in Go?
A/1/1800
DOIs
A/1/2189
[Ultralow density cluster]
A/1/2514
[Ultralow density cluster]
A/1/2840
Newline mid-sentence or near colons?
Cluster #54
A/0/360
The feature fires on parentheses, especially at the end of lines and sentences.
A/1/853
1) Lists 2) by 3) number?
A/1/1795
Newline after closing parentheses in English prose
Cluster #154
A/0/172
This feature fires for code snippets and other text with programming language syntax, as well as indented outlines and other structured/formatted text.
A/1/403
Code: Newline in HTML
A/1/716
Code: Newline in CSS
A/1/973
Code: Newline in shell commands / documentation
A/1/2824
Tokens near/predicting "Q" in various contexts, esp. "_"
A/1/2917
Newlines after "Q:" and maybe more generally technical troubleshooting
A/1/3121
Second newline after "A:"
A/1/4063
Newline in code-adjacent English prose
Cluster #153
A/0/18
This feature fires for source code, specifically highlighting programming language syntax such as curly braces, indentation, comments, include statements, function definitions, etc.
A/1/559
Code: Newlines in code comments?
A/1/578
Code: Newlines after/between lines beginning with "#" (comments or C/C++ preprocessor?)
A/1/1542
Code: Newlines between/after imports, esp. Python
A/1/3044
Code: Newline after closing delimiters
A/1/3715
Code: Newline in C/C++ especially after comment closing
A/1/23
Code: Newlines in or near C-style /* comments */
A/1/1375
Code: Newline in C/C++/Go?
A/1/2421
Code: "#" in C/C++ preprocessor
Cluster #52
A/1/1303
Code: '.' and '#' in C include statements
A/1/1490
Web file extensions in URLs or links
A/1/1741
"h"
A/1/3812
"\u23ce\n\t"/"\u23ce\n\t\t"/"\u23ce\n\t\t\t"/"\u23ce\n\t\t\t\t" in [context]
A/1/1302
[Ultralow density cluster]
A/1/2741
" {" in [context]
Cluster #160
A/0/170
This feature fires on opening and closing punctuation such as brackets, braces, quotation marks, tags, etc.
A/0/390
This feature seems to fire on semicolons and curly braces, which are common in programming languages. So it is likely detecting source code.
A/0/491
This feature attends to code blocks and style elements found in programming languages like indentation, brackets, semicolons, keywords, variables, and function names.
A/1/1008
Code: Semicolons concluding statements, sometimes with preceding punctuation
A/1/1677
Code: " }" concluding code blocks/functions
A/1/1938
This feature is picking up on parentheses and curly braces that contain code or math expressions.
A/1/2037
"}" in [context]
A/1/2077
Code: Closing delimiters and semicolons, predicting newlines and tab-based indentation
A/1/2336
Code: ends of HTML/XML tags
A/1/2414
Code: Semicolons, esp. ending "return" statements
A/1/2562
">" in [context]
A/1/3135
" {" in [context]
A/1/4030
Code: ";"-based statement enders in code with and predicting "\r\n" (+ spaces)
A/1/4088
" */"
A/1/1020
" []"
A/1/1611
This feature attends to the use of semicolons at the ends of lines of code.
Cluster #161
A/0/30
This feature fires on opening and closing HTML tags.
A/1/609
The feature attends to double quotes that signify the beginning or end of strings/keys in code.
A/1/995
">" in [context]
A/1/2995
This feature fires when it sees semicolons, especially at the end of lines.
A/1/3016
[Ultralow density cluster]
A/1/1909
Code: Newline followed by spaces in XML
Cluster #162
A/0/202
This feature seems to activate when it sees parentheses used to enclose parameters in function and method declarations and calls in programming languages.
A/1/2823
Code: "()" in function calls or definitions
A/1/3461
This feature attends to brackets, parentheses and curly braces.
A/1/1311
Code: HTML closing tag before any start of text
A/1/2477
"\">"
Cluster #158
A/0/494
This feature seems to fire on HTML/XML tags and related code elements. It pays particular attention to opening and closing tags, as well as elements like paths, classes, properties, fields, methods, etc.
A/1/1910
Code: HTML Template Format
A/1/2909
Code: end double quote (in HTML attributes)
A/1/2965
" \u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n "/"\u23ce\n " in [context]
Cluster #145
A/1/872
Code: uint when it's likely uint8/uint16/uint32/uint64
A/1/951
Code: Go?
A/1/1087
Code: Function receiver types in Go
A/1/1529
Code: punctuation, esp. ending in ",", and spaces, esp. in Go
A/1/2303
Code: Whitespace, esp. in Go structs
A/1/2534
Code: 8/16/32/64 in uintX_t types in C
A/1/463
Code: Newline+spaces in doc-comments?
A/1/1654
"{" in [context]
A/1/2969
"static"
A/1/3235
":" in [context]
Cluster #178
A/0/19
This feature fires when it sees punctuation like commas, semicolons, parentheses and brackets.
A/1/229
Code: "," in argument lists?
A/1/472
The feature attends to punctuation such as commas and parenthesis.
A/1/663
This feature fires when it encounters paired quotes (single or double) with comma-separated values inside.
A/1/928
"\":"
A/1/1877
"," in [context]
Cluster #187
A/0/268
This feature attends to code structures like variable and function declarations, as well as code syntax like braces, semicolons, parentheses, arrows, and comparison operators.
A/0/322
This feature detects code, as it fires on code syntax such as curly braces, semicolons, indentation, function definitions, comments, etc.
A/1/89
Code: Newline and then spaces, esp after "{"
A/1/158
Code: Comments: Runs of spaces?
A/1/249
Code: Newline+indentation in Python?
A/1/395
This feature attends to token sequences indicative of function or method declarations and definitions in programming languages like C++, Ruby and Python.
A/1/662
" if" in [context]
A/1/852
Code: equal/not-equal conditions in C/C++ code
A/1/988
Code: Newlines and spaces after curly braces (open or close) in C/C++/Java/similar
A/1/1878
Code: " =" assigning a variable in a JavaScript/Java context (variable naming)
A/1/2057
Code: " =" assigning a variable in a C/C++ context
A/1/3023
" return" in [context]
A/1/3031
" -" in [context]
A/1/3299
This feature seems to activate most strongly when there are 4 or more spaces at the start of a line. This trend is most noticeable in examples 2, 3, 4, 5, 6, 7, 8, 12, 25, 32, 37, and 41. It appears that this feature is detecting indentation in code.
A/1/3320
Code: C++ streams
A/1/3607
Code: Newline followed by spaces (C/C++/Java/C#?)
A/1/3334
Code: identifiers close after tabs
A/1/699
" /"
Cluster #188
A/0/109
The feature fires for code in C languages based on whitespace, curly braces, semicolons, comments, function declarations, preprocessor directives, etc.
A/1/418
Closing parenthesis or bracket, possibly with other punctuation, in technical contexts but not code
A/1/1380
" //"
A/1/1557
Code: punctuation beginning comments like "//", "/*", or "#"
A/1/1937
Code: C (or Java?) *'s in doc-comments
A/1/3157
" #"
A/1/3259
"####"
A/1/675
Code: Long runs of asterisks in comments
A/1/3177
[Ultralow density cluster]
A/1/570
Code: "define"
A/1/2873
Code: " *" for pointers in C/C++ code
A/1/2402
Code: "java" in (file.java:line) errors/stack traces
Cluster #60
A/0/166
This feature attends to URL links.
A/1/1279
"." in [context]
A/1/1320
Code: " at" in Java stack traces
A/1/1367
"." in [context]
A/1/1503
"." in domain names, esp. email addresses
A/1/1589
":" in [context]
A/1/2255
"." in domain names in URLs, mildly open-source-flavored
A/1/3281
"." in [context]
A/1/3582
Code: "." in Java import paths?
A/1/3732
Code: "." in C# import paths?
A/1/4064
"/" in URLs
Cluster #197
A/0/118
The feature appears to fire on parentheses, arithmetic symbols, punctuation, whitespace and other programming/markup language syntax.
A/0/139
A/0/298
This feature attends to text formatting characters such as slashes, quotes, brackets, braces, and backslashes.
A/1/764
Code: punctuation near string manipulation and parsing
A/1/767
Code: variables/attributes predicting "="
A/1/817
Code: Assembly
A/1/996
%-based Format String
A/1/1535
Whitespace or English text near square-bracketed numbers/text
A/1/1641
Math entity prediction
A/1/1865
[Ultralow density cluster]
A/1/2990
Code: Statistics programming languages? (some Pandas, some others)
A/1/3434
Code: Haskell
A/1/3610
Regular expressions
A/1/3314
This feature fires on lines, dashes, underlines, and other horizontal dividers in text.
A/1/1423
[Ultralow density cluster]
Cluster #59
A/0/406
This feature attends to sentence features that include object-oriented programming concepts like classes, methods, constructors, functions, variables, parameters, arguments, etc.
A/1/110
Code: "." in long namespace/attribute member access chains (Java?)
A/1/481
" ." in [context]
A/1/1436
[Ultralow density cluster]
A/1/1696
"." in [context]
A/1/1738
"." in [context]
A/1/1775
Code: Function/method invocation in getter chains
A/1/2219
"." in [context]
A/1/2814
Code: "." in Java import paths?
A/1/3022
Code: "." as attribute access, accessing TitleCase members; likely Go
A/1/3318
"." in [context]
A/1/3539
Code: "." separating CapitalCamelCase identifiers (C#, Unity, etc.)
A/1/3615
[Ultralow density cluster]
A/1/3972
Code: "." sometimes with preceding punctuation in varying contexts
A/1/4087
Code: `').` in jQuery invocation
Cluster #33
A/0/326
This feature fires on code snippets, specifically identifier names, keywords, braces, semicolons, and other syntactical elements.
A/1/1418
Tabs near uppercase letters (stock tickers? code?)
A/1/3830
"\t" in [context]
A/1/3872
"\t" in [context]
A/1/3988
Newline after "-"-based horizontal rules
A/1/940
[Ultralow density cluster]
A/1/3192
"://"
A/0/178
This feature fires on Twitter usernames in angled brackets (< and >).
Cluster #204
A/0/414
The feature fires when it sees groups of characters in URLs that often represent file paths, folders, file types, domains and subdomain names.
A/1/1570
"@", usually in email addresses
A/1/1979
Code: '="' in HTML attributes, particularly <a href>
A/1/2683
"/" and some other separators in URLs
A/1/3338
Code: hyphens in hyphenated code identifiers
A/1/4043
Code: "/" in import-paths
Cluster #175
A/1/152
Code: ":" in CSS specifically
A/1/1208
Code: CSS alignment (predicting that a vertical/horizontal alignment follows)
A/1/1430
Code: hyphen in CSS
A/1/1935
Code: "-" in font spec in CSS
A/1/2545
Code: =" when setting width/height in html/css
A/1/3577
"=\"" in [context]
A/1/3509
Subdomains or domains in URLs (after "https://" etc.)
Cluster #216
A/0/20
This feature attends to opening and closing HTML tags.
A/1/121
Code: HTML angle brackets opening/closing lists
A/1/290
Code: HTML tag syntax that could/does enclose table-related tags
A/1/968
Code: HTML/XML beginnings of close tags "</", sometimes with preceding punctuation
A/1/1030
Starts of HTML tags: "<" or "</", sometimes with preceding punctuation
A/1/1645
HTML tag beginnings "<", sometimes with preceding punctuation, predicting "br"
A/1/3556
"</", sometimes with preceding punctuation, beginning HTML closing tags
A/1/4005
Code: "><" in XML?
A/1/4081
Code: HTML Angle Brackets before UPPERCASE
A/1/279
"<" in chat logs
Cluster #174
A/0/284
This feature fires on HTML, CSS, and other code-heavy syntax like indentation, brackets, semicolons, and hash symbols.
A/1/953
Code: CSS preprocessor?
A/1/1449
Code: Newline +/- spaces in CSS
A/1/2253
Code: image file path manipulation
A/1/2635
"_" for snake_case within androidmanifest.xml files
A/1/3003
Code: CSS padding
A/1/3218
Code: Punctuation in HTML properties
Cluster #215
A/0/265
This feature fires on the opening angle bracket '<' symbol, especially when it is at the beginning of a line or followed by a tag name, suggesting it is identifying HTML/XML tags.
A/1/607
Code: "</" in XML (with <string>, <key> tags)
A/1/1720
Code: Token ends with HTML open tag character
A/1/2540
Opening quotes in technical writing
A/1/3056
"<" in [context]
A/1/3955
"<" in [context]
A/1/2423
Code: Punctuation near packages and versions, esp. in Node
A/1/2416
Code: opening delimiters and/or quotes of any kind
A/1/1764
"=" in [context]
Cluster #231
A/0/133
The feature seems to fire on code in various programming languages, particularly around string literals and function or variable names.
A/0/177
This feature seems to fire most strongly for open/close parentheses.
A/0/409
This feature seems to fire on punctuation such as quotes, parentheses, brackets, commas, colons, semicolons, arrows, and hyphens. It also fires a bit on dollar signs and formatting such as newlines and tabs.
A/1/39
Code: opening quotes in data?
A/1/918
Code: punctuation near price calculations/processing?
A/1/1344
Code: Opening parenthesis in C-like for/if/while statements
A/1/1616
Code: "_" used in snake_case to join multiple words together, usually in code contexts
A/1/2179
Code: usually some opening delimiter + some quote, often handling basic website user info (PHP/Perl?)
A/1/2224
"[" in [context]
A/1/2262
Code: " $" as variable sigil
A/1/2274
"=\"" in [context]
A/1/3093
Code: "(" calling a function/constructor
A/1/3864
" @" in [context]
A/1/3210
[Ultralow density cluster]
A/1/3019
Code: Punctuation near file handling
A/0/168
This feature seems to fire on code snippets and paths, looking for things like class names, function names, file extensions, #includes, and library imports.
A/1/2087
Code: punctuation in authentication-handling (PHP?)
A/1/1335
[Ultralow density cluster]
A/1/925
[Ultralow density cluster]
A/1/339
"_" within citation-ish things, often followed by a year-of-publication
A/1/2310
Code: punctuation near multiprocessing and scheduling
A/1/2708
"::" in [context]
A/1/1069
Code: time manipulation
A/1/172
Code: Network programming?
Cluster #232
A/0/10
This feature attends to programming code. The feature fires more for syntax common in programming languages such as curly braces, parentheses, semi-colons, arrows, commas, hashtag/number comments, indentation, variable declarations, function calls, if/else statements, html tags, etc.
A/0/462
This feature detects hyphens between words that indicate compound terms.
A/1/1546
Code: :: token in C++ namespaces
A/1/1846
Code: C for low-level (circuit boards etc.) programming?
A/1/2231
"->"
A/1/2748
Code: Underscores in snake_case identifiers
A/1/3747
Code: Open parentheses, sometimes with more punctuation, in C-like languages
A/1/1669
"_" for snake_case within mathematical variable names
A/1/1007
Code: security key references
A/0/199
This feature appears to fire on tokens related to computer networking, such as IP addresses, TCP, TLS, SSL, HTTPS, ports, sockets, servers, certs, URLs, etc.
A/0/216
A/1/1450
Code: mostly punctuation in maybe function metaprogramming?
A/1/28
Code: making/manipulating HTTP requests and responses
A/1/1709
Code: " __" in special identifiers
A/1/3270
Words related to logging in/authentication
Cluster #141
A/0/358
This feature seems to attend to code snippets and programming-related text, looking for things like variable names, function names, keywords, and other code-specific syntax.
A/1/126
Code: Underscores in UPPER_SNAKE_CASE identifiers
A/1/209
[Ultralow density cluster]
A/1/477
Code: "_" in UPPER_SNAKE_CASE identifiers (registers, low-level C?)
A/1/896
[Ultralow density cluster]
A/1/1201
Code: CMake
A/1/2328
" \"" for opening (and sometimes "." for closing) an all-caps quoted sentence, that guides further all-caps
A/1/3347
Code: _ in C++ macro, especially after BOOST
A/1/2444
Code: JavaScript or web frameworks (React etc.?)
A/1/575
Code: CSS
A/1/2009
Code: Command lines / shell scripts
A/1/2473
Code: C++? Gtk/UI programming?
A/1/3100
[Ultralow density cluster]
A/1/644
Code: C++ include statements
A/1/371
English descriptions of character classes
Cluster #118
A/0/207
This feature seems to fire on code-like tokens such as variable names, function names, keywords, preprocessor directives, nested blocks, etc.
A/1/1102
Code: ALL_CAPS with underscores
A/1/1325
ALLCAPS Syscall names, predicting runs of spaces?
A/1/1455
Code: Uppercase letters in SNAKE_CASE identifiers
A/1/2133
Code: Uppercase letters in SNAKE_CASE identifiers (in C preprocessor?)
A/1/3485
Capitalized letters and punctuation near/predicting "P"
A/1/3589
This feature seems to attend to proper names or acronyms starting with the letter M.
A/1/3651
TitleCase word prefixes predicting -ex, esp. in code contexts, esp. "Mut"
Cluster #194
A/0/257
This feature fires when it sees web addresses and URLs.
A/0/262
This feature seems to fire on stylistic features such as backticks (`) and different kinds of brackets/parentheses including angle brackets (<>), square brackets ([]) and curly braces ({}). It also fires on short hyphens/dashes (-) and tildes (~).
A/1/44
Usernames in a chat transcript of some sort?
A/1/105
Words in URLs (excluding the "/" delimiters), focusing more on characters than numbers
A/1/360
"com" in [context]
A/1/540
Code: Various kinds of markup code (emails, html, cssy-stuff, misc data formats, etc)
A/1/1458
TLDs in URLS (com in github.com, org in wikipedia.org)
A/1/2089
Parts of domain names
A/1/2620
Path components after and predicting "/"
Cluster #198
A/1/289
"https"
A/1/2370
Code: Linux command line
A/1/3011
" lib"
A/1/3129
"http" in urls
Cluster #148
A/0/384
This feature seems to fire strongly on uppercase, camel case and punctuation such as hyphens. These all appear to be common in code, variable names, function names, etc. So this feature likely attends to stylistic features common in programming languages and code.
A/1/769
OCR Errors
A/1/2774
Text or word fragments, maybe medical
A/1/3010
" extends"
A/1/3797
[Ultralow density cluster]
A/1/3085
Nouns in computer programming, which maybe hold code/state
A/1/2121
Object-oriented programming (English prose)
A/1/938
Capitalized New Word Prediction
A/0/150
This feature fires on variable, function and class names in a variety of programming languages.
A/1/1263
The feature appears to fire on tokens that typically contain the letter v. The feature fires most strongly on occurrences of the letter v itself, but also activates some for words containing v, variable and function names including v, programming language identifiers starting with v, urls containing /v/, and hex color codes containing v.
A/1/3367
Code: identifier fragments esp. non-English, or sometimes typos
Cluster #202
A/0/256
The feature fires when it sees code comments, denoted by /* ... */ or // in programming languages such as C, C++, Java, etc.
A/1/34
Code: Comments: Nouns in systems/database engineering?
A/1/577
Code: documentation in markdown?
A/1/1600
Code: Capitalized words at beginnings of code comments
A/1/1672
Code-related English prose, e.g. API documentation
A/1/3382
Documentation around memory/CPU concepts (stack, pointer, structure, clock, size, etc)
Cluster #201
A/0/362
This feature attends to code snippets, specifically Python code.
A/1/751
Generic part of variable name
A/1/3848
Code: Names being imported in Python
A/1/3884
Code: Python imports
A/1/4089
Components of period-separated attribute/package paths
A/1/2147
Code: C style comments about low-level hardware
A/1/3444
TitleCase identifier components, esp. final ones, esp. with some abstract code meaning like "Class"
A/1/692
Code: predicting byte/buffer?
A/1/724
"include"
A/1/567
"import"
Cluster #65
A/0/386
This feature fires for numbers enclosed with symbols such as parentheses or brackets.
A/1/1642
Caesar-shift encoded words
A/1/2950
ASCII control characters
A/1/2389
"x" in "0x" hex literals with many leading 0s
Cluster #107
A/0/119
This feature attends to hexadecimal strings. It appears to fire most strongly on strings of length 6-8.
A/1/97
Long strings of digits
A/1/198
Lowercase hexadecimal (digit token)
A/1/1444
Hexadecimal, esp. lowercase letters, predicting digits
A/1/2025
Digits esp. after the decimal point in decimals
A/1/2048
List of Numbers (space-separated)
A/1/3629
"x" in hexadecimal literals
A/1/3817
Hexadecimal (uppercase)
Cluster #64
A/0/45
This feature fires when it sees base64 encoded text.
A/1/1493
Synthetic math: repetitive letter strings being picked "without replacement from"
A/1/1544
base64 of ASCII
A/1/2357
base64 (esp. uppercase letters)
A/1/2364
base64 digits
A/1/1961
Letters in short fragments of uppercase hexadecimal or some alphanumeric IDs
Cluster #106
A/0/283
The feature attends to numbers containing hyphens commonly found in phone numbers, dates, times, and item numbers.
A/1/327
Hour parts of timestamps, esp. with leading space and 0
A/1/1881
Numbers in various contexts e.g. error IDs
A/1/2215
Grant and project IDs, mostly digits with few capital letters
A/1/3807
2-digit time components separated by ":"s
A/1/4027
Numbers in hyphen-separated sequences
A/1/3941
Parts of phone numbers
A/1/1036
Digits in URLs
A/1/2859
Dot-separated version numbers
A/1/1573
File path components esp. related to installable software, packages, fonts
A/1/605
Space+number tokens in various technical contexts
A/1/3920
Code: Punctuation esp. at end of first line of class declarations
A/1/2945
Code: Objective C method declarations, with ":(" indicating a type will come next as part of a declaration
A/1/3020
Python-adjacent text/punctuation, esp. "#," preceding " python-format" in .po/.pot files
A/1/2998
Usernames in irc-style <username> format
A/0/153
This feature attends to HTML tags.
Cluster #150
A/0/0
This feature attends to HTML and XML tags.
A/1/173
Code: HTML attribute names
A/1/259
Code: Numbers/hexadecimals inside quoted strings for UX-related config/markup (XML?)
A/1/978
Code: HTML class name, particularly abstract layout-related like "container" or "child"
A/1/985
Android XML layout file properties
A/1/1082
Code: HTML/XML tag names in closing tags
A/1/1713
Code: Java/C++ types inside a template/generic instantiation
A/1/3457
Code: HTML/XML opening tag names
A/1/4031
" class" in [context]
A/1/4086
HTML tag names (generally after "<")
A/1/1879
Text in HTML or XML tags (predicting close tags)
A/1/3034
Code: Parts of strings in compact JSON
A/1/4069
HTML/CSS tutorials (prose)
Cluster #151
A/0/266
This feature attends to HTML programming code including HTML tags and attributes.
A/1/244
Mid-word quoted text, predicting closing quotes
A/1/1124
Double-quotes string as first argument to a function or method call
A/1/2175
Code: Single-quoted strings (often in an array, set, or function call)
A/1/3226
Words in string literals in JSON or similar in package metadata ("integrity", "metadata", "outputs")
A/1/1603
Code: jQuery methods
A/1/2095
Short text in double quotes, esp. in technical contexts
A/1/815
" click" / instructions for interacting with UIs
Cluster #192
A/0/209
This feature attends to the syntactic and structural features of SQL code. It fires on common keywords, symbols, and formatting in SQL such as SELECT, FROM, DISTINCT, *, ", (, ), \n, AS, etc. as well as code structure like indentation.
A/1/1618
Letter strings that predict "ID"/"id" (no space)
A/1/2060
Verbs and adjectives in discussions of tabular databases / spreadsheets
A/1/2164
Code: Testing-related
A/1/3518
Code: Visual Basic for Excel
A/1/3550
Code: SQL-related C/C++/Java code, constants, and comments
A/1/4010
Code: SQL
A/1/3825
[Ultralow density cluster]
A/1/2038
Code: Error conditions esp. "Failed" in error message string literals
A/1/1242
" var"
A/1/3012
Code: .Net Manifest XML?
A/1/1269
Code: related to IDs, users, accounts, and similar
A/1/2779
Code: Java CamelCase identifier constituents? ("Manager" "Application" "Bean" etc.)
A/1/3072
Code: Field names in JavaScript
Cluster #240
A/0/39
The feature attends to code-like structures such as identifiers, keywords and symbols and longer code-fragments in various programming languages.
A/0/211
This feature seems to attend to UI layout code for mobile apps, firing strongly on class names, methods, and UI element names commonly used when defining app layouts and styles.
A/1/236
Code: "get" in getter method names
A/1/668
Code: CamelCase identifier components esp after "get"
A/1/1026
Code: File Manipulation
A/1/1140
[Ultralow density cluster]
A/1/1318
Code: the start of a line of indented code that is operating on a object/class (vs control flow)
A/1/1749
Code: Java class/interface names in a class definition
A/1/1766
"nbsp" and some other HTML ampersand escape names
A/1/1844
Code: Date/time-related in Java/Objective-C languages
A/1/1876
Code: Android UX
A/1/2015
Code: CamelCase versions of UI/UX concepts
A/1/2107
Code: Objective C function/method calls on iOS
A/1/2532
Code: relating to processes and tasks and async execution
A/1/2607
Code: words in CamelCase, particularly prepositions: "To", "For", etc.
A/1/2886
Code: C# components of CamelCase identifiers (metaprogramming?)
A/1/4012
Spanish and Portuguese: Pseudocode or coding related words?
A/1/2892
Code: abstract-ish identifiers in object names, method calls, variable declarations
A/1/3228
Code: $-sigil-prefixed variable names (PHP/Perl?)
A/1/232
Code: some serialization or dump of networking info?
A/1/1606
"Exception"
A/1/1227
Code typically near string concatenation
A/1/2417
This feature seems to activate for code snippets and programming-related text. Some indications of this: - Frequent use of programming-related symbols and keywords like :=, void, int, def, public, etc. - Indentation and structure common in code, like braces { }, indentation, newlines, etc. - Frequent variable and function names like input, printf, copy, assign, etc. - Presence of comments like /* ... */ and // So in summary, this feature strongly attends to programming code and related text.
Cluster #243
A/0/388
The feature appears to respond strongest to code-like text and variable names, particularly shorter words like "var", "int", "mem", etc. It also seems to fire on function names, boolean values, and math operators.
A/1/20
Code: println and other method calls, esp in Java
A/1/3165
Code: PHP loop iteratees?
A/1/3684
Code: arguments in comma-separated argument lists
A/1/3644
Code: " String" in C++
A/1/2342
Code: C++ " private" and other keywords
A/1/1149
Code: Variable names after "var", likely JavaScript
Cluster #173
A/0/511
This feature attends to code and programming language syntax such as variable declarations and function calls.
A/1/186
[Ultralow density cluster]
A/1/454
Code: punctuation near geometry / graphics calculations
A/1/1762
Code: identifier components used in geometry / graphics
A/1/2306
" E" or other identifier prefixes or punctuation predicting "GL" in OpenGL contexts
A/1/4080
This feature attends to text dealing with lung diseases, specifically chronic obstructive pulmonary disease (COPD).
A/1/1995
Code: Components of CamelCase identifiers (in C/C++?)
A/1/1476
Code: Python (Machine Learning) (esp. punctuation)
A/1/2193
[Ultralow density cluster]
A/0/349
This feature attends to code blocks, particularly Python code. It fires on syntax like variable assignments, function definitions, class definitions, imports, strings, comments, and other Python-specific syntax.
A/1/2574
Code: Tree traversal (mix of Java, Go, Javascript, maybe others)
Cluster #225
A/1/430
" char"
A/1/837
" void"
A/1/1845
" unsigned"/"unsigned"
A/1/3286
" int"
A/1/2101
Code: identifier fragments, often after "_", related to data or dimension/shape
A/0/460
This feature attends to programming language syntax, particularly C/C++.
A/1/386
Code: C++ template type variables, esp. " T"
A/1/573
Code: " std" and other C++ namespaces (anticipating "::")
Cluster #207
A/0/140
The feature appears to fire on identifier names in programming languages. It attends to variable, function, and class names in code.
A/0/411
This feature fires on source code tokens related to declaring variables, functions, structs, etc. such as int, void, struct as well as punctuation like ; , ( ) { } and identifiers that look like variable names.
A/1/29
Code: Beginnings of components of snake_case identifiers
A/1/218
Code: short variable names?
A/1/447
" i" in [context]
A/1/1051
Code: space+letter tokens, often single-letter variable names)
A/1/2993
Code: C in hardware drivers
A/1/144
Code: C++ type-level programming
A/1/94
Code: Python Definition
A/1/3992
Code: " null" or sometimes " false" esp. after ":" or "return"
A/1/1760
Fields accessed with member-of-pointer (arrow operator "->") in C++
A/1/2468
" len"
A/1/899
" x"
A/1/3048
Code: argument name after "(" or "(&"
Cluster #92
A/0/305
This feature seems to fire on numbers with decimal points, especially when they are close to 0.5.
A/1/705
Code: Single digits, usually not numeric literals (e.g. in identifiers or versions)
A/1/718
Single-digit tokens esp. after "=", often some software flag or argument
A/1/3989
" 0" in [context]
Cluster #242
A/0/196
The feature seems to fire on string formatting in code, particularly left and right parentheses, string formatting characters like %s, and tokens associated with variable names.
A/1/409
Code: Identifiers after "(", likely arguments, in C#?
A/1/1729
Code: variable in if () condition
A/1/3033
Code: words referring to measurements or quantities: "length"/"width"/"height"/"count" etc.
A/1/3128
Code: End of parenthetical
A/1/1442
Code: Small numeric constants in parentheses (predicting close parentheses)
A/1/1379
" As"
Cluster #265
A/0/282
This feature seems to fire when it detects the use of comparative language such as "than", "except when", and "other than". It detects phrases that compare two things, either explicitly using "than" or "more/less than", or implicitly through use of "except" or "other than".
A/1/16
" than" in [context]
A/1/3849
" than" in [context]
A/0/351
This feature attends to the word "as" used as a conjunction.
Cluster #264
A/0/104
This feature fires when it sees the word "as" being used as a preposition or conjunction.
A/1/555
" as" in [context]
A/1/631
" as" in [context]
A/1/1978
" as" in [context]
A/1/3252
" as" in [context]
A/1/3666
" as" in [context]
A/1/963
Adverb in "as ... as"
A/1/1083
"As"
A/1/1390
" since"
A/1/3293
" until"
A/1/1686
" except"
A/1/3674
This feature fires when transition words like "furthermore", "moreover", "thus", etc are used.
A/1/3035
" while"
Cluster #253
A/1/1111
Express uncertainty with maybe/probably/perhaps
A/1/1784
" then" in [context]
A/1/2054
" Well"
Cluster #274
A/0/502
This feature seems to fire on words like "moreover", "because", and "however", which are used to signal logical connections in arguments. So in summary, this feature attends to argument structure and transition words.
A/1/622
" but" in [context]
A/1/1560
Sentence-starting word like "Moreover" and "Therefore" (predicting ",")
A/1/3525
" however" mid-sentence (after commas or semicolons)
Cluster #275
A/0/29
This feature fires when the word "if" is present.
A/1/592
" if" in [context]
A/1/2278
"if"
A/1/2406
"If"
A/1/3059
"though"/"although"/"while"/forms
A/1/3852
" Whether"/" whether"
A/1/3918
" when"
Cluster #263
A/0/116
This feature fires in response to transition words like "however", "but", "although", "though", "even though", "yet", and "until".
A/0/191
The feature fires for the word "And" as a discourse or stylistic marker to conjoin phrases or clauses.
A/1/355
" Or"/"Or" in [context]
A/1/1021
" but" in [context]
A/1/2088
" Nor"
A/1/2972
Sentence-starting conjunctions " And"/" But"
A/1/3304
" So"/"So" in [context]
A/1/2531
" because"
A/1/2699
[Ultralow density cluster]
Cluster #269
A/0/141
This feature attends to phrases that contain the word "that".
A/1/254
" that" Relative Pronoun, news
A/1/450
" that" Relative Pronoun, Scientific
A/1/604
" that" Relative Pronoun, Narrative
A/1/1177
" that" conjuction scientific
A/1/1250
" that" Indicative/Conjunction
A/1/1551
" that" Demonstrative Pronoun
A/1/1863
" which" in [context]
A/1/2645
" that" legal
A/1/4082
" which" in [context]
A/0/127
This feature appears to activate when surrounding text contains relative pronouns like "which", "who", and "whom".
Cluster #258
A/0/222
This feature attends to the interrogative pronoun "what".
A/1/372
" how"
A/1/752
"Why"
A/1/2102
" what"
A/1/2603
" What"/"What" in [context]
A/1/3555
"How"
A/1/1362
" That"/"That" in [context]
A/1/1484
" Oh"/"Oh"
A/1/501
" who"
A/1/263
" Which"/"Which" in [context]
A/1/3288
Affirmative utterances: "Yeah", "Yes", "Okay" etc.
Cluster #177
A/0/186
This feature is firing for interrogative sentences starting with “what”.
A/1/130
" can"/" could"/" would" after "How"
A/1/156
Word after "What" where "what" is the start of a phrase
A/1/1580
" you" as a question (often a yes/no question)
A/1/2122
Concepts/words in questions, predicting the end of the question "?", optionally with more punctuation
A/1/3626
[Ultralow density cluster]
Cluster #170
A/0/122
The feature attends to statements or phrases using the words "there is," "there are," or "here" to indicate existence, presence, location or emphasis.
A/0/394
This feature fires when the token is "it's" or "It's".
A/1/141
" it"/" It" in scientific contexts
A/1/600
" it" in [context]
A/1/618
Abstract nouns in "If"/"When" sentences, esp. computing or astrophysics related (e.g. "algorithm", "universe")
A/1/794
" it" in [context]
A/1/1610
" you" and other pronouns after "thought"
A/1/1776
" it" in [context]
A/1/1870
"It" in "It's", at the start of quoted speech
A/1/2168
" There"/"There" in [context]
A/1/2546
" there"
A/1/3152
" It"/"It" in [context]
A/1/3460
" Here"/"Here" in [context]
A/1/3366
Emphasis, specificity, or exclusivity
Cluster #189
A/0/24
This feature attends to first person plural pronouns.
A/0/73
This feature fires when the word "you" or variations of it ("your", "you've", etc.) are present.
A/0/171
The feature fires when it sees second person pronouns such as "you", "your", and "yours".
A/0/190
This feature attends to sentences with the pronoun "they", especially in contexts where "they" refers to people.
A/1/77
" authors" (and capitalization/spacing variants)
A/1/486
" We"/" we" in [context]
A/1/616
" You"/"You" in [context]
A/1/655
" they"
A/1/1218
" You"/"You" in [context]
A/1/1346
" you" in conditional "if" sentences
A/1/1793
" They"/" they" in [context]
A/1/2084
" we", in political punditry / media reviews?
A/1/2270
" we", esp. after a comma, in legal contexts
A/1/2493
"We"
A/1/2568
Words after "we"/"We", esp. adverbs indicating time or chronological position ("firstly", "then", etc.)
A/1/2827
"they"/"you"/"didn" predicting U+2019
A/1/2847
" it"/" she"/" he" after "if"
A/1/3014
" you" as subject of subclause
A/1/3310
" you" as direct object
A/1/3532
" We" in scientific contexts (royal "we found" etc.)
Cluster #190
A/0/154
This feature attends to the pronoun "she" and associated female pronouns (her, herself).
A/0/482
The feature fires when it sees pronouns such as "he", "she", "who", etc. in the context of the sentence.
A/1/650
" he"/" she" esp. in legal reports
A/1/745
" He"/"He" in [context]
A/1/1342
" She"/" she"/"she"
A/1/2069
"he"
A/1/2549
This feature attends to instances of the pronoun "he", referring to a male person.
Cluster #179
A/0/224
The feature detects first person singular pronouns and contractions such as "I", "I'm", "I've", "I'd", etc.
A/0/271
This feature attends to the usage of the word "I" and related pronouns like "I'm", "I've", etc.
A/1/52
" I" present tense
A/1/1725
" I" (or "I") at the beginning of a double-quoted statement
A/1/2241
" I", often in technical/troubleshooting contexts
A/1/2624
"I"
A/1/2640
" i", typically as a pronoun or in "iirc" in casual writing or chat
A/1/2715
" I" in a conditional(subjunctive?) sense: "when I", "If I", "because I", etc
A/1/2765
" I" at the beginning of a sentence
A/1/3487
" I", esp. in medical contexts as either the Roman numeral for 1 or start of an abbreviation
A/1/3676
'I' usually after 'but'
A/1/4022
" I" in a proper noun (usually?
A/1/4045
" I" past tense
A/1/874
" Which"/"Which" in [context]
Cluster #223
A/0/444
This feature appears to fire for sentences with mathematical questions or formulas that reference specific terms in sequences, such as the nth term.
A/1/201
Synthetic math: " replacement"
A/1/1350
This feature attends to mathematical terms and concepts such as highest common factor, derivatives, prime factors, and integer rounding.
A/1/1388
" nearest"
A/1/2681
" prime"
A/1/3682
" divided"
A/1/3922
Synthetic math: " first"/" second"/" third" before " derivative"
A/1/3167
Newlines before/after "True/False" in math problems
A/1/2834
Synthetic math: " is" in division problems
A/1/653
Numerical place ('thousands', 'tens', 'hundreds'), predicts digit, from math dataset
A/1/3397
" the" in "What is the..." etc. math problems
A/1/3280
"-" in [context]
A/1/735
[Ultralow density cluster]
A/1/4007
Synthetic math: ":" in multisets that are being sampled "without replacement from"
Cluster #224
A/0/96
This feature seems to detect math-related questions and phrases, like "What is..." and "Find the...". It looks for mathematical operations and concepts like derivatives, divisors, factors, roots, etc.
A/1/485
" to" in [context]
A/1/777
"of" in a Math context, for denominator/term/derivative "of" a number
A/1/1813
" What"/"What" in [context]
A/1/2609
" is" in [context]
A/1/2764
" without" in [context]
A/1/3554
" prob"
A/1/3652
" in" in [context]
Cluster #185
A/1/930
" (" for multiple choice answers in math; generic "(" in math in lower quantiles
A/1/1323
" ||" in wikitext tables involving dates
A/1/2214
Synthetic math: Spaces and closing parentheses before choices in math multiple-choice questions
A/1/2729
"⁄" U+2044 in normalized UTF-8 as latin1 mojibake
A/1/3602
[Ultralow density cluster]
A/1/2192
" and" in [context]
Cluster #203
A/0/505
This feature fires on arithmetic symbols such as the minus sign and equals sign.
A/1/1986
" +"
A/1/2216
"," in [context]
A/1/2308
" -" as minus sign in math
A/1/2349
" -" indicating a negative number in a math problem
A/1/2474
This feature seems to fire when mathematical operations and notation are present, such as determining values, calculating probabilities, solving equations, etc.
A/1/2500
" =" in [context]
A/1/3991
Verbs, esp. imperative, in (often synthetic) math
A/1/3693
[Ultralow density cluster]
Cluster #191
A/0/195
This feature fires for characters and patterns associated with mathematical expressions and equations such as parentheses, decimal points, division signs, exponents, equals signs, variables, numbers, and equations.
A/1/782
[Ultralow density cluster]
A/1/3343
"?" after non-words?
A/1/3969
"." in [context]
A/1/2856
" be" in [context]
Cluster #186
A/0/105
This feature attends to numerical formatting such as numbers, mathematical symbols and fractions.
A/1/1826
"](#" in [context]
A/1/2789
[Ultralow density cluster]
A/1/3188
Burmese: ?
Cluster #219
A/0/293
This feature seems to attend to mathematical equations and formulas containing variables, operations, and numbers.
A/1/2277
Synthetic math: "Let" introducing a math equation after a period
A/1/2753
" Suppose"
A/1/2207
")." as an end of a math formula sentence
A/1/14
Synthetic math: Open parenthesis in "Let a(b)..."
A/1/3865
Synthetic math: numbers being exponentiated (after "**")
A/1/1022
"sqrt"
A/1/1568
"**" in [context]
Cluster #220
A/0/149
This feature appears to fire for numbers and math symbols, particularly exponents, in the context of mathematical equations and expressions.
A/0/233
This feature attends to the phrases like 'let x = ...' and other statements of equations, variables, and mathematical expressions.
A/1/62
[Ultralow density cluster]
A/1/495
Synthetic math: Single-letter variables or functions
A/1/812
Single-letter variables in synthetic math esp. polynomials
A/1/847
Synthetic math: numeric constants before "*"
A/1/855
This feature attends to mathematical expressions involving variables.
A/1/1471
Synthetic math: numbers, about to be multiplied, after "Solve"/"Suppose"
A/1/2302
Synthetic math: digits between newlines (problem numbers?)
A/1/2572
[Ultralow density cluster]
A/1/2662
Synthetic math: space followed by digit usu. after "+" or "-"
A/1/3032
Year in Month Day Year format dates
A/1/3091
Synthetic math: numbers after single-letter keys "x:"
A/1/3387
Synthetic math: number after "Let x =" (where x is any single letter)
A/1/3400
LaTeX math-mode-ending "$" esp. shortly after "Let"?
A/1/3526
Single-letter variables in polynomials? (where exponentiation is **)
A/1/3568
This feature attends to basic arithmetic operations and equations involving addition, subtraction, multiplication and equal signs.
A/1/3573
")" in [context]
A/1/3584
Synthetic math: single-letter variables esp. after number and "*"
A/1/3762
Synthetic math: Single Token "*" in problems
Cluster #206
A/0/68
This feature appears to fire on mathematical expressions involving fractions, exponents, parentheses, variables, operators, and numbers.
A/0/324
This feature fires on mathematical expressions and equations involving multiplication, exponentiation and variable names.
A/1/46
"*" in [context]
A/1/70
Synthetic math: numbers esp. in sqrt()
A/1/528
"(-" as the beginning of parenthesized negative number in math
A/1/583
Synthetic math: Variable name inside the parens of a definition of a function in math
A/1/593
Synthetic math: "(" of a sqrt(number) call where number is numbers and not a variable
A/1/781
Synthetic math?: Numbers and variables in equations
A/1/1321
")/(-" or "/(-" in mathematical expressions
A/1/1689
")/"
A/1/2743
Newline-space in some serialization format? (YAML?)
A/1/2952
Synthetic math: exponentiation starting with "**" with optional "(" and "-"
A/1/3095
Synthetic math: minus sign in mathematical expressions
A/1/3276
")*(-"
A/1/3472
"/" in [context]
A/1/3617
Synthetic math: numbers in nested fractions
A/1/3743
Synthetic math: Expression after "Simplify"
A/1/3870
"/" in [context]
A/1/332
xAB hex escapes (without backslashes or other delimiters) near text
A/1/842
LaTeX punctuation after \sum or \prod (either subscript or "\" anticipating "limits")
A/1/1564
Consecutive numbers
Cluster #210
A/0/292
This feature fires for mathematical expressions involving integers.
A/1/315
Synthetic math: Arithmetic, numbers with small digits, in unusual bases
A/1/876
Numbers, esp. negative, in comma-separated lists
A/1/1599
Synthetic math: Numbers that we will need to answer whether it is composite/prime or divides other numbers
A/1/2019
Synthetic math: Numbers or fractions that are to be sorted
A/1/2489
Synthetic math: problem involving division
A/1/2867
Synthetic math: Numbers in math problems
A/1/3306
Mathematical sequences (numbers separated by ", ")
A/1/3354
Synthetic math: Numbers in math problems involving rounding
A/1/1441
"}{-"
A/1/3814
Code: fileID, maybe numeric values in "key:value" more generally
A/1/635
Double-pipe delimited data
A/1/1416
"**" in [context]
A/1/810
Numbers near Japanese and/or " -->" arrows
A/1/3305
Bug ids (mostly in URLs but also outside of URLs in context too)
A/1/3076
Numbers written in exponential notation (but not the "E", which is predicted)
A/1/2687
Markdown footnotes
Cluster #134
A/0/301
The feature seems to attend to headers of different sections in text, it especially activates on section headers containing numbers and horizontal lines/dashes below the headers.
A/1/693
"######"
A/1/728
Lots of dashes separating sections in LaTeX
A/1/2180
"================================"
A/1/2298
Legal discovery emails, esp. "@" in "ECT@ECT" (Enron emails)
A/1/2761
"Introduction"
A/1/3341
"!["
A/1/3514
"References" / other headers surrounded by (double) newlines
Cluster #102
A/0/110
This feature appears to attend to numbers.
A/1/10
Numbers after 3-ish newlines (legal contexts?)
A/1/2504
1–2 digit tokens, esp. after double newlines, sometimes after other punctuation
Cluster #135
A/0/347
This feature seems to fire on backslashes, quotes, hyphens, newlines, parentheses and other punctuation. It is particularly attentive to common punctuation around citations, code, equations, lists, tables, headings, and other text formatting.
A/1/1513
EOT at start
A/1/2754
EOT in middle
A/1/1033
Long runs of hyphens"
A/1/1807
US court case numbers
Cluster #128
A/0/87
This feature seems to fire when there are code snippets and programming language examples in the text, especially with programming keywords like "try", "catch", "method", "class", etc. or symbols like curly braces ({}), tabs/whitespace, backticks (`) and newlines (\n). There also seems to be a preference for C# and Java examples.
A/0/323
This feature fires when it detects the presence of a question followed by a new line, indicated by the \n token. The feature seems to specifically look for questions in a Q&A format, with the question preceded by "Q:" and followed by a new line. Some of the strongest activations occur when a question is formatted in this way: Q: \n [question] The feature also seems to fire on multiple new line tokens (\n\n), indicating a double space after the question. While the feature responds most strongly to typical phrasing of "Q:" followed by the question, it also activates to some degree for other question-related words like "Why," "What," etc. So in summary, the feature is detecting a specific formatting pattern of questions in a Q&A context, looking for "Q:", new lines, and the content of the question.
A/1/717
Q before a : at the start of a sequence
A/1/1726
Title-Case metadata-ish nouns like "Title", "Name", or "Location"
A/1/2740
"0" or some other numbers after <EOT>
A/1/4036
"External"
A/1/2163
US Court of Appeals
A/1/377
Legal docs: Citation numbers in square brackets
A/1/2721
Legal docs
A/1/449
Period in legal docs, especially conflict of interest?
Cluster #123
A/1/1927
".", sometimes with preceding punctuation, in legal contexts
A/1/2404
"." in [context]
A/1/3231
"." in "v." (legal cases)
Cluster #96
A/0/405
The feature fires when sentences end with punctuation (e.g. period, question mark, exclamation mark).
A/1/1653
"." in [context]
A/1/2311
Period in mathematical proofs
A/1/2584
"." in [context]
A/1/3425
"." in [context]
A/1/3036
"." in [context]
Cluster #122
A/0/250
This feature fires when it sees case law citations.
A/0/346
This feature fires on abbreviations that are followed by periods such as "S." and "e.g.".
A/1/125
"." after "Dr" and few other honorifics
A/1/741
"." in [context]
A/1/1061
"." in [context]
A/1/1206
"." in [context]
A/1/1432
"." in [context]
A/1/1548
"." in [context]
A/1/1697
"." in abbreviations for legal citations
A/1/1783
"." in legal/formal abbreviations?
A/1/2373
"." in [context]
A/1/2642
"." in [context]
A/1/3520
"." after single letters, likely abbreviations ("i.e.", "e.g.", "p.")
A/1/3950
"." in [context]
A/1/4035
"." in [context]
A/1/2794
"." after initials of people (in academic contexts / citations?)
Cluster #91
A/0/376
This feature attends to citations of journal publications in the format "<Last name>, <Initials.>, <Publication name>, *<Volume number>* (<Year>)."
A/1/1060
"." in [context]
A/1/1343
".," in [context]
A/1/1438
")." suffixing a year, in a bibliography/citation context
A/1/1608
"**]{},"
A/1/1640
" [**"
A/1/2079
"." in abbreviations of chemistry/physics journals?
A/1/3796
"." in [context]
Cluster #69
A/1/1045
" \u00a7"
A/1/1125
Numbers inside of [**NUM**]{} citations
A/1/2533
Page numbers in citations
A/1/2865
" \u00b6"
Cluster #78
A/1/2173
" See"
A/1/3138
Patent Numbers
A/1/4006
" also" of "See also"
Cluster #81
A/0/22
This feature attends to references to sections and subsections of legal documents, such as statutes, patent numbers, court cases, etc.
A/0/169
This feature seems to attend to text that shows dates and days of the week, often in news-like contexts such as headlines.
A/1/61
Legal: ends of case numbers
A/1/136
Legal numbers after " at"
A/1/183
" v" in "v." (legal cases)
A/1/789
Numbers in legal citations, usually after "," and predicting a court
A/1/1489
Section number (after "section" or "§", esp. after "Code")
A/1/1559
"d"
A/1/1659
Legal digit esp. after "F." before "d"
A/1/1761
Supreme Court case numbers
A/1/1806
" Id"
A/1/2132
" Cir" Legal Citation
A/1/2222
This feature fires on parenthetical citations with a year inside such as (2002).
A/1/2706
Months in dates, predicting days of months
A/1/3749
Space and digit after "X v. Y," court case citations
A/1/3755
" supra"
A/1/4059
Numerical day of a date (Month day, day Month, Month day'th, etc)
A/1/398
"." in [context]
A/1/787
"." in "Fig." (predicting a figure number)
A/1/1612
"." in [context]
A/1/1512
Supplementary data in papers
Cluster #94
A/0/378
This feature fires on the label "Fig." that usually precedes a figure number in academic papers.
A/1/278
" Fig"
A/1/1531
" Table"
A/1/2488
" FIG"
A/1/3723
" Eq" before an equation reference
Cluster #95
A/0/238
This feature attends to citations of figures in academic papers.
A/1/265
Short table names (" S" in "Table S1.")
A/1/282
"\"}"
A/1/362
"Fig"
A/1/3070
Figure numbers (after "Figure"/"Fig"/...)
A/1/370
"[", especially after "Fig. "
A/1/543
Letters in figure names that are parenthesized or bracketed, like "[Figure 4A]"
A/1/3675
Many hyphens
A/1/353
Chemical molecular data, showing atoms, bonds, and geometry, near many spaces and "---"
A/1/283
"/" as dimensional units "x per y"
Cluster #129
A/0/465
This feature attends to lines and horizontal lines in particular.
A/1/2051
Many hyphens
A/1/3868
Whitespace, often right after EOT
A/1/3880
Long runs of spaces in medical reports?
Cluster #26
A/0/160
This feature fires for square or curly brackets [ ] { }. The brackets are mostly used for citations, references, links, math expressions, arrays, lists, and occasionally for emphasis or indicating action.
A/1/473
"[" in [context]
A/1/1229
" (**"
A/1/2949
" [" in [context]
A/1/3291
" (["
A/1/702
_{ and ^{ in LaTeX
Cluster #25
A/0/229
This feature attends to parentheses () in the text.
A/1/457
" (" in [context]
A/1/536
Opening parenthesis after name or institution, where the parenthesis often contains a date
A/1/929
Open parentheses after more than one newline
A/1/1065
" (" in an informal narrative or commentary
A/1/1259
" (" Citation, Reference, or Note
A/1/1491
[Ultralow density cluster]
A/1/2276
" (" medical/epidemiological context with a focus on statistical analysis
A/1/2943
" (" in a math/physics technical paper
A/1/3064
"(" in a legal context referencing a particular clause: ie "924(c)(1)"
A/1/3365
" (" in a legal/judicial context
A/1/3449
" (" in [context]
A/1/3689
Open parentheses with no leading space, often in math notation or chemical formulas
A/1/3776
" (" in a medical/biological context with a focus on biological terms/parenthetical-acronyms
A/1/3844
" (" in a software-development/technical document
Cluster #21
A/0/52
This feature attends to acronyms in parentheses.
A/0/249
The feature attends to parentheticals, which are phrases or clauses set off by parentheses or other punctuation marks.
A/1/157
Letter after opening parenthesis, particularly "(i.e.)" or "(e.g.)"
A/1/292
Parenthesized numbers of quantities/units
A/1/768
Short lowercase word after "(", esp. in English prose mid-sentence
A/1/1457
Single lower case letter in parentheses
A/1/2204
"see"
A/1/2248
Years in parentheses, esp. citations
A/1/2296
Words inside parenthetical expressions that could be the end of the parenthetical phrase, predicting punctuation including ")"
A/1/2553
Text in parentheses, esp. proper nouns
A/1/2673
Parenthesized abbreviations (abbreviating the preceding text?)
A/1/2842
This feature activates most strongly when it sees the presence of mathematical functions and equations, signified by characters like parentheses, exponents, division signs, etc.
A/1/3088
"0" in [context]
A/1/3096
"," in [context]
A/1/3466
Words inside parentheses, esp. right after open parentheses, in medical/experimental contexts
A/1/4074
Numbered citations/footnotes
A/1/3990
" \u00b1"
Cluster #138
A/0/313
This feature attends to numbers with decimal points representing measurements such as weight or concentration.
A/1/2140
" **" in [context]
A/1/3162
"--"
Cluster #140
A/0/472
This feature fires for numbers formatted with spacing around the decimal points.
A/1/59
" \u23ce\n "/" \u23ce\n "/"\u23ce\n "/"\u23ce\n " in [context]
A/1/166
Long runs of whitespace near numbers/statistics
A/1/404
" "/" "/" "/" "/" "/" "/" " in [context]
A/1/726
Runs of whitespace in medical contexts?
A/1/1178
" "/" "/" "/" "/" "/" " in [context]
A/1/1453
Long spaces next to escaped arithmetic operators, potentially in tables?
A/1/3028
" =" in [context]
A/1/3808
" ×" in numeric dimension pairs
A/1/2377
"---"
A/1/1382
" "/" "/" "/" "/" " in [context]
A/1/3504
Capital letters and/or punctuation predicting 3-digit numbers?
Cluster #137
A/0/88
This feature seems to fire on numbers, punctuation like parentheses, commas, periods, colons, and mathematical operators like +, -, =.
A/1/88
Closing parenthesis after number predicting run of space
A/1/3567
Punctuation or tokens preceding "fn" referring to footnotes in [](){} citation format
A/1/134
" vs" / "versus" etc.
A/1/133
Hebrew?: mojibake UTF-8 as latin1?
A/1/712
" *" when used in sequence with additional "*" and " * tokens, in comments
A/1/3040
" *", usually as markdown bold delimiters
Cluster #139
A/0/400
The feature attends to tables and formatting, specifically numbers, text alignment, and punctuation.
A/1/1424
Numbers and punctuation near numbers, often "%)"
A/1/2424
Human body or medical supply related, near and predicting long runs of spaces
A/1/2781
" \\<"
A/1/3885
"*" in [context]
A/0/477
This feature appears to attend to text containing various symbols like parentheses, dollar signs, percent signs, asterisks, hashtags, tildes, etc. Often there is math notation or references to figures/tables.
A/1/819
"," in [context]
Cluster #136
A/0/67
The feature fires when it sees numbers.
A/1/1510
Digits, usually 2 or 3, after the decimal point in such numbers separated by many spaces
A/1/3467
" 0" in [context]
A/1/3756
Predicting whitespace after legal citations
Cluster #89
A/0/247
The feature fires when it sees dates formatted as two-digit to four-digit numbers representing years between 1900-2020.
A/1/869
A year, in talking about history and what happened
A/1/1634
Years, 19xx or 20xx, often in citations of journals and the like
A/1/642
"*" in [context]
A/1/834
ANOVA and related statistical tests
A/1/66
[Ultralow density cluster]
Cluster #131
A/0/94
The feature attends to numbers with decimals in scientific writing.
A/0/242
This feature fires when it sees statistical symbols and formatting, particularly p-values, standard error, and statistical significance indicators.
A/1/469
Numbers in percentages
A/1/709
Digits after the decimal point after "=" in parentheses
A/1/2793
Numbers after/near " ±"
A/1/3233
"%" in [context]
A/1/3887
" C" in [context]
Cluster #90
A/0/261
This feature attends to numbers between 20-50.
A/1/224
" million"/" billion" big numbers or things measured in such quantities
A/1/235
Various European languages: Number text
A/1/740
Page numbers esp. in page ranges
A/1/828
Numbers after Title-Case nouns suggesting sections of a larger work, like "Volume" or "Issue"
A/1/867
Single letters, esp. lowercase, immediately after numbers
A/1/954
Inline numbers in English text, esp. >2 digits predicting measurements/units
A/1/1126
This feature fires on numbers that contain a 1, 4 or 8 in the tenths or hundredths place.
A/1/1209
Digits in mixed digits/uppercase letter sequences (uppercase base36 maybe)
A/1/1862
Small single-digit numbers (as Arabic numerals)
A/1/1992
Numbers in English sentences
A/1/2026
~2-digit numbers (ages, page numbers)
A/1/2448
Numbers in Bible verse citations (separated by colons)
A/1/2634
This feature attends to numbers greater than or equal to 100.
A/1/2820
Numbers after " in"/" on" etc., predicting months
A/1/3703
Numeric dollar amounts, after "$"
A/1/3902
This feature fires when it sees numbers and letter combinations that appear to be vehicle, product or model numbers.
A/1/908
"*" in [context]
A/1/226
"^" in Markdown-style superscripts in chemistry?
Cluster #110
A/0/152
This feature attends to numerical values and measurements.
A/1/1183
Nuclear Magnetic Resonance
A/1/1954
" 0" in [context]
A/1/2100
Space + 2 or 3 digit number, inline in English prose (mechanical engineering?)
A/1/2337
~2-digit numbers in medical studies
A/1/2394
Numbers of units after the decimal point
A/1/2812
" 1"
A/1/2857
Numbers before acronyms/units in scientific papers
A/1/3284
1 to 3 digit numbers, often scientific measurements/units
A/1/2210
"~" in chemistry Markdown subscripts
A/1/533
This feature fires when it sees exponents, superscripts, and other mathematical notation.
Cluster #111
A/0/51
The feature fires when it detects scientific notation and chemical formulae.
A/1/154
Badly formatted math operators
A/1/816
~2-digit numbers in scientific contexts, predicting various units
A/1/1465
Numbers in Markdown ("~") subscripts near single capital letters (chemistry?)
A/1/2167
Code: double-asterisk-emphasized numbers and letters
A/1/2362
" \u00b0"
A/1/1162
" (" Preceding Roman Numeral
A/1/461
Code: HTML & escape character before lt/gt/nbsp/quot/amp
A/1/2877
Punctuation in JSON-like data, regexes... predicting single digits
Cluster #39
A/0/466
This feature attends to the ampersand symbol (&).
A/1/393
[Ultralow density cluster]
A/1/4013
LaTeX table (delimiters?)
Cluster #38
A/0/407
This feature fires on numeric values, including both integers and decimals.
A/1/388
Numbers in LaTeX tables
A/1/878
Whitespace esp. near/between LaTeX math mode expressions
A/1/1122
Base-ten numbers, especially decimal, negative, and in comma-separated lists without spaces
A/1/2351
Numbers and some English/abbreviations, separated by and predicting & (LaTeX?)
A/1/2393
Numbers in (X,Y) pairs? (coordinate-based plotting?)
A/1/1670
Punctuation tokens in math ending with "(" and near "x"
A/1/1991
Math notation, synthetic math operators, or LaTeX near the letter "w"
A/0/287
This feature attends to LaTeX code blocks.
A/1/488
Newline-spaces in LaTeX commands
Cluster #63
A/0/146
This feature attends to LaTeX math expressions containing fractions.
A/0/228
This feature seems to fire for dollar signs ($) used in mathematical formulas and equations.
A/0/303
This feature attends to mathematical expressions, especially those containing Greek letters, math symbols, and formulas.
A/0/340
This feature attends to subscripts that use curly braces such as C_{D}.
A/0/470
This feature attends to subscripts of mathematical variables.
A/0/501
This feature attends to mathematical equations and formulas that contain exponents.
A/1/167
"^{"
A/1/237
LaTeX: "-" in math mode (subtraction) esp. superscripts or subscripts
A/1/330
LaTeX: " =" assigning a variable
A/1/345
"{" in LaTeX after mbox/hbox/text/...
A/1/364
Synthetic math: operators near "f"
A/1/444
"{" in [context]
A/1/813
"(" and other opening delimiters after LaTeX "\left"
A/1/931
Backslashes predicting "u" in Unicode character escapes
A/1/1089
Synthetic math: parentheses and arithmetic near "c"
A/1/1181
"{" in [context]
A/1/1233
LaTeX begin inline math
A/1/1256
Predicting Short Math Variable
A/1/1328
"_{" in [context]
A/1/1754
Math punctuation near/predicting "n"
A/1/1768
LaTeX subscripts with preceding punctuation, predicting "i", "j", etc.
A/1/1923
" $" in [context]
A/1/1955
LaTeX: "_" used for subscript
A/1/1982
" $" in [context]
A/1/2263
LaTeX: Underscores and similar punctuation, predicting digits
A/1/2316
"^" in [context]
A/1/2405
"," in [context]
A/1/3180
"}{" in [context]
A/1/3248
Synthetic math: "*" in polynomials etc
A/1/3403
"{" in [context]
A/1/3529
Open parentheses in LaTeX?
A/1/3569
Parens, brackets, subscripts, and mathematical operators, predicting a variable "k"
A/1/3632
Mostly capital letter fragments predicting X, Y, Z
A/1/3889
"{" in [context]
A/1/3952
Operators and punctuation in math near/predicting "q"
A/1/3976
"=" in [context]
A/1/871
Letters near "+", often but not always scientific notation "1e+5"
A/1/1017
LaTeX esp. opening braces
Cluster #58
A/0/416
This feature fires on math equations written in LaTeX.
A/1/375
LaTeX: Curly brace in setting mathbb font
A/1/399
"}{" in LaTeX, esp. array environments
A/1/678
"{" in [context]
A/1/1664
"{" in [context]
A/1/1105
Punctuation in LaTeX
A/1/1275
LaTeX measurements
A/1/1135
Capital letters or punctuation near/predicting "G"
A/1/1947
"}" in [context]
A/1/107
LaTeX math environment names
Cluster #34
A/0/312
This feature fires on text snippets that contain LaTex syntax. In particular, it appears to be attending to structural elements like section headers, equations, references, and formatting.
A/0/316
This feature attends to LaTeX math notation.
A/1/55
"}" in [context]
A/1/273
"]{" in LaTeX
A/1/679
LaTeX: " \\"/"\\" in [context]
A/1/943
"}" in [context]
A/1/1076
"}" in [context]
A/1/1205
"}" in [context]
A/1/2381
"12"
A/1/2778
"}" in [context]
A/1/3490
"{" in [context]
A/1/3981
"was"
Cluster #47
A/0/329
The feature attends to LaTeX math environments, equations, and symbols.
A/0/418
This feature fires for LaTeX math mode environments, equations, theorems, tables, figures, algorithms, etc.
A/1/113
" [*"
A/1/1132
LaTeX equation tags
A/1/1283
Citations and references after (\[, predicting \])
A/1/1461
" (\\["
A/1/2205
Filenames in maybe Markdown link targets?
A/1/3543
"{" esp. in LaTeX labels
A/1/3578
" \\["
A/1/3587
":" in [context]
A/1/3660
Markup references to tables/algorithms/etc., in brackets before colons
Cluster #27
A/0/21
This feature fires when it sees brackets [] around numbers, letters and variable names.
A/1/396
Code: Single-digit numbers in square-bracket array indexing
A/1/3505
English in [square brackets]
A/1/2094
"$-", sometimes with prior punctuation, joining hyphenated LaTeX-math-and-word forms
A/1/212
"." surrounding sub/sup for subscript/superscript formatting requests
A/1/359
Data Label prefixes: basically of the form: .[]{data-label="
A/1/999
LaTeX tokens after open square brackets (left ends of intervals, etc.)
A/1/3275
LaTeX formatting
A/1/3863
"amsmath"
A/1/851
Code: identifiers (variables, key or field names) in curly braces
Cluster #292
A/0/259
This feature fires when it sees mathematical symbols and equations such as dollar signs, subscripts, fractions, greek letters, parentheses, etc.
A/1/402
"}$$"
A/1/638
This feature fires when equations (written in LaTeX or MathML format) are present in the text.
A/1/1507
"\\]"
A/1/2266
Closing LaTeX math mode
A/1/3912
This feature attends to mathematical symbols and formulas like variables, operators, functions, fractions etc.
A/1/1475
"$" in [context]
Cluster #291
A/0/40
The feature attends to mathematical notation such as inline formulas, equations, theorems, proofs, symbols, Greek letters, subscripts, and other math syntax.
A/1/175
" Let" in/near math or LaTeX
A/1/879
" Then"/" then" in [context]
A/1/3557
Prepositions near LaTeX math
A/1/3873
" that" LaTeX Scientific
A/1/3646
Numeric tokens, sometimes after all-caps abbreviations/identifiers
A/1/1474
'follows' in logic, usually related to code
A/1/549
" such" in [context]
A/1/437
Mathematical quantifier, e.g. " some"/" any"/" every" after "for", near LaTeX and predicting " $"
A/1/1195
This feature attends to mathematical formulas.
A/1/1268
" Proposition"/" Theorem"/capitalized words for mathematical statements
A/1/936
Prepositions and relational verbs, esp. at beginnings of sentences in LaTeX/math-related contexts, predicting " Eq" or math mode starting
Cluster #307
A/0/281
This feature attends to mathematical terminology and notation related to abstract algebra, especially homomorphisms, isomorphisms, and topological spaces.
A/1/491
Topology, abstract algebra
A/1/1661
Advanced physics (mostly GR, but some QFT)
A/1/2595
Abstract adjectives in math: " convex", " regular", " arbitrary", etc.
A/1/3740
Astronomical bodies and related phrases: stars, galaxies, dwarfs, pulsars, etc
A/1/3847
Adjectives in math / linear algebra: " linear" etc.
A/1/1932
[Ultralow density cluster]
A/1/2871
High energy physics
Cluster #308
A/0/84
This feature attends to technical language related to physics and astronomy, looking for words and phrases related to brightness, luminosity, energy, and associated units of measurement.
A/1/588
[Ultralow density cluster]
A/1/3711
Galactic astronomy (prepositions)
A/1/1950
" between"/" through"/similar prepositions in electromagnetics/quantum physics?
A/0/103
The feature fires in response to usage of the preposition "by" in contexts describing mathematical or scientific processes.
A/1/1555
Abstract verbs or endings, as might be done to equations or expressions, in math / theoretical physics? (" solve", " compute" etc.)
A/1/3429
" of" in [context]
Cluster #293
A/0/239
This feature seems to attend to mathematical expressions, formulas, and equations.
A/1/1626
" Note"
A/1/2979
Math nouns, predicting inline LaTeX
A/1/3090
This feature appears to activate when numbers or mathematical symbols appear in the context of equations, matrices, vectors, norms, spaces, parameters, eigenvalues, functions, etc.
A/1/3542
" theorem", " section", and other parts/units of math papers
A/1/3761
The feature appears to fire on symbols and terminology associated with mathematical expressions, equations, operators, and concepts.
A/1/3975
[Ultralow density cluster]
A/1/4054
Abstract nouns in mathematics after "the"
Cluster #295
A/0/318
This feature fires when it sees the word "the" in the token immediately preceding a technical term.
A/0/341
This feature seems to attend to mathematical expressions involving the word "the". Many of the highest activations contain math equations, formulas, or terminology with "the" present.
A/1/512
" a" in the context of math, expected to be followed by a math word. E.g. "a subset" or "a Hilbert space"
A/1/1078
" the" in physics, esp electromagnetism?
A/1/1452
" the" in theory of machine learning / statistics
A/1/1638
" the" in math (advanced geometry? algebra?)
A/1/1652
" The" in [context]
A/1/1780
" the" in astrophysics?
A/1/3362
" the" in mathematics/physics/dynamical systems?
A/1/2183
Astronomy, especially solar-system
A/1/976
" a" in physics or other sciences contexts
A/1/3503
Famous mathematicians, predicting things named after them like " theorem"
A/1/1637
" for" in [context]
A/1/2395
" such" in [context]
A/1/1811
The feature fires when it sees scientific terms or concepts.
Cluster #304
A/0/357
This feature attends to words related to Coulomb interactions in a physics context.
A/1/128
Physics relating to motion and change: speed, rising/falling, developing/unstable/steady, etc
A/1/342
" electron"
A/1/1558
" of" in [context]
A/1/2878
Key actions, operands and logical constructs that are part of statistical analysis / modeling (prose)
A/1/3502
" with" in math (esp. geometry?)
Cluster #285
A/0/315
This feature attends to hyphens appearing in scientific writing.
A/1/216
Abstract nouns like "stability" in numerical systems, astrophysics, etc.
A/1/823
Graph theory (esp. adjectives)
A/1/3158
Nouns in math, esp. linear algebra ("matrix", "vector")
A/1/3435
Neuron networks/machine learning (prose)
A/1/1164
" function", esp. in biology
A/1/541
Physics-related noun of a particular flavor, e.g. "curvature", "velocity", "lattice", "transition"
A/1/341
This feature attends to scientific terminology related to dynamics, structures, gradients, perturbations, equations, behaviors, and parameters.
Cluster #277
A/0/16
This feature fires on word endings that describe geometric shapes and locations.
A/0/210
This feature fires when it sees technical terms and concepts commonly found in physics and engineering documents, particularly those related to optics and lasers.
A/1/65
" wave"
A/1/322
Physics (Optics)
A/1/2112
" horizontal"/" vertical" geometry?
A/1/3733
Graphics processing?
A/1/2134
" respectively"
A/1/118
Statistics (English text)
A/1/272
Abstract nouns esp related to paths in astronomy
Cluster #16
A/0/63
This feature fires when the word "according" is followed by the word "to".
A/1/890
" according" in [context]
A/1/1403
"According"
A/1/2064
" according" in [context]
A/1/1518
Graph/diagram-related nouns (curves, lines, squares, diagrams)
A/1/2744
Functions of mechanical systems / devices
A/1/800
" invention"
Cluster #276
A/0/31
The feature attends to text related to semiconductor transistors with respect to their components and fabrication.
A/0/129
This feature attends to technical language describing the physical structure or form of mechanical objects.
A/0/306
This feature attends to descriptions of rotating mechanical parts such as shafts, knobs, axles, and gears.
A/1/365
Prepositions and abstract verbs in mechanical engineering? metalworking?
A/1/521
Printing / recording / photographing
A/1/685
"relates to" (and similar phrases) in patent filings.
A/1/915
HVAC
A/1/1319
English nouns for mechanical components, predicting numbers (patents?)
A/1/1742
Semiconductors / Computer Manufacturing
A/1/2127
" crystal"
A/1/2485
English (Signal Processing)
A/1/3074
Material science phrases
A/1/3227
Electrical engineering
A/1/3792
Engineering devices (circuit, lamp, disk drive, instrument, device)
A/1/3901
Adjectives describing geometries of mechanical connections
A/1/3230
Concurrency, multiprocessing?
Cluster #259
A/0/81
This feature fires on text talking about technical concepts related to electronic communication, signal processing, and data transmission.
A/1/233
CPU architecture
A/1/307
Computer networking concepts
A/1/1841
Connector words in electronic storage/protocols
A/1/2066
Wireless networks
A/1/2096
" surface"
A/1/1166
Nouns or word fragments in biology, esp. animals or fungi
A/1/3631
" needle" and broadly similar nouns in sewing/crafting
A/1/2907
Abstract nouns in medicine and physics, often related to filtering
A/1/1301
Generic, inspecific terms for mechanical, biomechanical, and electronic components
A/1/1034
Words in scientific text, esp. atomic chemistry
A/1/3322
"aid" in "Chinese/Japanese Patent [Application] Laid-Open"
Cluster #272
A/0/254
The feature appears to fire on words and phrases related to chemical processes.
A/1/373
Excerpts from technical documents describing polymer chemistry processes and properties
A/1/2910
" with"/" containing" in chemistry
Cluster #260
A/1/241
"xe"
A/1/385
Phrases around chemical substances
A/1/1567
Chemical abbreviations
A/1/181
Chemistry " pH", " temperature", etc.
A/1/3287
" results"
A/1/3261
Statistics-related nouns or endings thereof, in "######" section titles
Cluster #312
A/0/218
The feature attends to words and phrases commonly found in sections describing the results of scientific papers.
A/1/3214
" study"
A/1/3253
" studies"
A/1/4076
" difference"
Cluster #311
A/0/83
The feature attends to words referring to different types of cells.
A/0/179
The feature attends to text describing figures and plots.
A/0/321
The feature attends to words and phrases related to cell death and injury.
A/1/159
This feature fires when it sees words related to chemical processes, such as polymerization, oxidation, precipitation, elution, etc.
A/1/336
The feature fires on tokens related to biological or medical samples.
A/1/997
Mostly " cells" and similar terms
A/1/1070
" Expression"/" expression"
A/1/1144
" age"
A/1/1434
" group"
A/1/1981
The feature fires when it sees words related to time periods or durations such as 'after', 'during', 'weeks', 'months', etc.
A/1/2510
This feature attends to words and phrases related to growth, proliferation, and increase.
A/1/2951
The feature fires when it sees words related to genes and genetic concepts.
A/1/3324
Some nouns esp. in medicine/science, but pretty varied
A/1/3332
Biomedical techniques
A/1/3470
Blood test results (as levels or concentrations)
A/1/3948
Abstract plural nouns in broadly technical/scientific contexts
A/1/4071
This feature attends to words related to research, diagnoses and treatments for diseases and disorders.
A/1/2321
[Ultralow density cluster]
A/1/1412
" population"
A/1/1840
The feature fires when it follows words that end with "s".
Cluster #286
A/0/49
This feature seems to fire on words related to measurements, including units of measurement and words describing amounts. For example, it often fires on words like "temperature", "pH", "concentration", "volume", "score", "sensitivity", "specificity", "latency", "rate", "flow", "velocity", "weight", etc. as well as numerical values.
A/1/98
" count"/" phase" in various contexts
A/1/3772
Mechanical engineering performance
A/1/1086
" patients"
A/1/3750
" disease"
A/1/3131
Biology terms
A/1/907
" treatment"/" therapy" medical nouns
A/1/3760
Unit of time in days/months/years (but not minutes/hours)
A/1/1405
Demographic information stratified by age
A/1/1933
Synthetic biology
Cluster #317
A/0/41
This feature attends to wording related to perceptual and cognitive processing in scientific and research writing.
A/0/145
The feature attends to parenthetical information (e.g. numbers in parentheses).
A/1/482
"%" or verbs in stats from medical trials?
A/1/811
Medical imaging, esp. abbreviations like " CT"/" MR"
Cluster #169
A/0/108
The feature seems to be attending to biological and scientific language describing experiments, results, and processes related to cells, including activation, signaling, proliferation, inflammation, expression, phosphorylation, suppression, upregulation, pathways, responses, and treatments.
A/1/22
+ and - in T cell names? (Markdown-superscripted "^+^"/"^-^")
A/1/182
[Ultralow density cluster]
A/1/849
"**" markup in biology/medicine?
A/1/1237
" although"
A/1/2519
Verbs related to biology / cell processes
A/1/3442
Biochemistry
A/1/3533
Prepositions in scientific/statistical contexts
A/1/2358
" chronic"
A/1/3964
Pregnancy
Cluster #318
A/0/387
The feature appears to fire on words related to anatomy, in particular anatomy of the abdominal and chest region. It attends to words referring to surgical procedures in those regions as well as anatomical structures such as arteries, tendons, and organs.
A/1/970
Sentences relating to cardiovasciular anatomy and related conditions
A/1/2269
Adjectives in neuroscience?
A/1/3186
[Ultralow density cluster]
A/1/3201
Prepositions / abstract verbs in medical/anatomical contexts
A/1/3821
Medicine (especially surgery)
A/1/3078
Cancer
Cluster #24
A/0/221
This feature attends to numerical measurements including volume and concentration, as well as experimental procedures in biology such as staining and incubation.
A/1/25
Antibodies (maybe antibody assays?)
A/1/1420
Bio/biochem laboratory terminology (samples, buffers, solutions, etc)
A/1/1707
Life sciences company names with "Technologies" in them
A/1/1930
"%" in chemistry contexts
A/1/2212
This feature attends to scientific notation, separators, and punctuation frequently found in scientific literature such as research papers.
A/1/3272
Biological science assay kits
A/1/3540
" After" and similar connectors in biological contexts (cells etc.)
A/1/2686
Neuroscience
A/1/707
Fitness
Cluster #310
A/0/291
This feature appears to fire on words related to science and medicine, particularly biochemistry, nutrition, and digestion. It seems to be looking for technical terms and jargon related to health, diseases, treatments, and research, as well as measurements and chemicals associated with these topics.
A/1/211
Medicine delivery in trials?
A/1/508
Text about pharmaceuticals
A/1/1505
Diabetes diagnosis
Cluster #168
A/0/431
The feature seems to fire on content words, especially nouns, in scientific papers that describe biochemical processes. It is particularly attentive to words related to molecular interactions and binding.
A/0/443
This feature fires on text related to genome analysis, in particular mentions of genetic sequences, homologous genes, PCR, variants, transcripts, etc.
A/1/947
Biochemical interactions
A/1/1552
[Ultralow density cluster]
A/1/3572
Biology / genetics?
A/1/3182
Adjectives in biology / medicine
A/1/3641
English abstract nouns about psychology, behavior, etc.
A/1/1088
Virology studies and infectious disease, discussing viruses, medical conditions, and protective measures
Cluster #302
A/0/38
This feature attends to biological and biochemical concepts, particularly related to cells.
A/1/557
This feature attends to references related to biological and biochemical concepts, such as molecules and substances found in the body.
A/1/1127
"cell", mostly as token and somewhat as in biology
A/1/2440
The feature attends to words and concepts related to tumors and cancer.
A/1/2525
Adjectives or word endings like "infected", applied to cells, in medicine/biology
A/1/2632
" gene"/" DNA"/" RNA"/" protein"
A/1/2805
" Ig" esp. in antibody names
A/1/2837
Biological and cellular activity/effect
A/1/3237
Cell types/adjectives in biology
A/1/3768
" kinase" etc in biology
A/1/4092
This feature fires on examples related to biological processes, cellular biology concepts, and biochemistry jargon. Some key examples it picks up on include cellular viability, immunoglobulin, metabolism, morphology, and characterization. It also seems attuned to biological methods and techniques like electrophoresis, chromatography, and microscopy.
A/1/3611
Nutrition
A/1/287
This feature fires when it sees words or word parts related to chemistry, especially organic chemistry, such as enzyme and substrate names, chemical reaction and process names, etc.
Cluster #296
A/1/759
" brain", " liver", and other organs
A/1/2099
" immune"
A/1/3238
Medical, organ-related adjectives (pulmonary, renal, cardiac)
Cluster #289
A/0/392
The feature attends to concepts related to healthcare, education and training. It picks up on words and phrases associated with these topics such as training, nurse, mentor, continuing education, patient, health education, intervention, clinical, physician, treatment, specialist, academic interests, professional organizations, providers etc.
A/1/2005
Hospital / health care administration
A/1/2565
Mental health
A/1/2921
Surverys and related concepts
A/1/3050
Words after hyphens in hyphenated phrases, esp. in medicine
Cluster #298
A/0/327
This feature seems to fire most on Latin words, Latin abbreviations, and Latin scientific names.
A/1/93
Biological genera
A/1/861
Endings of Latin anatomical terms / diseases
A/1/1019
Medical: drug names
A/1/1575
Phrases related to plants and agriculture
A/1/2289
"ide"/"ine" at ends of words, often chemistry or medicine
A/1/2486
"in" in [context]
A/1/3292
Skincare
A/1/109
Names in author lists, particularly ends (predicting " et" of " et al.")
Cluster #261
A/0/476
This feature attends to chemistry/biochemistry related terms containing substructures like propyl benzyl and sulfonyl groups.
A/1/192
" hydro"
A/1/753
Tokens, often ending in "z", in long chemical names
A/1/2665
" methyl"
A/1/2978
Tokens in long chemical names with parentheses, brackets, and hyphens
A/1/3118
Words in organic chemistry, esp. predicting "alkyl" or "aryl"
Cluster #299
A/0/188
This feature seems to be attending to descriptions of living things, especially animals. It is particularly activated by words having to do with animal behavior, appearance, habitat, etc.
A/1/2279
Horticulture
A/1/2524
Transitive verbs and prepositions related to biology/marine sciences
A/1/2994
Adjectives/verbs describing animals
Cluster #290
A/0/86
The feature attends to words or phrases related to relationships, both romantic and familial.
A/1/2554
Legal descriptions of child abuse
A/1/3282
Sex (general) / queer text
A/1/2359
" oxygen"/" carbon"/chemical elements and similar
A/1/2484
Emergency and hospital services and phrases
A/1/1716
" degrees"
A/1/471
Education, Schools
A/1/3373
Knitting (+tailoring?)
A/1/1173
" with" and other connectors near color/texture/shape adjectives
Cluster #297
A/0/486
This feature fires on measurements and cooking instructions.
A/1/424
" the" in recipe instructions
A/1/835
"," in recipes
A/1/850
Connector words in recipes
A/1/1273
Recipe Name (Title Case)
A/1/1649
Recipe ingredient lists
A/1/2455
Numbers, often in fractions, of measurement in recipes
A/1/3337
Recipe text
A/1/3706
Food reviews
A/1/2383
" care"
A/1/1609
Words for organisms or living structures, esp. "bacteria" or plural
A/1/1179
Words/tokens, often " crypt", often in cellular biology or other sciences
A/1/2509
" risk"
A/1/1085
" method"
A/1/1157
The feature fires when it encounters the word "These" at the beginning of a sentence.
A/1/3783
The feature fires when the word "for" is followed by a word related to an analytical or scientific purpose such as determination, detection, estimation, measurement, planning, modeling, study, analysis, formulation, measurement, planning, processing, etc.
A/1/1295
" confidence"
Cluster #358
A/0/155
This feature seems to fire on words and phrases related to business operations and processes. Some examples include marketing, optimization, management, efficiency, identification, testing, inspection, collaboration, and planning. There is a particular focus on search engine optimization.
A/1/1355
" benefit"
A/1/3386
Corporate buzzwords
A/1/3606
" research"
Cluster #359
A/0/442
The feature attends to concepts related to techniques, methods, programs and systems.
A/1/160
Abstract nouns in biology and technology related to testing, support, etc.
A/1/1174
" issues"/" problems" in formal English
A/1/3879
" etc"
Cluster #99
A/0/156
This feature attends to the use of the word "using" in English text.
A/1/162
Code: comments: web programming questions
A/1/576
Conjunctions?
A/1/877
" Some"/" some" in [context]
A/1/1255
" example", often after "for"
A/1/1410
Words shortly after I / you / aux. verb + "be" / contraction + "have"
A/1/1975
Installation/build tool instructions that refer to directories
A/1/3585
Verbs like "convert"/"add"/"remove", often targeting data, esp. present-tense or infinitive
Cluster #348
A/0/352
The feature fires when it sees words related to data and information.
A/1/2862
" material"/" content"
A/1/3401
Scientific data
Cluster #344
A/0/197
This feature attends to descriptions and uses of systems and models.
A/1/706
" area"
A/1/1155
Nouns related to paths, shapes, and movement
A/1/1425
" point"
A/1/2669
" line"
A/1/4061
" form"
A/1/2267
" error"
Cluster #356
A/0/113
This feature appears to attend to mentions of UI elements and programming concepts like functions, objects, variables, tables, and code snippets.
A/1/239
Abstract nouns in prose discussion of code
A/1/1680
Programming-related nouns esp. pertaining to web requests
A/1/1791
" version"
A/1/1837
Data format or handling tool after "from"/"in"
A/1/2802
" file"
A/1/3211
Abstract nouns esp kinds of data or abstractions in code
A/1/3746
Abstract data-holding nouns in programming
A/1/3426
" information"
Cluster #355
A/1/565
" class" in [context]
A/1/1509
" text"
A/1/1883
[Ultralow density cluster]
A/1/1945
" link"
A/1/2674
" system" and similar abstract nouns, esp. in computing contexts
A/1/2104
" switch"
A/1/2880
"photo", most likely mentions of "photo" in caption in the news articles
Cluster #343
A/1/982
[Ultralow density cluster]
A/1/1996
This feature fires when there are words related to modification or descriptors such as '-ing' endings. It also activates for words related to creation or development.
A/1/3843
Comma-delimited lists of items
A/1/2268
Words in technical contexts with manual line breaks?
A/1/3313
" search"
A/1/1254
Abstract provisions, often after "provide" or "has"
A/1/2946
"attempt" to do smth
A/1/460
" account"
A/1/2626
" test"
Cluster #346
A/0/163
This feature attends to words and concepts related to taxation and finance.
A/1/206
" business" and related nouns or synonyms
A/1/2350
" money"
A/1/2992
[Ultralow density cluster]
A/1/3136
" estate"/" property"/...
A/1/3837
" parking"
A/1/4034
" credit"
A/1/4040
" tax"
A/1/2535
Economic terms and phrases
A/1/1467
" security"
A/1/1682
" boot", esp. in computing/OS contexts
A/1/505
Climate Change
Cluster #326
A/0/204
The feature attends to sentences with numbers including percentages and dollar amounts.
A/1/301
Economic discussion of tax and subsidy policy
A/1/3441
Economics / Business
A/1/3437
Word fragments, esp. ends, faintly construction-related ("ver" in "cantilever")
A/1/3621
Word endings in medicine
A/1/4072
Software names
Cluster #230
A/0/212
This feature attends to long technical terms, units and some domain specific symbols.
A/0/217
The feature attends to end-to-end workflows that include information about the service provider, specific steps, and goals of the workflow.
A/0/436
The feature fires when it sees names of technologies and software, such as operating systems, browsers, programming languages, apps and tech companies. For example, it activates on words like Android, Windows, Chrome, Swift, Java, Apple, Microsoft, etc.
A/1/48
SEO tips
A/1/85
Physical goods commerce
A/1/129
Fantasy games (e.g.. WoW, MTG)
A/1/684
Gambling
A/1/881
Topics relating to books (both "cover" and sketchbooks and guides and records, but also ebooks, paperwhite, and PDFs)
A/1/1376
CPU specs
A/1/1656
Smartphones
A/1/2185
Cybersecurity
A/1/2187
Electronic devices (cameras, phones, TVs)
A/1/2695
" web" (almost always as a single word
A/1/2831
" email"/"mail" in "e-mail" and other mail-related text
A/1/2915
Video games
A/1/3039
" software"
A/1/3189
Operating systems and similar platforms
A/1/3193
Context: offering a sale/discount
A/1/3549
Enterprise or resume language
A/1/3623
Technology hardware review?
A/1/3624
" computer"
A/1/3944
Cryptocurrency
A/1/3701
Specifications or ad copy for physical devices / equipment
A/1/519
[Ultralow density cluster]
A/1/2901
" figure"
A/1/3430
Physical objects in construction/architecture/mechanical engineering
A/0/274
This feature fires when it sees terms and phrases related to specific parts of the home such as a garage, alley, trailers, ceilings, walls, and floors.
A/1/515
[Ultralow density cluster]
A/1/1931
Sentences about automatives and cars
A/1/661
Some mid-word tokens predicting plural endings like "ers", broadly related to hygiene, medicine, firearms, or machinery
A/1/760
[Ultralow density cluster]
Cluster #61
A/0/227
This feature appears to attend to various types of medical and biomedical terminology. It activates on words and phrases related to medical conditions, treatments, procedures, diagnoses, anatomy, and other biomedical concepts.
A/1/946
Nouns, esp. abstract and related to medicine or illness
A/1/1699
Capitalized words shortly after EOT
A/1/1736
Only vaguely software-related verbs, but predicting HTML/JS-related words?
A/1/3809
Plural nouns / endings thereof, vaguely related to tourism
A/1/2071
[Ultralow density cluster]
A/1/151
" force"
A/1/413
[Ultralow density cluster]
Cluster #342
A/0/72
The feature attends to scientific and medical terminology.
A/1/410
" motor"/"Motor"
A/1/827
" electric"/" magnetic"
A/1/1040
" nuclear"
A/1/1397
" medical"
A/1/1464
Climate change
A/1/1788
Biological/genetic adjectives
A/1/2228
" health"
A/1/2903
" visual"
A/1/3213
" mental"
A/1/3777
" human" and some other species/nouns in medicine and social sciences?
A/1/3905
"supplementary"
Cluster #233
A/0/5
This feature looks for words that have hyphens in them, as well as some compound words without hyphens like "offline" and "firstname."
A/0/232
This feature attends to words ending in "-able", "-er", "-ist" and other similar suffixes that turn a verb or adjective into a noun that performs an action.
A/1/27
Word components, often near word ends, often of professions or "Xer" words that mean "people who X"
A/1/335
This feature fires when it encounters words that end in -er, -ar, or -or.
A/1/376
This feature fires when it sees words that end in -off, as well as the word "link", "stop" or "screen". It seems particularly focused on two word phrases where the second word ends in -off, like "back-off", "stand-off" or "pick-off".
A/1/969
[Ultralow density cluster]
A/1/1501
This feature seems to fire for words that contain "over" in the middle, especially when they are location names like Hanover-square and Durnover Moor.
A/1/1918
Mid-word tokens, esp. in medieval/fantasy contexts
A/1/2317
Words after hyphens in hyphenated phrases, esp. adjectival phrases?
A/1/2769
Text near soft hyphens
A/1/3147
Mid-word or ending tokens
A/1/3254
[Ultralow density cluster]
A/1/3608
Short letter sequence token after hyphen in hyphenated phrase
A/1/3680
This feature seems to attend to words ending in -er or -en.
A/1/3799
" social"
A/1/40
End-of-word "ic", esp. in chemical/biological contexts
A/1/2353
Large bodies of water and related words: rivers, coasts, oceans, etc
A/1/2518
" olive" esp in recipes
Cluster #345
A/1/677
" urban"/" rural", predicting types of areas
A/1/1905
" agricultural"/" industrial"/adjectives
A/1/2922
" political"
Cluster #347
A/0/69
The feature seems to fire on words ending with "ing".
A/1/613
Descriptions of luxurious accommodations
A/1/2211
Building, construction, exterior, materials related words
A/1/3115
Connector words in weather/nature descriptions?
A/1/3130
Flowers, trees, descriptions of nature
A/1/3678
Clothing style
A/1/2199
Adjectives and descriptions of transportation (buses, trains)
A/1/4052
This feature seems to fire on words that end in "ins" or variations of that spelling. Some examples of words it fires on include Martins, Sri Lankans, Cousins, states, rinks, delicacies, injunctions, Alps, draws, jokes, rants, floats, fellows, councils, felis. It seems to specifically detect words ending in "ins", "ans", "s", or similar spellings.
A/1/3866
" drug"
Cluster #352
A/0/379
The feature attends to colors, particularly darker colors like black, brown and dark green.
A/1/1313
'dark' token
A/1/2171
" wild"
A/1/3209
Colors, particularly " red" and " black"
A/0/288
This feature seems to fire for locations with a geographical focus, specifically rivers, ridges, creeks and valleys.
A/1/1338
[Ultralow density cluster]
Cluster #375
A/0/380
This feature fires when the text mentions water.
A/1/15
" fire" and some similar nouns in police/military/weaponry contexts
A/1/75
" gas", some other forms of power in lower quantiles
A/1/271
" heart"
A/1/442
Beverages esp. " wine", with some food at lower quantiles
A/1/462
" gold"
A/1/615
" food"
A/1/736
" power"
A/1/799
" blood" and other medical/bodily fluids
A/1/961
" fat"
A/1/1698
" sleep", " pain", and human conditions
A/1/1968
Body parts, particular on ones with bones or joints or can be injured
A/1/2463
" body"
A/1/2600
" sun"
A/1/3000
" ball"
A/1/3077
" air"
A/1/3176
" mass"
A/1/3627
" heat"
A/1/3741
" snow" and other precipitation
A/1/3957
" hair"
A/1/3987
" light"
A/1/1225
" self"
A/1/2141
" grass" and other farm-related concepts
A/1/3985
" land"
A/1/3241
" car"
Cluster #376
A/0/258
This feature attends to words and phrases related to roads, streets, and pathways.
A/0/509
This feature attends to any mention of a hand, whether in literal hand references like "he shook my hand" or figurative ones like "the stock was in capable hands".
A/1/465
" mind"
A/1/561
" door"
A/1/647
" room" and similar building/architecture-related nouns
A/1/1308
'head' token, most strongly for terms that aren't the body part
A/1/2202
"border"?
A/1/2301
" road"
A/1/2552
" hand"
A/1/3763
" side"
A/1/2040
Telephones/phones/cell phones
A/1/7
Nouns described by other nouns (locations / " game"?)
A/1/1534
" home"
A/1/2680
" front"
A/1/2689
Nouns related to weapons/machinery/transportation
A/1/1480
Cardinal directions, e.g. " north"
A/1/1668
" step"
A/1/3590
Food-related or celebratory occasions like " dinner"/" tea"/" Christmas"/" wedding"/" birthday"
A/1/3114
"Welcome"/" blog" and other words in website self-descriptions
A/1/2702
"cry" as a verb, as part of compound words, and as part of the idiomatic phrase
A/1/1731
" school"
A/1/506
" city"/" town" and similar location words, esp. discussing historical locations
A/1/1113
'part' token
A/1/493
" sound"/" voice"/similar nouns
A/1/1683
" secret"
A/1/1012
" war"
A/1/1804
" States"
Cluster #338
A/0/183
This feature fires on documents involving general concepts around official government and court communication, including words related to reports, hearings, statements, affidavits, letters, and other formal language.
A/0/377
This feature attends to words associated with government and politics.
A/1/1339
" family"
A/1/1759
" team"/" crew"
A/1/1941
" voting"/" election"/" campaign"
A/1/866
Government organizations: Committee, Government, Congress, Legislature, House, Administration
A/0/432
The feature appears to fire on proper nouns that are locations such as cities, regions, states, countries, trails, rivers, etc.
A/1/451
Cities after "the"
Cluster #341
A/0/97
This feature seems to fire for words related to film and cinema, such as movie, drama, TV, television, and video.
A/1/962
Mostly single-token " game" / associated sports words: " defense", " race"
A/1/2145
" TV"/" television"/similar forms of media
A/1/2914
" book"
A/1/3369
" news"
A/1/665
" project"
Cluster #152
A/0/433
This feature fires when it sees job titles and roles, specifically ending in -er or or. Examples include coordinator, editor, designer, manager, director, writer, analyst, mentor, performer, and adviser.
A/1/1005
" attorney"
A/1/1758
Code: Strings in smart quotes, esp. in JSON-like formats
A/1/1903
" Judge"
A/1/1908
Professions or endings thereof, esp. writing-related, e.g. " writer", "ologist"
A/1/2254
" Clerk"
A/1/2422
Capitalized legal parties (" Defendant", "Appellant", " Respondent", " Plaintiff")
A/1/2963
Political Titles, e.g. " Minister" or " President", predicting politician names
A/1/3601
The speaker of a phrase, esp. a noun, predicting " said" or how they said that phrase
A/1/1771
Transportation/natural-light-related nouns after "the"
A/1/1604
Body parts and clothing, most strongly " chin"
Cluster #280
A/0/269
This feature attends to words associated with male gender such as "man", "boy", "boyfriend", "father", "husband", etc.
A/1/1194
Words for human relatives like " mother"
A/1/1330
" user"/" customer"
A/1/2663
"God"/" God"
A/1/3045
" man"
A/1/3298
" son"
A/1/3699
Fantasy roles and characters: "warrior", "angel", "hero", "vampire"
A/1/3980
" character"
A/1/3049
Body parts in the context of showing emotion (eyes, cheeks, hands, palms)
A/1/2890
[Ultralow density cluster]
A/1/3445
" report"/" statement"/synonyms, as released or read by a person or organization in news contexts
A/1/3904
" patent"
Cluster #328
A/0/148
This feature fires when it sees references to time measured in months, years, days, weeks, quarters, or hours.
A/1/511
" time"
A/1/1054
" years"
A/1/1830
" season"
A/1/1849
Days of the week
A/1/1915
Plural units of time like "hours" or "minutes" esp. after small numbers or "few"
A/1/2242
Time of day
A/1/2283
Larger units of distance
A/1/2882
"year"
A/1/3469
" moment"
A/1/3788
" year"
A/1/3380
The feature appears to fire on words related to official meetings, such as meeting, event, launch, conference call.
A/1/1700
" decision"
A/1/3174
" comment"?
A/1/2906
" view"
A/1/326
" case"
A/1/2806
" bit"
A/1/1676
" problem"
A/1/1270
" question"/" answer"
A/1/887
" subject"/" object"
A/1/690
Abstract nouns after "Each"/"Every"/"One"/"Another"/"first"/...
A/1/146
" lot"
A/1/2941
This feature attends to nouns related to information and data such as report, process, data, library, tape, quiz, review, collection, etc.
A/1/2594
" idea"
A/1/1068
Abstract nouns esp. after "is/has/have no"
Cluster #366
A/0/450
This feature fires when it sees sentences that start with "The" and are the first sentences of paragraphs.
A/1/1443
Nouns (inanimate / "information"-related?) after "The"
A/1/2670
[Ultralow density cluster]
A/1/3581
Abstract noun usually after "This"
Cluster #360
A/0/174
The feature fires when it sees the term "here" and words related to talking about facts or sharing information.
A/1/21
" way"
A/1/2023
" place"
A/1/2118
" reason"
A/1/3190
" deal"
A/1/895
" fact"
Cluster #367
A/0/64
This feature fires when the input contains words related to collecting or a collection.
A/0/393
This feature fires when there are terms related to the importance or significance of something.
A/1/73
Abstract nouns after "NAME is a/an"
A/1/397
" name"
A/1/1099
The feature seems to fire when the word "the" is followed by a word related to importance, meaning, nature, science, or power.
A/1/1134
Mid-word/name lowercase token after "A"
A/1/1936
" kind"
A/1/2165
This feature appears to fire for abstract nouns which represent a concept, idea or plan. It tends to fire for nouns related to decisions, arguments, secrets, virtues, policies, tactics, recommendations, objectives, signals, efforts, instructions, defenses, assumptions, barriers, distortions and structures.
A/1/2757
Varied nouns after "a"
A/1/3355
Abstract nouns about choices/plans esp. after adjectives with strong valences
A/1/3700
" couple"
A/1/3393
This feature fires when it detects phrases describing measurements like size, length, rate, concentration, etc.
A/1/2593
Word after "each" with "each thing" semantic meaning
Cluster #321
A/0/420
This feature attends to mentions of presence or absence.
A/1/1098
Abstract nouns in project management and logistics
A/1/1231
Abstract nouns, esp. quantity-related, after "a"
A/1/1383
[Ultralow density cluster]
A/1/1922
" number"
A/1/2078
" basis"
A/1/2671
" amount"
A/1/3510
" role" in " play a [...] role"
A/1/3560
The feature fires when it sees terms and phrases related to presence or absence.
A/1/3729
" price"
A/1/3881
" ability"
Cluster #217
A/0/302
This feature seems to attend to both descriptions of the face (eyes, mouth) and emotions (tears, smile, grin).
A/1/1385
Standing/walking/climbing and associated adverbs
A/1/3026
The feature fires when it sees words related to emotional distress, struggling, pain, and hardship.
A/1/3475
" frowned"
A/1/3663
Physical / human-body-related verbs
A/1/3973
'shook her head' token, probably gender-neutral
A/1/4042
This feature seems to be firing for words related to things you do with your hands such as reading, tying, drawing, and rowing.
A/1/628
This feature seems to attend to things like solutions, choices, behaviors, opportunities, strategies, moves, arguments, and interactions. In general, it seems to fire on concepts related to decision-making, problem-solving, and key actions.
Cluster #350
A/1/755
Abstract plural nouns, broadly related to games/music/aesthetics
A/1/1723
This feature attends to mentions of towns, cities and other geographic areas.
A/1/3738
Abstract nouns for forms of text/images/media
A/1/2177
Plural noun after a number
A/1/2752
theoretical constructs, elemental concepts and functional dynamics that make up larger philosophical ideas/ frameworks
A/1/247
Abstract nouns indicating relations between usually two things
A/1/453
Physical nouns related to nature or less modern construction
Cluster #340
A/0/57
This feature fires on words and phrases related to unpleasantness, aggression, suffering, conflict, and mortality.
A/1/630
Mostly " death" and related terms, but also " birth"
A/1/1246
The feature seems to fire most strongly when proper nouns related to military, governments, or political groups/movements are present.
A/1/1561
" world"
A/1/2326
" life"
A/1/2742
" violence"
A/1/3084
This feature seems to fire on words ending in -ism, particularly when they are associated with schools of thought, philosophies, or belief systems.
A/1/3498
This feature attends to the word "his", "their", "path", and similar words/concepts related to goals, opportunities, and progress.
A/1/3614
" shame", " fear", emotions
Cluster #281
A/0/308
This feature attends to negative or insulting words that end in -ed or -er.
A/1/470
Profanity?
A/1/3963
Mental health / wellness?
A/1/3774
Tokens/nouns, esp. a few tokens after "how"
A/1/1778
Abstract nouns after prepositions
A/1/268
Abstract verbs, ending in -s, related to imparting emotions or reputations
A/1/3909
Ends of words, mostly plural nouns (ending in -s), predicting ends of sentences?
Cluster #349
A/0/320
The feature attends to plural nouns ending in -s or -es.
A/0/449
This feature fires on words related to professions and groups of people.
A/1/2391
The feature fires for words associated with professional roles such as researchers, analysts, curators, etc.
A/1/2762
Human roles/occupations in society
A/1/2766
Social/political identities
A/1/2929
Numbers that increase by factors of 10
Cluster #337
A/1/303
Potential final words in quotation, predicting punctuation with close-quote
A/1/1189
The feature fires when plural nouns particularly related to people end in "s".
A/1/4033
" people"/" children"
A/1/2868
" men"
A/1/2376
End of quotations
A/1/2675
" law"
A/1/2983
" evidence"
A/1/3385
'peace' in diplomacy/news
A/1/3381
" Court", predicting verbs describing court rulings like " erred" and " remanded"
Cluster #149
A/0/246
This feature seems to fire on legal or court related language, particularly around the concepts of judgments, appeals, jurisdiction and evidence.
A/1/1191
This feature seems to mainly fire on the phrase "the validity". It also often fires on phrases related to legal documents and court proceedings.
A/1/1297
Abstract legal nouns related to "duty"/"defense"/etc.
A/1/1351
'trial' before court in legal briefs
A/1/2523
" judgment"
A/1/3178
This feature fires when it sees legal language related to facts, such as "facts and circumstances", "stipulated facts", "agreed based on facts", "need the facts", "proof", "requirements", "showing", "representations", "assertions", etc.
A/1/3739
The feature fires on sentences mentioning legal topics and terms related to contracts, protection, rights, court proceedings, evidence, compensation, employment, discharge, review, zoning, statutes, oaths, identity, complaints, remediation, negotiation, appeal, resignation and unions.
A/1/2728
Legal terms like " plaintiff", " defendant", " petitioner" referring to parties in a legal case, esp. in legal principles and case law
Cluster #12
A/0/85
This feature fires for attention-grabbing capitalized content, such as quotes, slogans, or exclamations.
A/1/564
Short quoted text inlined into longer sentences
A/1/1526
Text shortly after "smart quotes" (U+201C)
A/1/2571
A first-initial-only capitalized token when words cannot tokenize with a prefixed space, as is often the case when beginning a quoted text
A/1/2655
Abstract nouns largely in legal contexts
Cluster #156
A/0/300
This feature fires when there is legal or economic terminology regarding contracts, liability, and related concepts.
A/1/955
Financial/legal text?
A/1/1340
Legal (Civil Law, Lawsuit, Torts)
A/1/2309
Sentences relating to job employment
A/1/2471
US Constitutional Law
A/1/3647
Abstract adjectives/verbs in credit/debt legal text
A/1/3455
[Ultralow density cluster]
A/1/1167
This feature attends to the word "unless" and other similar words like "although".
Cluster #144
A/0/59
The feature fires when legal offenses and related terminology are present such as "charged", "convicted", "sentence", and "crime".
A/1/911
Criminal law / sentencing language
A/1/1757
Legal/crime reports?
A/1/3704
[Ultralow density cluster]
A/1/1500
' filed' in legal
Cluster #157
A/0/248
This feature fires when the text contains legal phrases related to presenting claims, evidence, arguments, issues and rulings in court cases.
A/1/1842
" examining" and other gerunds describing abstract actions in legal contexts
A/1/2044
[Ultralow density cluster]
A/1/2073
Verbs describing court rulings like " stated" or " concluded"
A/1/2843
" denied"
A/1/2513
Judicial opinion / legal/court text
A/1/42
Legal / tax code past tense verbs or verb endings ("ed")
A/1/1130
" by" in [context]
A/1/4032
Title Case (Military - partially army-specific)
Cluster #271
A/0/389
This feature seems to fire for words associated with military topics and concepts, such as ranks, units, operations, weapons, equipment, etc.
A/1/1498
Army
A/1/1583
Ships and seafaring, describing voyages, wrecks, vessel types and terminology
A/1/2807
Aviation and aerospace topics
A/1/1391
Crime logs
Cluster #238
A/0/280
The feature seems to fire for proper nouns, particularly names of people, films, songs, and other works of art/media.
A/1/714
Connector words in film/TV/shows
A/1/862
Music ?
A/1/1332
Movies / TV / Fiction (esp. action, superhero)
Cluster #336
A/0/244
This feature attends to proper nouns and places, especially when referring to political topics such as countries, wars, conflicts, governments, etc.
A/0/500
The feature appears to fire on syntactic constructions related to sponsorship or support of policy or legislation, especially by politicians.
A/1/127
Middle East ? esp. prepositions
A/1/139
Middle-east organizations (conflict-related)
A/1/941
American political parties
A/1/2273
UK elections, especially Brexit
A/1/2654
Political entity prediction
A/1/3410
Mueller investigation / Russiagate / Political Scandal
Cluster #339
A/1/1215
" armed"
A/1/1253
" state" (legal)
A/1/1595
Nationality/ethnicity adjectives, or ends thereof
A/1/2735
" former"
A/1/3004
" police"
A/1/3493
" military"
Cluster #11
A/0/404
This feature fires for dialogue and quotes, attending to punctuation like quotation marks, as well as discourse markers like "well," "hey," and "yes".
A/1/143
" a" and other articles/pronouns in quotes (dialogue)
A/1/514
Negated (after "n't" or "not") verbs
A/1/730
End of a name in quotes
A/1/1094
Acknowledgement of another character's point as a sentence fragment at the start of quoted speech
A/1/1655
End of short clause prediction (in dialogue?)
A/1/2190
Various verbs esp. gerunds in quoted text
A/1/2233
Noun phrases (that are not pronouns) inside quotations, often after "a"/"an"
A/1/2970
Direct objects, esp. pronouns
A/1/3858
Text in quotes, esp. imperative verbs / commands
A/1/2446
Visual art
A/1/2506
Political language, predicting a political ideology like " socialism"/" democracy"/" capitalism"
A/1/3326
Narratives / conversations ?
A/1/102
Ends of names or phrases specifying/near people; broadly fiction-plot-related
A/1/119
Prepositions describing physical/locational relations between people (characters in fiction?)
Cluster #226
A/0/447
This feature fires when it sees words commonly associated with elevated/formal language and old-fashioned styles, such as older literature. It attends to words like "thou", "hast", "thy", "verily", archaic verb forms ("knowest", "fathered"), and titles like "lord" and "sir".
A/1/1639
Flowery language?
A/1/2103
Descriptive language and old fashioned vocabulary?
A/1/3335
Early Modern (Shakespearean) English
A/1/3925
English text after mid-sentence linebreaks
Cluster #227
A/0/356
The feature fires when the sentence contains prepositions like "for", "in", "to", "with", "at" etc.
A/1/2577
Conjunctions / connectors like "then" and "yet"
A/1/2647
[Ultralow density cluster]
A/1/2894
Constructs from excerpts in stories or personal narratives
A/1/3548
Verbs and prepositions, esp. receiving people as the object, predicting female pronouns/titles
A/1/26
Personal pronouns (" I"/" you"/...) in Biblical contexts?
Cluster #228
A/1/1042
Short words immediately after em dashes
A/1/1298
[Ultralow density cluster]
A/1/1309
Text of informal speech, inside speech marks
A/1/2396
Words in political/religious/military text/speeches
A/1/2614
[Ultralow density cluster]
A/1/3223
[Ultralow density cluster]
A/1/3986
Either token in " ii." or nearby punctuation/numbers, typically volume references
A/1/163
"worship" and other words/phrases of judeo christian texts
A/1/3124
Male pronouns, esp. " himself", or nearby words, predicting similar
A/0/147
The feature attends to words or phrases describing violence or destruction.
Cluster #320
A/1/1815
Adjectives for specific religions
A/1/1824
"era" concepts, both ancient greek/roman, medieval, and modern
A/1/3591
Harmful/bad adjectives relating to prejudice and aggression
A/1/420
"ful" and other word/word components esp. emotion-related
A/1/841
Abstract, esp philosophical, -ical/-ive adjectives or endings, predicting further suffixes like -ism
Cluster #241
A/0/201
This feature attends to proper nouns and titles.
A/0/373
This feature appears to activate when: 1. The text contains words ending in "-tion" or "-ing" 2. Prepositions like "of", "to", "between", etc. 3. Words associated with philosophical or academic writing ("implications", "formulations", "predicated", etc.) So in summary, this feature seems to fire on text with a philosophical/academic style, particularly words with the suffixes "-tion"/-"ing" and common prepositions.
A/0/448
This feature seems to attend to formal or archaic-sounding religious language, with a focus on worshipping, professing belief, preaching, praying, invoking God, etc.
A/1/49
Historical empires
A/1/179
Religious (Christian)
A/1/1701
[Ultralow density cluster]
A/1/1990
Philosophy
A/1/2788
Linguistics
A/1/2958
Spiritual or religious texts about meditation, enlightenment and transcendence
A/1/3977
Prepositions in contexts discussing poetry/poets
Cluster #319
A/0/165
This feature attends to the concept of politics and political structures.
A/1/2427
" national" and similar location-related terms, predicting suffixes like "-ities"/"-ize"
A/1/3215
" legislative"/" disciplinary"/ some long words in legal contexts
A/1/3247
" public"
A/1/3804
" foreign"
A/1/1210
This feature fires on words that are generally considered negative but not strong negatives or curse words - words like "unfair", "illegal", "dangerous", "sharp", "massive", "significant". It does not activate on mild negatives like "unnecessary" or stronger negatives like "terroristic" or "sacrificed".
A/1/2550
" end"
A/1/497
" top"
A/1/2650
" middle"
A/1/3477
"th"
A/1/3105
First/Second/Third/Next/Last prefixes
A/1/4041
Capitalized ordinal numbers (First, etc.) in legal contexts (predicting Amendment, Circuit, etc.)
Cluster #369
A/0/23
This feature seems to fire on ordinal numbers and words indicating something is first, next, last, etc.
A/1/269
" last"
A/1/1317
Ordinal numbers or sometimes fractions, e.g. " third"
A/1/2322
" final"
A/1/3551
" instant"/"instant"
A/1/3917
" hot"
A/1/1165
" real"
A/1/1781
" young"
A/1/3913
Technique description, predicting a word for a technique
A/1/475
" open"
A/1/3194
Adjectives or ends thereof after "a"/"an", esp. "thin", describing material, thickness, or physical quality
A/1/1658
" common"
Cluster #305
A/0/488
This feature attends to words and phrases related to pleasantness and politeness.
A/1/104
Adjectives/adverbs after " more"/" less"
A/1/202
This feature fires for the concept of greater magnitude, attending to words like "stronger", "louder", "more evident", "more visible", etc. It seems to capture the idea of something being bigger, faster, longer, more prominent, more apparent, etc.
A/1/313
This feature attends to concepts related to difficulty, complexity, or hesitance.
A/1/612
" good"
A/1/1169
" better"
A/1/3030
" right"
A/1/3243
" hard"
A/1/3319
" fun"
A/1/3364
This feature seems to fire on sentences describing something as amusing, funny or interesting.
A/1/3464
" close"
A/1/3791
" free"
Cluster #368
A/0/27
The feature fires for sentences that describe something as pleasant or wonderful.
A/0/143
The feature fires when it encounters words and phrases that are often used in formal or technical writing, such as "furthermore", "structured partials", "known to benefit", "fully targeted lessons".
A/1/64
" general"
A/1/296
" new"
A/1/499
" special"
A/1/720
" past"
A/1/733
" same"
A/1/773
Superlatives like biggest/greatest/largest
A/1/1120
'particular' as a token
A/1/1921
" recent"
A/1/2067
" early"
A/1/2338
Strong-valence adjectives
A/1/2366
" prior"
A/1/3065
" best"
A/1/3145
" double"
A/1/3507
" main" / some synonyms
A/1/4037
" big"
A/1/8
" fair"
A/1/2698
" later"
Cluster #313
A/1/252
" full"
A/1/584
" wide"
A/1/1103
" long" token, low-activating " short"
A/1/1972
" small"/" large"/size-related
A/1/1850
" upper"
Cluster #314
A/0/3
This feature attends to words or phrases associated with low to high levels or ranges.
A/1/340
This feature fires for words and phrases related to similarity and identicalness.
A/1/727
" equal"
A/1/1143
" high"
A/1/1593
" positive"/" negative" in scientific/technical contexts
A/1/2286
" higher"/" lower" comparing values
A/1/3481
" significant"
A/1/3229
This feature attends to words indicating increase, decrease or other changes in quantities/measurements.
A/1/1859
" inner"
A/1/3311
[Ultralow density cluster]
A/1/2709
" little"
A/1/1629
" maximum"/" minimum"/etc. in vaguely scientific contexts
A/1/1745
The feature attends to words with the adjective/past participle suffix "-ed".
Cluster #334
A/0/203
This feature seems to fire on the word "one" when used numerically, either for counting or as an ordinal number.
A/1/1522
" one" in [context]
A/1/2161
" one" in [context]
A/1/2758
" every"
A/1/2954
" One"/"One" in [context]
A/1/3359
" another"
A/1/3630
" half"
A/1/3811
" other" or " another" esp. after " each" or " one", though sometimes " the" after " one[ of]"
A/1/4055
" each"
A/1/1222
Adjectives in economic/financial news
A/1/932
Valenced adjectives after " a"
A/0/37
This feature seems to fire when there are words related to quantitative descriptions and measurements, especially broader or larger quantities. Some key words it attends to are "substantial", "large", "significant", "broad", "additional", "strong", "increased", "practical", "direct", "dramatic", "array", etc. Quantitative terms like percentages, time periods, ranges, and other measurements also tend to trigger higher activations.
A/1/762
" present"
A/1/2610
" omn"
A/1/756
" very", " rather", adverbs indicating extent of something
A/1/2285
Physical adjectives, maybe in mechanical or material engineering/design
A/1/2971
" one"/" only" after "the"
A/1/2783
Qualitative descriptors in scientific context, e.g. " unique"
A/1/1797
" exact", " specific", and similar abstract adjectives
A/1/1597
" following"
A/1/2773
" above"
A/1/4085
Adjectives about people esp. after possessive pronouns
A/1/2098
This feature fires for words that indicate transitions such as new ideas, new topics, new paragraph or sections. Examples include words like "the latter", "following", "thus", "however" and ordinal numbers like "fourth".
A/1/1459
[Ultralow density cluster]
A/1/1704
Possessive pronouns: "his", "its", "their", in religious/theological contexts
A/1/1395
[Ultralow density cluster]
A/1/2791
Christian church management
Cluster #244
A/1/328
Possessive pronouns in religious/mythical contexts?
A/1/1818
[Ultralow density cluster]
A/1/3301
" his" in [context]
A/1/43
Legal adjectives usu. describing abstract nouns
Cluster #255
A/0/342
This feature seems to fire when it detects words relating to courts and legal proceedings, such as "trial", "court", "district court", "judge", "petitioner", "plaintiff", etc.
A/1/829
" the" in police reports?
A/1/1106
" the", in legal context
A/1/1563
'the' in legal
A/1/2184
" this" in [context]
A/1/2718
" a" in legal contexts
A/1/3878
" the" in [context]
A/1/213
'his' as possessive prnoun in legal
Cluster #245
A/0/456
This feature seems to attend to possessive determiners like "my", "his", "her", and "their". Specifically it fires strongly when "my" is present.
A/1/41
" His" (esp. familial contexts?)
A/1/348
" its"
A/1/429
" our"
A/1/711
" her", predicting a person-relationship word
A/1/967
" Her"/"Her" in [context]
A/1/1090
Possessive pronouns
A/1/2031
" My"/"My" in [context]
A/1/2558
" his" in [context]
A/1/2888
[Ultralow density cluster]
A/1/3099
" your"
A/1/3303
" their"
A/1/3422
" my" in [context]
A/1/3813
" own"
A/1/3833
This feature attends to words indicating possession such as "my", "your", "his", and "their".
Cluster #254
A/0/1
This feature attends to the word "The" at the beginning of sentences.
A/0/70
This feature attends to the word "the".
A/0/78
The feature fires when it sees the word "the".
A/0/438
This feature fires when the phrase "the" is present in the example.
A/0/510
This feature attends to the word "the".
A/1/1
"the" in a project management context
A/1/69
Usually " the", after " at"
A/1/100
" the" token in "on the", generic use
A/1/228
" the" after "all"
A/1/474
" the"/"The" in [context]
A/1/798
" the" usually before a location
A/1/1039
" the" in [context]
A/1/1063
Discussions of books in general?
A/1/1110
'the' preceding <location>
A/1/1148
"The"/" The" in book titles, esp. underscore-delimited
A/1/1293
" the" in geography?
A/1/1304
'the' before a title
A/1/1674
" the" in "over the" event or time period
A/1/1751
" the" in [context]
A/1/1898
" the" in aeronautics? sea travel?
A/1/2116
" the" in discussion of music/film
A/1/2206
" the" in royalty-related contexts
A/1/2288
" the" in a medical context
A/1/2811
" the" in economics?
A/1/2879
[Ultralow density cluster]
A/1/2916
" the" in descriptive writing, esp physical descriptions of people?
A/1/2931
" the" in Biblical contexts?
A/1/2932
" the" in [context]
A/1/3199
" the" in (U.S.) politics
A/1/3358
" The"/"The" in [context]
A/1/3378
" the" in international relations / geopolitics
A/1/3388
" the" in [context]
A/1/3407
" the" in [context]
A/1/3530
" the" as objects of verbs, possibly military/political/physical contexts?
A/1/3564
" the" in philosophy/psychology?
A/1/3685
"the" in Fantasy games
A/1/3937
" the" in formal English?
Cluster #256
A/0/220
This feature attends to the word "the".
A/1/5
"the" in electrical engineering contexts
A/1/676
" the" in photography? optics?
A/1/731
" the" in mechanical engineering
A/1/1746
" the" in chemistry
A/1/2799
" a" in mechanical engineering
Cluster #249
A/1/234
" a" esp. after prepositions like "by"/"with"
A/1/2068
" a" after "with" and other prepositions
A/1/2942
" the" after "is"/"be"... predicting superlatives?
A/1/3104
" the" in [context]
Cluster #250
A/0/34
This feature fires when the word "a" appears as an indefinite article in sentences. It gives especially high activation to sentences where "a" is followed by a noun, but also activates for other uses of "a" as an indefinite article, such as before adjectives.
A/0/417
The feature seems to fire most strongly when the word "a" is used immediately before a descriptive noun phrase.
A/1/230
" a"/" an" after " as"
A/1/674
" a" esp. after "make" and word forms
A/1/1727
" a" after "in", sometimes after other prepositions
A/1/2024
" a" after "has" or sometimes "is" and word forms or contractions
A/1/2264
" a" after "with" or some gerunds
A/1/2434
" a" esp. after names / proper nouns
A/1/3516
" a" after "is"/etc.
A/1/3576
" a" after "take", "get", "give", etc. and word forms
A/1/3766
" a" in political or current events contexts
A/1/4078
" a", sometimes after "like"
Cluster #248
A/0/484
This feature fires when the word "an" appears before a word starting with a vowel sound.
A/1/666
" An"
A/1/3635
" an" in [context]
A/1/2384
" the" in [context]
Cluster #247
A/0/439
The feature seems to fire whenever it sees the determiner "this" being used in a sentence.
A/1/935
" this" in [context]
A/1/952
" This" in [context]
A/1/1281
" this" in [context]
A/1/1584
" This"/"This" in [context]
A/1/3758
" this" in [context]
Cluster #246
A/0/74
This feature seems to respond most strongly to the word "the", especially when it is being used in a grammatical sense (e.g. "the car", "the problem", "the server" etc). It also occasionally fires for other determiners/articles like "a" and "an".
A/1/820
" a" as object of verb or preposition in programming contexts
A/1/1175
Code: " the" in code comments?
A/1/1400
" my" in [context]
A/1/3893
" an" in [context]
A/1/3054
This feature attends to concepts related to possession, firing primarily on tokens such as "their", "his", "its", "our", etc.
Cluster #257
A/0/90
This feature attends to the definite article "the".
A/1/1214
" the" in [context]
A/1/1520
" The" in [context]
A/1/1857
" a" esp. after "showed" or synonyms
A/1/1893
" the" in medical/anatomical contexts
A/1/2400
" the" in [context]
A/1/2841
" the" in [context]
A/1/3784
" the" in biology/pathology?
Cluster #56
A/0/490
The feature fires when there are instances of the word "no" in the text.
A/1/1101
"Any" token, w or w/o leading space
A/1/1622
" no" in [context]
A/1/1789
" any"
A/1/2033
" no" in [context]
A/1/3089
" No"
A/1/2526
" These" in science?
Cluster #55
A/0/495
This feature attends to descriptive words that indicate quantity, such as numbers, "some", "any", "several", "many", etc.
A/1/142
" those" single-token feature
A/1/291
Small spelled-out English numbers, esp. in scientific contexts
A/1/807
" multiple"
A/1/937
" many"
A/1/1470
" different"
A/1/2006
Small spelled-out English numbers, esp. measuring units of time
A/1/2529
" These"/" these" in [context]
A/1/2587
Large numbers written out in words
A/1/3116
Adjectives after words stating number or quantity like "two" or "several", especially suggesting comparison
A/1/3234
" other" in [context]
A/1/3978
" some" in [context]
Cluster #53
A/0/264
The feature seems to fire on the word "all" in most cases. More specifically, it appears to activate when "all" is used as an intensifier to emphasize something, like "all the books" or "giving everything up".
A/1/1394
This feature seems to fire when it encounters words related to duality or a choice between two options, such as "both," "either," "neither," "outside," etc. It appears sensitive to expressions involving two alternatives or opposites.
A/1/1623
" All"/" all"
A/1/1868
" none"
A/1/922
's token in legal
Cluster #50
A/0/453
This feature seems to fire when it encounters possessive nouns and pronouns ending in 's.
A/1/1108
"s" after \u2019 unicode apostrophe
A/1/1545
"'s" in [context]
A/1/3583
The feature fires when it sees possessive nouns, particularly proper nouns ending in 's.
A/1/3710
"'s" in [context]
Cluster #51
A/1/611
"'s" in contracted pronouns: he's, she's and occasionally who's and nobody's
A/1/1714
"'s" in "That's"
A/1/3183
"It's" __
A/1/3001
"'s" in "There's"
A/1/3242
"m"
Cluster #44
A/0/219
This feature fires on apostrophes.
A/1/792
"\u2019" in [context]
A/1/1176
"\u2019" in [context]
A/1/2492
"\u2019" in [context]
A/1/3420
"\u2019" in [context]
A/1/3580
"'"
A/1/3657
"\u2019" in [context]
A/1/4094
U+2019, apostrophe, after "It"
A/1/2148
"re"
A/1/1735
" AL"
A/1/1266
ALLCAPS fragments ?
Cluster #125
A/0/317
This feature fires on all uppercase words or short phrases.
A/0/421
The feature fires for words written in all caps.
A/1/704
"EE"
A/1/791
Uppercase formal Greek Unicode character names? (between "{~" "~}"?)
A/1/1100
ALL CAPS, usually at start of line
A/1/1150
" AN"
A/1/1348
" THE"/"THE" in [context]
A/1/1359
" CON"
A/1/1527
ER or SON name ending when referring to judges
A/1/1556
All-caps word beginnings esp. two letters ending in O
A/1/1633
" IN"
A/1/1667
" AND"
A/1/1812
UPPERCASE Suffix / End of word
A/1/1980
" RO"
A/1/2011
All-caps beginnings of words (or, more rarely, abbreviations), particularly " NE" (predicting "VER")
A/1/2018
" SE"
A/1/2398
"RE"
A/1/2606
" IS"
A/1/2631
" LI"/"LI"
A/1/2810
" DO"
A/1/2821
This feature fires when it sees the word announcement or variations like announce or announced. This applies across different contexts from formal press releases to informal social media posts.
A/1/2926
" MA"
A/1/2956
" HE"
A/1/3094
The feature fires for an acronym of 3-4 capital letters.
A/1/3197
All-caps names near phone numbers
A/1/3269
ALL CAPS TEXT, esp. in screenplays/dialogue
A/1/3405
All-capital legal documents about contracts and public records
A/1/3439
" DI"
A/1/3565
Capital letter prefix that seems to predict the rest of the word will be in all caps
A/1/3753
"UR" and other mid-word all caps English
A/1/3782
" EX"
A/1/3801
" COMP"/"COMP" in [context]
A/1/2976
" CO"
A/1/2299
" CH"
Cluster #126
A/0/412
This feature attends to acronyms and abbreviated terms, as it fires strongly on uppercase sequences of 2-4 characters.
A/1/527
Roman numerals
A/1/2657
The feature fires when it sees acronyms or abbreviations in all caps.
Cluster #108
A/0/136
This feature appears to fire on scientific acronyms, particularly those related to biology and chemistry.
A/1/582
Short ALLCAPS fragments
A/1/1496
" SC"
A/1/1712
Astronomy and physics acronyms
A/1/1953
Capital letters in abbreviations, esp. when not the beginning of the abbreviation
A/1/2672
All-caps letters in abbreviations, esp. tokens after the first (without a preceding space)
Cluster #119
A/1/18
" ST" with other uppercase letter sequences lower
A/1/1029
Last letter in all-caps abbreviations
A/1/2739
"AT" in ATM and AT&T acronyms
A/0/503
The feature seems to be activated by the presence of single capital letters, especially R and T, in technical contexts. Often these appear as abbreviations in formulas or chemical names.
A/1/2528
This feature attends to abbreviations related to chemistry, biology and medicine.
A/1/1663
Short letter sequences esp. ALLCAPS in technical contexts
A/1/2974
From the examples above, it seems like this feature attends to acronyms and initialisms, particularly three-letter acronyms/initialisms in all caps.
Cluster #87
A/1/320
ALLCAPS abbreviation fragments
A/1/1959
Beginnings of all-caps abbreviations/initialisms, particularly predicting "MA"
A/1/2072
Space+capital-letter fragments usu. in abbreviations
Cluster #79
A/0/33
This feature attends to capital letters like M, L, E, G, R and T in the middle of words or acronyms.
A/1/45
[Ultralow density cluster]
A/1/547
[Ultralow density cluster]
A/1/1987
Space+letter tokens esp. in technical writing
A/1/2344
This feature fires for proper nouns that start with the letters "M" or "N".
A/1/2498
[Ultralow density cluster]
A/1/3066
" Q" in [context]
A/1/3604
Space + capital letter tokens, esp. " Q"
Cluster #74
A/0/344
This feature attends to acronyms and abbreviations for biological and chemical concepts.
A/0/489
This feature attends to DNA sequences with codes like CTT, GAG, AAT, etc.
A/1/68
" CD"
A/1/161
ALLCAPS abbreviations in biology?
A/1/830
Capital letters beginning biology/chemistry abbreviations for molecules etc.
A/1/875
Scientific nomenclature for genes and proteins
A/1/1353
DNA/RNA, fires especially on directionality label
A/1/2151
[Ultralow density cluster]
A/1/2435
The feature is attending to protein abbreviations such as SHP, S1P, TLR4, etc.
A/1/2937
DNA (upper case)
A/1/2982
[Ultralow density cluster]
A/1/3027
[Ultralow density cluster]
A/1/3173
" IL"
A/1/3392
Lowercase letter fragments in medical abbreviations, e.g. proteins or genes
A/1/3412
Various ALLCAPS fragments predicting "K"
A/1/2281
DNA (lower case)
A/1/1944
This feature attends to the protein p and numbers that appear to be p-values from scientific experiments.
Cluster #112
A/0/348
This feature fires when it sees italicized text or italicized abbreviations/symbols in scientific writing.
A/1/507
Text, particularly scientific names of organisms, surrounded/italicized by *'s
A/1/1856
Letters in Molecules "Eg O2"
A/1/3017
Mostly-small physical dimensions: mm/cm distances for diameter/thickness/height/length
A/1/3146
"P"
A/1/3588
" mg", " μg", unit abbreviations
A/1/4053
C representing Celsius
A/0/181
This feature appears to activate for measurement symbols and abbreviations, such as m for meters/mass, mL for milliliters, mm for millimeters, s for seconds, etc.
Cluster #113
A/0/374
This feature fires when detecting certain protein, genetic, and biochemical concepts such as mentions of specific genes, enzymes, and processes like phosphorylation. It attends to both gene symbols (e.g. CRE1, RCK1) as well as written out names of proteins and cell types (caspase-3, CD25+ T regulator cells). Numbers that indicate genetic loci and molecular weights also activate the feature.
A/1/560
Numbers esp. after capital letters (biology?)
A/1/1310
Number in CDn genes, eg. 57 in CD57
A/1/2656
This feature fires when it sees numbers with a single digit (1-9).
A/1/2256
" most"
Cluster #273
A/0/371
The feature fires on the word "more" and words related to degrees and comparisons such as "most", "equally", "highly", etc. It seems to be attending to words used for comparisons.
A/1/199
" more" and some " less"/" most" after " is"/" are"/" was"/etc.
A/1/971
" more"/" less" in scientific contexts
A/1/1077
" much"
A/1/1462
" More"/"More" in [context]
A/1/1999
" more" in [context]
A/1/2625
[Ultralow density cluster]
Cluster #284
A/0/458
This feature attends to the use of adverbs that end in "-ly" in the text.
A/1/329
" highly"
A/1/649
This feature fires for words related to doing something automatically or by hand, such as "manually", "automatically", "directly", etc.
A/1/901
" closely"
A/1/1062
" rather"
A/1/1213
The feature appears to fire on words with the -ically suffix, which denotes "in a manner of" or "pertaining to".
A/1/2592
" completely"/" totally" adverbs indicating extremeness
A/1/3278
" well" in [context]
A/1/3650
Adverbs indicating short amounts of time: " soon"/" shortly"
A/1/3765
" far"
A/1/4000
This feature fires on adverbs that end in -ly.
A/1/2319
" further"
A/1/405
" simply" and " always" after "is" and word forms
Cluster #278
A/0/464
This feature fires when it detects words related to emphasis and intensity such as "very", "quite", "totally", "extremely", "absolutely", etc.
A/1/67
" very" with some other intensifiers
A/1/1572
" relatively" predicting size/money
A/1/1874
" too"
A/1/2318
" pretty"
Cluster #262
A/0/483
The feature appears to activate when a verb is used to describe something that typically or frequently occurs.
A/1/758
" also" and similar frequency modifiers in medicine/science?
A/1/2984
" significantly"
A/1/3374
" nearly"/" almost"/synonyms
A/1/2838
"seem"
Cluster #283
A/0/415
This feature seems to fire when the word "just" is used.
A/0/451
This feature seems to fire when the word "so" is used as a conjunction or adverb to emphasize or indicate consequence. It attends to sentences where "so" connects clauses or comes before adjectives and adverbs.
A/1/2
"also" after an institution
A/1/749
" just"
A/1/1041
" particularly" and some synonyms, esp. after ","
A/1/2449
" once"
A/1/2491
This feature attends to formal or descriptive words that indicate typicality or commonality.
A/1/2839
Localizing events temporally - the relative timing of events as ongoing ("still"), completed ahead of schedule ("already"), or concurrent ("currently")
A/1/3383
" exactly"
A/1/3594
" really"
A/1/3696
" even"
A/1/1404
" never"
A/1/2053
" so", sometimes after contracted "is"/"are" or dashes/hyphens
A/1/3903
" only" token
A/1/3519
" now" and other time-related words
A/1/2074
" always" / intensifiers or frequency-indicating adverbs
A/0/134
This feature fires on phrases that express negation, uncertainty, impossibility, or doubt. Some examples include "cannot", "not", "never", "doesn't", "unlikely", etc. It seems to activate most strongly on the words "not" and "cannot", but also fires on other negative phrases as well.
A/1/3919
" Not"/"Not" in [context]
A/1/2433
" not" in [context]
A/1/1753
"'t" in contractions
A/1/389
" not" after "does" and similar
A/1/974
Auxiliary verbs (in technical writing/docs?)
Cluster #268
A/0/252
This feature attends to contractions with n't.
A/1/169
"t", usually after U+2019 as apostrophe in contraction
A/1/770
"'t" in " couldn't"
A/1/3600
"'t" in [context]
A/1/317
"Can"/" Can"
A/1/2830
"ve" in "I’ve" / other " 've" constructions
Cluster #267
A/0/117
This feature attends to modal verbs like "would," "could," and "might."
A/0/142
This feature fires when the text includes words like "could", "may", "should", "might", "would", etc. that indicate possibility, uncertainty, or conditional statements.
A/0/215
The feature attends to sentences where the modal verb "can" is used.
A/1/350
" may"
A/1/456
" can" in [context]
A/1/610
" can" in [context]
A/1/780
" shall"
A/1/1192
" would"
A/1/2536
" simply" and other adverbs after "can"/"could"/"would"
A/1/2804
" can" in [context]
A/1/3212
" must" in [context]
A/1/3534
" will"
A/1/3835
"'ll"
A/1/2380
"'d"
A/1/3080
" Is"
A/1/338
" Do"
Cluster #239
A/0/391
The feature is attending to past and present verb tenses, particularly those using the word "has".
A/1/256
" has"/" have" usu. after singular nouns
A/1/441
" have" after " would"/" might"/...
A/1/2371
" has" in [context]
A/1/2372
" has"/" have" in scientific contexts
A/1/2693
" Has"
A/1/3156
" have" directly after plurals, usu. groups of people
A/1/3216
" had"
A/1/3308
" has" in [context]
A/1/3659
" have" after "I"/"You" in double quotes
A/1/3823
" have"/" having" in technical contexts, asking questions or troubleshooting
A/1/2292
Object pronouns after "let"
Cluster #235
A/0/107
This feature fires on present tense conjugated forms of the verb "to be" in negative contexts such as don't, isn't, weren't, etc.
A/0/121
This feature attends to contractions and short forms of the words "do not" and "does not".
A/1/579
" won"
A/1/1472
" didn"
A/1/1577
" don"
A/1/2174
" Don"
A/1/2497
" isn"
A/1/3928
The feature seems to fire most strongly when the word "did" appears. This suggests it is particularly attuned to past tense verb phrases. The feature does not appear to be sensitive to any particular semantic content or style, simply responding most strongly to sentences with past tense verbs.
Cluster #195
A/0/125
This feature fires for sentences containing the phrase "like" or "about".
A/1/1770
" like" in [context]
A/1/2948
" about"
A/1/3175
" with" describing agreement/opinions/responsibility
A/1/3851
" like" in [context]
Cluster #180
A/0/28
This feature fires when a sentence starts with a clause that ends with the word "after".
A/1/180
" after"
A/1/187
" during"
A/1/3149
" before", predicting "-ing" participles
A/1/933
This feature seems to fire when it encounters the preposition "among" connecting different nouns or noun phrases.
Cluster #221
A/0/7
The feature seems to fire when the word "by" appears in the middle of sentences. It appears to detect the preposition "by" that connects a verb phrase to its object.
A/0/273
The feature seems to fire when the word "with" is used in a sentence.
A/1/71
" for" in [context]
A/1/487
" for" in [context]
A/1/857
" include" and word forms, usually introducing examples
A/1/2379
" without" in [context]
A/1/2580
" by" in [context]
A/1/3416
" with" in finance/economics
A/1/3524
" with" in stories? folktales?
A/1/1866
" such" in [context]
A/1/1847
" between"
A/1/2589
" By"/"By" in [context]
Cluster #165
A/0/42
Based on the examples, this feature seems to fire most strongly when it detects the phrase "one of the" or variations of it (e.g. "one of", "one of these", etc). This indicates that it may be attending to this common phrase structure.
A/0/328
The feature fires for the phrases "of the", "of", and "University of" which are commonly found in organization names. So it seems to be paying attention to organization names.
A/0/419
The feature fires when it sees the preposition "of" followed by a noun phrase.
A/0/428
This feature fires when it sees the preposition "of".
A/1/243
" of" in "Board of ..." and similar administrative units of something
A/1/314
The feature fires when the word "of" is used to indicate something is comprised or consists of something else.
A/1/343
" of" in [context]
A/1/349
" of" after noun, esp. food/drink/clothing
A/1/407
" of" in a legal document
A/1/492
" of" in [context]
A/1/786
" of" in [context]
A/1/916
" Of"/"Of" in [context]
A/1/1067
" of" in [context]
A/1/1128
" of" in [context]
A/1/1543
" of" in [context]
A/1/1553
" of" in [context]
A/1/2061
" of" in [context]
A/1/2181
" of" in [context]
A/1/2198
" of" in [context]
A/1/2227
"of" in university names
A/1/2334
" of" in [context]
A/1/2628
" of" in [context]
A/1/2691
"of the" in patent statement
A/1/2694
" of" in [context]
A/1/2826
" of" in [context]
A/1/2832
" of" in [context]
A/1/2854
"of" in History Book Titles
A/1/3563
" of" in [context]
A/1/3718
" of" in [context]
A/1/3840
" of" in [context]
A/1/3900
" of" in [context]
A/1/664
" for" in [context]
A/0/187
The feature fires when it sees the word "for" in contexts where it means "because."
Cluster #166
A/0/13
The feature seems to fire when the word "at" is used to indicate a location or specific place.
A/1/284
" at" in legal contexts referencing a (page) number?
A/1/620
This feature fires for words that indicate a time or moment.
A/1/680
" at" in [context]
A/1/1808
" At"/"At" in [context]
A/1/1939
" at" in [context]
A/1/3125
" at" in [context]
A/1/3665
" at" in [context]
A/1/1958
" On"/"On" in [context]
A/1/2382
" For"/"For" in [context]
A/1/1867
" under" in [context]
A/1/898
Prepositions about (U.S.?) historical figures/events, predicting 18xx years?
Cluster #222
A/0/71
This feature attends to the word or phrase "over" used as a preposition in a variety of contexts.
A/0/253
This feature seems to fire when there are words related to moving or transitioning from one thing to another. Examples include "from", "into", "towards", "away", "through", etc. It detects words and phrases that indicate a change in position, location, or state.
A/0/441
This feature attends to the preposition "on".
A/1/494
" around"
A/1/1038
" from", often in government or geography contexts
A/1/1182
" along"
A/1/1230
'on' token, generic use
A/1/1292
" over"
A/1/1315
'on' meaning 'in relation to', eg. in 'based on'
A/1/1409
" beyond"
A/1/1488
"across" a spatial/geographical area
A/1/1644
" through"
A/1/1681
'on' in technical text, mostly referring to information contained in some reference
A/1/1894
" under" in [context]
A/1/2028
" into"
A/1/2059
" Over"
A/1/2378
" against"
A/1/3402
" within"
Cluster #171
A/0/115
This feature fires when the phrase "in" appears by itself or as part of another word or phrase. It often activates in sentences where "in" is used to indicate location or being part of something, but also fires for other uses of "in" such as phrases like "in terms of", "in order to", "in respect", etc.
A/0/251
This feature fires when the word "in" appears at the beginning of a sentence.
A/0/353
This feature attends to the phrase "in front of" and words related to ordering and sequencing.
A/0/446
This feature fires when it sees the preposition "in". The feature seems to activate most strongly when "in" is used to indicate location, inclusion, or being part of something.
A/1/539
" in", tending towards text with a formal tone
A/1/608
" In" in [context]
A/1/1145
" in" (English?)
A/1/1243
" in" in [context]
A/1/1833
" in" for medical results
A/1/1956
" in" in [context]
A/1/2178
" In" in [context]
A/1/2407
" in" in [context]
A/1/2432
" in" in [context]
A/1/2467
" in" in [context]
A/1/2483
This feature fires for ``in ____ condition" or similar phrases describing the state or condition of something.
A/1/2604
" in" in [context]
A/1/2836
" in" in [context]
A/1/3058
" in" in [context]
A/1/3179
Various languages: " in"
A/1/3616
" in" in [context]
A/1/3622
" in" in [context]
A/1/3721
" in" in [context]
A/1/3827
" in" in [context]
A/1/4049
" in" in [context]
A/1/47
" for" in [context]
A/1/1224
'for' in biomedical text
A/1/1419
" order"
Cluster #218
A/0/473
The feature attends to prepositional phrases containing the preposition "with".
A/1/484
" from" in scientific studies
A/1/1046
" using"
A/1/1679
" with" in science/photography?
A/1/2462
" by" in [context]
A/1/4028
" via"
A/1/3483
" with" in technical contexts? web servers?
A/1/2905
" with" in medicine / clinical trials?
A/1/1031
" by" in [context]
A/1/645
" of" in [context]
Cluster #155
A/0/335
This feature fires when examples start with "In order to".
A/1/627
Words shortly after sentence-opening "In"
A/1/1451
" the" after "On"
A/1/1511
" the" after "In"
A/1/3315
" addition"
A/1/2022
" from" in programming/software contexts
A/1/2451
" considered" and broad synonyms in passive voice
A/1/2883
" clear"
A/1/1245
Word fragments: passive verbs or some adjectives after "is"/word forms
Cluster #303
A/1/381
Adjectives after "was"/"are" esp describing people
A/1/1392
The feature fires for words that have adjectival suffixes like -ful, -ous, -ible, -able, -al, etc. that indicate judgment or evaluation.
A/1/2287
" crucial"/" important" and synonyms, esp. after "plays a/an" (predicting "role")
A/1/3151
" unlikely"/" impossible" after "it is"/"it was"
A/1/3916
Emotions, often " about" something
A/1/4077
" enough"
Cluster #300
A/0/193
This feature seems to fire the most strongly when the word "showed" is present in the text. It also appears to activate slightly for other verbs and phrases related to demonstrating or revealing results, like "found", "observed", "evidenced", and "obtained". So this feature likely attends to language describing the results or findings of a study or experiment.
A/0/234
The feature attends to the verb "is defined as".
A/1/165
" shown"
A/1/195
" thrown" and similar action verbs likely to receive a direction/preposition
A/1/245
Past-tense verb about data in prose discussion of code
A/1/920
" used"
A/1/1371
Adjectives describing known or established information, esp. " known", esp. after "is"/"are", in medicine
A/1/1414
Past-tense verb after "was" esp. in history/government
A/1/1632
Past-tense verbs/participles, passive voice, often describing conceptual actions like "defined"/"characterized"
A/1/2020
" due"
A/1/2143
" presented" and similar verbs in science
A/1/2290
'measured' etc in scientific papers
A/1/2567
" showed"
A/1/2578
This feature fires when it sees phrases describing a process of manufacturing, depositing, arranging or forming materials, substances or components.
A/1/2911
" obtained"
A/1/3494
This feature attends to words used to describe things being required, necessary, or expected.
A/1/3513
" described" / " discussed" esp in legal contexts
A/1/3523
" available" and some approximate synonyms
A/1/3593
The feature fires when it encounters words ending in `ed`, `ly`, or `ing`, which are common past-tense, adverb, and progressive verb endings in English.
A/1/3737
" compared"
A/1/3780
" associated"
A/1/3822
The feature fires on passive voice verbs and phrases indicating that something was done.
A/1/3081
" resulted"
Cluster #301
A/1/1796
Past-tense verbs like " interested"/" participated", for people in activities, predicting " in"
A/1/2045
" arising"
A/1/2507
" located"
A/1/3448
This feature fires when it sees a verb form related to the word "consist." It especially activates for forms like "consisted," "consisting," "consist," and "composed," but also somewhat activates for related forms like "comprised," "comprise," "correspond," etc.
A/1/3708
" exist"/" occur"
A/1/1023
[Ultralow density cluster]
Cluster #270
A/0/12
The feature fires when it sees words like "yet" and "only", suggesting that the feature is paying attention to words highlighting the lack of something or that something hasn’t been completed.
A/1/894
Adverbs that indicate aspects of time
A/1/2329
Adjectives after "has"/"had", often size-related metaphorically describing results or plans
A/1/3962
Words predicting me / myself / I ?
A/1/2314
"Thank" (maybe religious?)
A/1/3871
" me"
A/1/13
" thanks"
A/1/4060
Pronouns mainly him and them
Cluster #164
A/0/198
The feature fires when it sees the word "something" and words related to the general concept of something.
A/1/458
" itself"
A/1/1446
" here"
A/1/1607
" nothing"
A/1/2021
" anyone"/" someone"/" somebody"
A/1/2450
" something"
A/1/3709
" Everything"/" everything"
A/1/3820
"somewhere else"
Cluster #266
A/0/237
This feature fires on the verb "do" and its variations such as "did", "done", and "doing."
A/1/302
"please", sometimes capitalized or with leading space
A/1/629
[Ultralow density cluster]
A/1/3009
" well" in [context]
A/1/3206
" let"
A/1/3317
" doing"
A/1/3895
" do"
A/1/4001
" need"
A/0/492
This feature fires when the token "me" appears.
A/1/1312
Spiritual, self-help style psychology/charity text, of the sort that would be titled 'Giving is the Key to Happiness'
A/0/102
This feature fires when it detects words related to gaining attention or awareness.
Cluster #279
A/1/640
This feature fires when it sees words about physical interactions such as hands, arms, fingers, etc.
A/1/2320
Nouns, esp. abstract, after "make" or word forms
A/1/2707
Pronouns after "make" and word forms
A/1/3013
Indirect object pronouns
A/1/3433
" care"/" custody"/etc. after "take"
A/1/3890
" together"
A/1/417
Object pronouns after "tell" or word forms (predicting " why"/" how"/etc.)
Cluster #282
A/1/1131
" able"
A/1/1241
Past participles after "have"/"has"/"had", usually transitive and predicting object pronouns
A/1/2166
" unable"/" fail" esp after "was"/"were"/etc.
A/1/3224
The feature attends to sentences that describe someone's feelings or emotions.
A/0/330
This feature seems to fire on the word "want" and its variations such as "wanted", "wanting", "wishes", etc. So the feature is attending to expressions of desire or intent.
A/1/1408
" want"
A/1/2275
" trying"
Cluster #306
A/0/65
This feature is activated when it detects an instance of the phrase "going to" or related variations (such as "gonna", "go ahead", "go back", etc.).
A/1/1422
" bet"
A/1/1613
" going"
A/1/2154
" try"
A/1/3874
" go"
A/1/3994
" went" (some " gone"/" goes"/...
A/1/3
"worth" single-token feature, esp. after "is" or word forms
A/1/2234
" said"
Cluster #294
A/0/138
The feature seems to fire when expressions of thinking such as "think", "figured", "believe", etc. are used.
A/0/480
The feature fires for phrases related to knowing or understanding.
A/1/585
" believe"
A/1/775
" forget"
A/1/1080
Present-tense verb after / near "you"/"You"
A/1/1234
" say"
A/1/1334
" told"/" tells"/word forms
A/1/1381
" know"
A/1/1651
" sure"
A/1/2386
" hope"
A/1/2420
" mean"
A/1/2482
" means"
A/1/2582
" show" and word forms
A/1/2630
" see"
A/1/2751
" heard"
A/1/2797
" think"
A/1/3828
" thinking"/" thought" word forms
A/1/3929
Verbs after "I", esp. present-tense
A/0/235
The feature fires when it sees dialogue and related tags such as "he/she said", "she replied", "he told me", etc.
A/1/1119
[Ultralow density cluster]
Cluster #322
A/0/333
This feature seems to fire on the base verb "take" in a variety of grammatical contexts and tenses.
A/0/468
This feature fires when it sees verbs related to giving or making (give, makes, made, keeping, etc).
A/0/497
This feature seems to fire when a form of the verb "get" is present. Examples include "getting", "got", "gotten", "get", "gets".
A/1/500
" keep"
A/1/554
" take"
A/1/1666
"make" and word forms
A/1/1747
" taken" in [context]
A/1/1880
" spent"/" spend"/etc.
A/1/2658
This feature fires when it sees verb forms related to the word "bring". The feature attends to words like bring, brought, bringing, and brings.
A/1/3495
" put"
A/1/3628
" give"
A/1/3984
" lose" word forms, or rarely " gain" word forms
A/1/4025
" find"
A/1/251
" wanted" and other verbs of volition
Cluster #323
A/0/381
This feature fires when it sees verbs and adjectives related to visual perception and appearance such as "look", "seem", "stare", "glow", etc.
A/1/2225
Active/physical verbs, performed by personal pronouns
A/1/3264
" look"
A/1/3888
" speak" and word forms
A/1/1075
" come"
A/1/1943
This feature fires on the lemma "get" in various forms (gotten, getting, etc).
A/1/2933
"looked" and "looking" as verbs to describe the act of visually directing one's gaze, as well as more figurative usages indicating scrutiny, seeking.
A/1/1615
" killed"
A/0/58
This feature fires when it sees words indicating that someone or something has arrived from elsewhere.
A/1/1843
The feature activates when it sees words related to beginning, such as began, started, finished, etc.
Cluster #331
A/0/440
This feature fires when the word "led" appears.
A/1/648
The feature attends to verbs that indicate the role something plays, such as serves, referred, attributed, known, appointed, joined, etc.
A/1/809
" met"
A/1/3816
" receive" and word forms
A/1/4075
" followed"
A/1/4046
" directed"
A/1/3446
" led"
A/1/3262
The feature fires when it sees past tense verbs that describe actions in a formal or official context, such as established, developed, accepted, monitored, advertised, executed, introduced, formed, occupied, manufactured, contemplated, challenged, approved, visited, received, retired, and dragged.
A/1/1957
" return" in [context]
A/1/3511
" decided" / " agreed" etc.
A/1/2722
" happen"
Cluster #333
A/0/60
This feature seems to fire on verbs related to physical actions and movements like pull, set, carry, move, cut, burn, freeze, etc.
A/1/351
" drink", " eat", and word forms
A/1/1003
" break"
A/1/1104
" turned", turns, turning - token
A/1/1156
" set"
A/1/1838
" carried"
A/1/2913
" pick"
A/1/3195
" cut"/" pull"/" knock"
A/1/3536
" play"
A/1/3681
" follow"
A/1/3713
" buy"
A/1/2516
" talking"
A/1/174
" read"
A/1/215
" wait"/" waiting"
A/1/2374
" fall"
Cluster #332
A/0/463
This feature fires when a sentence contains "out" as part of a directional phrasal verb.
A/1/86
Primarily " died", few other past-tense verbs for major life events
A/1/347
" grow"/" grown"
A/1/958
" up"
A/1/1216
" off"
A/1/2315
" out" in [context]
A/1/2403
" out" in [context]
A/1/3071
" start"
A/1/3204
" move" and word forms, of people/things, predicting explanations of how it moved
A/1/3267
" back"
A/1/3417
" away"
A/1/3438
[Ultralow density cluster]
A/1/3321
" called"
A/1/2245
" interact"
A/1/1027
" beginning"/" starting"
Cluster #329
A/1/1901
" living"/" staying" and forms
A/1/3375
" stand"
A/1/3612
" lie"
A/1/1636
This feature seems to fire on past tense verbs, especially ones ending in -ed.
A/1/1822
Past-tense verbs for physical actions performed by people
A/1/3139
Word endings with "-ing"
A/1/552
" left"
Cluster #327
A/0/32
This feature seems to attend to verbs that indicate some sort of change, such as reassign, switch, upgrade, enact, etc.
A/1/765
" create"/" develop" and word forms
A/1/3370
Abstract political actions
A/1/3945
The feature seems to fire on verbs related to concealing or revealing information.
A/1/4026
" help"
A/1/526
" agree"
A/1/1728
" learn"
Cluster #330
A/0/263
This feature fires when it sees words related to establishing or determining something.
A/1/691
" provide" and word forms, usually in an abstract sense
A/1/1721
" suffer" and word forms, sometimes other verbs related to injuries
A/1/2119
" seek" and word forms
A/1/2375
This feature attends to words and phrases associated with allowing or granting permission, approval, or access. Examples include "allow", "enable", "permit", "gives", "recommend", "consent", and "restricting".
A/0/496
This feature fires when it sees present participle verb forms (-ing) as well as some similar sounding endings like -ington, -ation, etc.
A/1/1306
'choose' token, literal meaning of choice
A/1/2999
" love"
A/1/2530
" ensure" and word forms
Cluster #176
A/0/2
This feature fires on sentences that have the word "based" or similar words like "relying", "depend" and "insistence". These sentences often describe something being built or formed from something else.
A/1/255
" based"
A/1/1133
" rely"/" focus" word forms etc.
A/1/1983
" depending"
A/1/2639
Gerunds related to cleaning/physical maintenance
A/1/3953
" inspire"/" enjoy"/positive language
A/1/2345
" opening"
A/1/1024
" determine"/" ascertain"
Cluster #287
A/0/434
This feature is activated by examples describing the act of doing work or a job.
A/1/617
" act"/" acts"?
A/1/785
" check", esp. " check out"
A/1/2746
" works"
A/1/2787
"change" as a process of transition, development in living organisms, natural systems, social dynamics and human institutions
A/1/2825
" working"
A/1/3491
" work"
A/1/369
" pay"
A/1/3793
" sat"
A/0/56
This feature attends to phrases and words related to contacting and getting in touch such as "Contact us", "Call today", "Pick up" etc.
A/1/3255
" Google"/"Google"
A/0/245
The feature fires when it sees case law citations such as "State v. DefendantName" and legal language such as "appellant" and "jurisdictional status".
A/1/1352
Tokens in the middle or at the end of names in legal contexts
A/1/1581
Capitalized start-of-sentence imperatives/instructions
A/1/880
Title-case, mainly business/marketing-related words
A/1/38
Party in a court case of "A vs B" formatting
Cluster #234
A/1/483
" Supreme" in "Supreme Court", often following a U.S. state
A/1/3102
Title Case words in names of laws, legal agreements, etc.
A/1/3933
" Act"
Cluster #236
A/1/1601
" United"
A/1/2494
Capitalized compass directions esp. " North" predicting plausible location names
A/1/2719
Directional adverb in geography
A/1/3677
" News" and other names of news organizations
A/1/686
Title Case Nouns esp. undelimited titles in sentences
Cluster #237
A/1/452
" Ever"
A/1/563
" New"
A/1/1160
This feature fires when it sees black (the color or race) in text.
A/1/1300
" High" in high school names, e.g. "Riverdale High School"; or rarely " Middle"
A/1/1799
" Big"
A/1/2522
" Golden"/"Golden"
A/1/3476
" Good"/"Good" in [context]
Cluster #252
A/0/461
This feature fires on proper names containing one or more capital letters, in the context of surrounding text that looks like it could be an article, paper, or other professional writing. Some clues: - The feature consistently has high activation on capitalized words, especially multi-letter abbreviations like NYPD, NETL, CC BY, etc. - It also often fires on locations and organization names like Stonehill College, Aer Lingus, Padova Astronomical Observatory. - The surrounding text has a professional/academic style, mentioning things like a postdoctoral fellowship, the Second World War, International Conferences, etc. So in summary, the feature is detecting proper names and abbreviations, in the context of an academic/professional article or report.
A/0/485
The feature fires when it sees proper nouns such as names of people, places, organizations, acronyms, etc.
A/0/508
This feature attends to proper nouns that contain uppercase words or abbreviations in the middle of a sentence.
A/1/80
Paper acknowledgements, esp. names of institutions therein
A/1/176
Title Case (News Headline)
A/1/308
Title Case formal model names
A/1/419
This feature fires on words and phrases related to legal cases, such as "Case:", "Claim No.", "Case No.", "Court", "Order", "Form", etc.
A/1/701
Capitalized scientific publications/institutions
A/1/1172
Title Case words for abstract units of data or relating to finance/payments
A/1/1413
Capitalized terms for organizations
A/1/1587
" Health"
A/1/1703
" Law"
A/1/2046
Title Case Medical terms, e.g. in titles of papers/studies
A/1/2075
" War", esp. in "World War", predicting " II"
A/1/2126
" Down"
A/1/2232
Capitalized, esp. last, words in formal names of organizations/awards/etc. but before the "kind of thing" it is (e.g. not " Committee" or " Association")
A/1/2300
" Time"
A/1/2772
Title-case words, esp. " Dream", esp. used in a "compound word" name like "Dreamcast"
A/1/3166
" National" in Title Case organization names
A/1/3307
Academic Institution Name
A/1/4066
" World"
Cluster #251
A/0/99
This feature fires when proper nouns are present, especially locations like cities and states.
A/1/293
Names of roads and streets
A/1/306
Countries or continents, capitalized
A/1/687
Places with bipartite names (_ Creek, _ Falls)
A/1/2566
" City"
A/1/3447
Major American city names, predicting locations/things in them, e.g. " City" or " County"
A/1/4018
" York" in "New York", predicting things in New York, e.g. " City"
A/1/1985
Music/Dance/Broadway/Movies named entities
A/1/1752
Ends of Title Case company/brand names
A/1/3869
" Part"
A/1/2904
" General"
Cluster #229
A/0/337
This feature fires on italics or book/article titles.
A/1/1665
Title Case words / word ends in book titles
A/1/2851
Underscore-delimited names/words
A/1/3861
Capitalized royal/religious titles (" Lord", " "King", etc.)
A/1/2070
Mostly ends of proper nouns / names of people, predicting past-tense verbs they might perform
A/0/297
This feature attends to names ending in a, ie, or y.
A/1/3456
[Ultralow density cluster]
A/1/3936
" Lock"
A/1/2705
" Out"
A/1/2460
Title Case location names
A/1/2312
The feature fires when words contain "wood".
A/1/346
Common first names, predicting past-tense verbs
A/1/394
" Inc"
Cluster #357
A/0/445
This feature fires on English names in text, particularly last names.
A/1/123
Names of authors/locations/publishers
A/1/646
This feature fires when it sees words containing the substring "ley".
A/1/672
Common English last names
A/1/1084
[Ultralow density cluster]
A/1/1190
[Ultralow density cluster]
A/1/2293
The feature seems to fire on Spanish surnames ending in vowels such as a, e, i, o.
A/1/2703
Ends of names, particularly people, in English
A/1/2918
Roman/Greek proper names
A/1/2973
Names of people or final tokens thereof
A/1/3098
Last token of surname, in legal descriptions
A/1/3240
" Trump" (Donald Trump) / some other U.S. presidents
A/1/3664
English, esp. British, place names, predicting "shire"
A/1/3818
Tokens in the middle or at the end of a name that looks like a location, predicting e.g. " County"
A/1/3060
[Ultralow density cluster]
Cluster #354
A/1/17
"er" and "ter", usually near ends of words
A/1/101
"er"
A/1/525
Ending of a person's name
A/1/1240
This feature fires for words whose final syllable sounds like "ore".
A/1/2041
[Ultralow density cluster]
A/1/2123
The feature attends to tokens that end or have a "y" sound.
Cluster #364
A/0/370
This feature attends to male first names ending with "n" (and variations) such as Kevin, Steven, Ken, John, Glenn, etc.
A/1/124
Typical English honorifics "Mr"/"Ms"/"Mrs"/"Miss"
A/1/1244
Common English first names, esp. male, predicting last names
A/1/2697
" Dr"
A/1/891
[Ultralow density cluster]
Cluster #363
A/0/314
The feature fires when there are names or words ending with "in".
A/1/1592
This feature attends to foreign names and words, especially German ones.
A/1/2036
Parts of names, esp. endings and esp. people's last names
A/1/3717
This feature fires on words that end in "ar" such as "Dakar", "Hoyer", and on words that end in "al" such as "Scalial". It also seems to activate slightly on some words ending in "er" such as "Birch".
Cluster #365
A/1/1456
" Tim"
A/1/3250
" Christ"/"Christ"
A/1/3970
" Ben"
A/1/1969
Tokens in middle or end of names, esp. location names
A/1/1805
Ends of names or usernames, and "gate" in "enronXgate"
A/1/2712
"e" in the end of last names of people
A/1/1056
Conjunctions/prepositions near Indian locations/government
A/1/231
Transliterated Arabic names of people?
A/1/1093
Beginnings of names
Cluster #362
A/0/457
This feature fires on words and names ending in "a".
A/1/1053
[Ultralow density cluster]
A/1/1827
This feature appears to activate on syllables and sounds at the end of words like "ah", "uh", and "ih".
A/1/1926
[Ultralow density cluster]
A/1/2136
This feature attends to Japanese, Korean, Tamil and Hindi names and words.
A/1/3256
Mid- or end-word "as" or "is" fragments
A/1/3501
Mostly "la" but various non-English word/name endings ending in "a"
Cluster #361
A/0/260
This feature attends to proper nouns ending in "el", "en" or "er".
A/1/839
"in" in [context]
A/1/939
Parts of a large variety of various names (Korean, Japanese, Persian, Yoruba...)
A/1/1305
Single-lowercase-letter tokens, e.g. "b"/"l"/"n"/"m"/"k", inside capitalized words and names with unusual spellings
A/1/3842
This feature fires when it sees names or words ending in -en.
A/1/1502
People's names, particularly Spanish/Latin-American
A/1/1848
[Ultralow density cluster]
Cluster #390
A/0/481
This feature attends to the letters "ul" surrounded by consonants.
A/1/280
[Ultralow density cluster]
A/1/411
Tokens before German name suffixes like -berg, -stein, -reich, etc.
A/1/856
~2-letter tokens after the start of a title-cased name, esp. "ul"
A/1/865
Token after " Ca", either name continuation or number/punctuation for Calcium
A/1/1354
"ac"
A/1/1417
Tokens in a capitalized word esp. after "Con"
A/1/1454
Tokens in a capitalized word esp. after "Sch"
A/1/1687
" McG"
A/1/1962
Tokens in names, esp. " Lind"
A/1/2196
This feature fires when it sees words containing double consonant letters and diphthongs in Spanish.
A/1/2418
Space + capital letter after a first name, either a middle initial or starting a last name
A/1/3283
The feature appears to fire on proper nouns, especially names, that start with hard "k" or "c" sounds.
A/1/3488
Mid-title-case-name tokens esp. "it"/"ac" after " Cap"
A/1/3926
" Jr"
A/1/1602
"ist"
A/1/904
"mb"
Cluster #35
A/0/189
This feature fires for two or three initials followed by a period, which is a common way of citing author's names in academic papers.
A/1/509
" Rev" in in citation-like things, most prominently "Phys. Rev."
A/1/1291
Advanced math book titles
A/1/2369
First initial of author in citations (usually S)
A/1/3075
Capital initials and initial capital letters of names, esp. in bibliographies, e.g. "S" in "S. S. Smith"
A/1/3839
Middles and ends of European last names, esp. in bibliographies
A/1/3694
[Ultralow density cluster]
Cluster #353
A/1/1153
German: ch?
A/1/2002
"av"/"ov" and other mid-word/name tokens with "v"
A/1/2947
"ik" in last/first names from Slavic or Eastern European origins
A/1/3954
The feature fires when it sees words with the "ang" phoneme in them, such as bang, rang, fang, anguish, etc.
A/1/965
Sanskrit, Arabic, and Indic: ?
A/1/3960
" Phys"
Cluster #351
A/0/230
The feature seems to attend to foreign names and terms, primarily names from South Asian languages such as Sanskrit.
A/1/1028
Japanese: traditional names written in Romaji
A/1/1448
Pinyin-transliterated Chinese names
A/1/1579
Russian: Names connected to the country, not necessarily ethnically Russian
A/1/3639
English transliterations of Indian names?
A/1/683
Word component "ost" or "st"
A/1/3619
This feature attends to the suffix -ill and names ending in -ill.
A/1/1447
Space + single capital letter tokens, typically in title case
A/1/1919
[Ultralow density cluster]
A/1/2291
"am"
A/1/2621
This feature fires on words that end in "ot". This includes words like "faggot", "campot", "expert", "caput", etc. It seems to especially activate on these endings when they appear at the end of a line, like in lists or at the end of a sentence.
A/1/384
Spanish first names
A/1/2786
" Gi"
A/1/4048
European languages: "az"/"iaz"
A/1/778
" Man"
Cluster #391
A/0/345
This feature activates for words containing the sound "cap" (cap, capacitance, capacitors, etc).
A/1/478
" Mon"/"Mon" in [context]
A/1/1296
" Bon"
Cluster #389
A/1/1836
" Santa"
A/1/2304
" Saint"
A/1/2660
" San"/"San"
A/0/383
This feature appears to fire on names and places, particularly with Spanish origin such as Benigno Aquino, Cristo Yacente, María de los Milagros, etc.
A/1/2900
" St"/"St" in [context]
A/1/1732
"enn"
A/1/3265
"und"
A/0/236
This feature fires for the prefix abbreviation 'St.', most commonly as part of location names such as cities and states.
A/1/3638
This feature fires when it sees names with Polish spelling, including endings like -ski, -owicz, etc. It also activates for some normal English words that end in -ow like "shadow", but definitely has a bias for Polish names.
A/1/1886
" Math"
A/1/210
" Ann"
A/1/1803
Pinyin-transliterated Chinese names
A/1/2260
This feature attends to words that end in -ur.
A/1/1329
" Bow"
A/1/1142
This feature fires on words that have the "-ol" sound in them. Some examples include "Rolfe", "Roller", "Rolland", "Rolipram", "Rolens", etc. While it sometimes also picks up on other words that end in "-ol" like "control" or "alcohol", there is a clear bias for names and words where the "-ol" sound is more emphasized. The feature does not seem to care about the semantics or context, just attending to any word with that phonetic ending.
A/1/2818
" Van"
A/1/3108
" Cast"
A/1/2479
" Fran"
A/1/4047
"ar"
A/1/4038
" Com"
A/1/1533
Mid-word "og" or other fragments ending with "g"
A/1/909
" Am"
A/1/2160
" Ok"
A/1/2538
" Met"
A/1/1212
" Jo"
Cluster #392
A/0/223
This feature fires when a word includes the syllable "rog".
A/1/58
" Hen", handful of " Ken"/" Len"/name starts
A/1/431
" Med"
A/1/594
" Ang"
A/1/696
" Ser"/"Ser" in [context]
A/1/790
" Mac"/"Mac" in [context]
A/1/836
" Mar"
A/1/924
"Mont"
A/1/1180
" Mur"
A/1/1384
" Mad"
A/1/1519
Beginnings of proper nouns esp. " Em"/" Ham"
A/1/1853
" Er"
A/1/1890
The feature attends to words with /məl/, /i/, and /ɑː/ sounds (such as Mal-, I-, A-) in the middle or end of words.
A/1/1948
" Brit"
A/1/1984
" Mc"
A/1/2157
" Mer"
A/1/2527
" Mor"
A/1/2627
" Har"
A/1/2644
" Luc"
A/1/2809
" Rem"
A/1/2981
" Bur"
A/1/3185
" Jan"
A/1/3424
" Mah"
A/1/3527
" Mag" etc.
A/1/3562
" Pet"
A/1/3673
" Ed"
A/1/3702
" Cor"
A/1/3794
" Pat"
A/1/3802
" Mart"
A/1/2003
" Az"
A/1/2261
" Bel"
A/1/1258
" Hor"
A/1/3452
" Sim"
A/1/2052
" Sign"
A/1/2732
"it"
Cluster #393
A/0/382
The feature attends to words ending in "-al", especially when it is part of a proper noun.
A/0/471
This feature fires on words that contain the syllable "terra" such as territory, terracing, Terran, and terraced.
A/1/147
" sal"
A/1/153
" Wil" in names
A/1/352
" Car"
A/1/548
" Col"
A/1/566
"ou"
A/1/984
" Ter"/"Ter"
A/1/1219
" Sac"
A/1/1536
"mar"
A/1/1621
" ger"
A/1/1630
" Sand"
A/1/1730
" Par"
A/1/1733
" Sy"
A/1/1896
" El"
A/1/1925
" Ban"/"Dun"/" Dun", esp. beginning names
A/1/1952
" Kar"
A/1/2282
" Bal"
A/1/2340
" ar"
A/1/2352
" Anal"
A/1/2517
" Gal"
A/1/2573
" Hel"
A/1/2704
"ij" in "ijerph" citations ("@B1-ijerph-...")
A/1/2845
" Pol"
A/1/2875
" Al"
A/1/3047
" Cal"
A/1/3062
" Grand"/"Grand"
A/1/3246
" Ber"
A/1/3331
" Far"
A/1/3404
" Vol"
A/1/3521
" Ly" and similar usually capitalized word beginnings
A/1/3522
" Ar" and similar usually capitalized word beginnings
A/1/3800
" Sher"
A/1/3876
" Del"/"Del"
A/1/3999
" Wal"
A/1/4065
" Val"
A/1/2150
"leg"
A/1/2090
" bar"
A/1/942
Mid-capitalized-word tokens ending in "t", sometimes "ct", predicting "orial"
A/1/1532
" Port"/"Port"
A/1/2515
Ver
A/1/3274
" Mat"
Cluster #382
A/1/2940
" Dam"
A/1/2953
" Mot"
A/1/3126
" Sav"
A/1/1439
" Gen"
Cluster #381
A/1/1037
" Ant"
A/1/1361
" Ind"
A/1/2008
" ed"
A/1/2547
Mostly " Ent", some "Jac"/"Vent"
A/1/3295
" Vis"
A/1/502
"os" ending a word (often Spanish word or name)
Cluster #371
A/1/595
" Res"
A/1/888
" Den"
A/1/1483
" Nat"
A/1/2197
" pul"
A/1/3725
" Mus"/"Mus"
A/1/3427
Word fragments predicting "let"
A/1/737
" Pan"/" pan"/"Pan"
A/1/1960
" Pal"
A/1/3396
" Fre"
A/1/1508
" Sil"
A/1/763
" Appro"
A/1/276
" sol"
A/1/3200
" Di"
A/1/2142
" Re"
A/1/1550
"co"
A/1/3379
" La"
Cluster #384
A/0/365
This feature attends to names or words starting with 'Pe' or 'Se'.
A/1/203
" Pe"
A/1/274
" Co"
A/1/425
" Me"/"Me" in [context]
A/1/893
" Be"
A/1/1401
" Bre"
A/1/1487
" Se"/"Se" in [context]
A/1/1566
" Le"
A/1/1794
" Ke"
A/1/2039
"ste"
A/1/2159
" Che"
A/1/3109
" Ste"
A/1/3734
" Ne"
A/1/2508
This feature attends to words ending in "o" or containing "cho".
A/1/117
" De"
Cluster #385
A/0/176
This feature attends to tokens that start with Sa or Ba.
A/1/722
The feature attends to words that start with "Ro".
A/1/1591
" Ha"
A/1/1628
" Bo"
A/1/2258
" Ca"
A/1/2343
" Sa"
A/1/2426
"ri"
A/1/3083
" po"
A/1/3458
" ca"
A/1/3658
" Lo"
A/1/590
" Ki" and other space, capitalized-consonant, vowel tokens
Cluster #383
A/0/361
The feature seems to fire when words start with "tra".
A/1/200
" Fra"
A/1/423
"ra"
A/1/667
" Tre"
A/1/1276
" fe"
A/1/1643
" Bro"
A/1/1685
"Tro"/" Tro"
A/1/1855
" pri"
A/1/2977
" Cro"
A/1/3667
" Bra"
A/1/3705
" Tra"/" tra"
A/1/3798
" Dra"/" dra"/"Dra"
A/1/3803
"ro"
A/1/3819
"du"
A/1/3832
" Gra"
A/0/62
This feature seems to fire on co- words, pairs that share a prefix such as co-author, co-founder, etc.
A/1/1387
" co"
A/1/2598
" tri"
A/1/3443
" Ju"/"Ju"
A/1/1248
" fre"
A/1/4003
" ri"
A/1/2968
" cal"
A/1/4093
Mid-word "iqu", start-of-word "nu", and other "-u" tokens
A/0/506
The feature attends to occurrences of "et al" in citations or lists of authors.
A/1/2027
" et"
Cluster #335
A/1/636
Start-of-word "vari"/"propri", predicting "ety" and forms
A/1/744
" fore"
A/1/3690
" dise"
A/1/3489
" al"
A/0/131
This feature fires when it sees references to scientific papers or documents. It looks for things like author last names followed by "et al.", citations like [CR17], brackets for citations like (@CR17), numbered references in the text, and other stylistic patterns common in academic writing.
A/1/2723
" hal"
A/1/1333
" Cy"/"Cy"
A/1/439
" Ad"
A/1/3683
" per"
A/1/304
"Per", as a word prefix
A/1/2085
" en", generally not Spanish
A/1/2863
" Sur"
A/1/285
" En"
A/1/132
" Ge"
A/1/270
French or German: sur
A/1/412
" ap"
A/1/2082
" Pro"
A/1/53
" Impro"/" impro" with some " Retrie"
A/1/3154
" hy"/"Hy"
Cluster #315
A/0/369
This feature fires when it sees the Latin prefix "con-" or "com-" meaning "together, with" at the beginning of English words. Some less frequent languages like French and Italian are also activated when they contain words starting with "con-" or "com-". The feature does not activate on other standalone letters like "c" or "o".
A/1/54
"con"
A/1/261
" Con"
A/1/689
" ex"
A/1/1460
" con"
A/1/2975
" Ex"
A/1/3111
" trans"
Cluster #324
A/0/54
This feature attends to prefixes dis- and in-, which are often used to negate words or express the opposite meaning.
A/0/332
This feature attends to words starting with "re" that are often prefixes or suffixes.
A/0/410
This feature seems to attend to words with the "un-" prefix, including misspellings or errors such as "untrained" or "uncertificated".
A/1/998
" Un"/"Un", esp. right after EOT or in news/politics
A/1/2047
" mis"
A/1/2646
" re"
A/1/3067
" pro"
A/1/3451
" Dis"
A/1/3998
" un"
A/1/1322
"Pre"
A/1/2149
"Tran"/" Tran", predicting "script" and forms
A/1/1433
" dis"
A/1/1368
" Des"
Cluster #325
A/0/422
This feature fires when it sees prefixes like: pre-, non-, intra-, inter-.
A/1/1055
" inter"
A/1/1171
" sub"
A/1/1515
" pre"
A/1/1660
" micro"
A/1/2203
" bio"
A/1/2412
" anti"
A/1/2445
" hyper"
A/1/2633
" non"
A/1/2677
" intra"
A/1/3389
" super"
A/1/3855
" Sub"
A/1/4020
" multi"
A/1/3479
" mid"
Cluster #372
A/0/50
The feature seems to fire when it sees words ending with "res". This includes common English words like "resist", "rescue", "resign", as well as words from other languages like French ("Desrosiers"), German ("Fes"), Spanish ("Residuos") etc. The feature is picking up on a specific suffix/ending shared across languages rather than attending to any deeper semantic or stylistic features.
A/1/1092
" Es"/" es"
A/1/3344
" Ass"
A/1/3545
Start-of-word "Sus"/" Sus"
A/1/3620
" tele"
A/1/2461
" neuro"
A/1/443
" electro"
A/1/464
" Gu"
A/1/294
" mal"
A/1/746
" Nut"
Cluster #374
A/1/551
Mid-word token like "ib", containing vowel followed by "b"
A/1/1690
" Lib"
A/1/1719
" Ab"
A/1/3892
" cor"
A/1/3769
" McK"
A/1/3570
" res"
A/1/1196
" Uns"/"Uns"
A/1/3745
" Fer"
A/1/2425
Mostly " Benef"/" Jud" predicting "iciary"
A/1/2959
" ammon"
A/1/989
" ep"
A/1/3915
" ant"
A/1/1372
"colog"
A/1/2829
" Art"/" art"
A/1/1588
" min"
A/1/2649
"ob"
A/0/338
This feature attends to the string "ab" occurring anywhere in a word, regardless of case.
A/1/1942
" ab"
A/1/3260
" ob"
A/1/3599
" imm"
A/1/1481
"ap"
A/1/3492
" Emp"/"Emp"
Cluster #380
A/0/478
This feature attends to words related to admissions such as "admission", "admissions", "admissible", etc.
A/1/1605
" Bot"
A/1/1620
" Cap"
A/1/1688
" rect" and "scar", predicting "ify" and forms
A/1/1708
" mod"
A/1/1907
" gall"
A/1/2194
" Log"/" log"
A/1/2249
" Net"/" net"
A/1/2803
" Reg"
A/1/3087
" col"
A/1/3132
This feature seems to fire the most on words that have the -ion or -ive suffixes and sometimes -ment. Specifically, it looks for words like communicative, classification, adaptable, detachment, etc.
A/1/3198
" ad"
A/1/3465
" Bet"
A/1/2763
" cur"
A/1/3496
" reg"
A/1/344
" amp"
A/1/761
" App"
Cluster #396
A/0/279
The feature seems to fire when it sees words related to computing and computers, such as "computed", "computationally", "computer", etc.
A/1/115
" ag"
A/1/221
" inf"
A/1/258
" ext"
A/1/750
" rep"
A/1/771
" amb"
A/1/1539
" imp"
A/1/1650
" ref"
A/1/1835
" emb"
A/1/2081
" exp"
A/1/2700
" Comp"
A/1/3137
" conf"
A/1/3225
" Dep"
A/1/3279
" Sup"
A/1/3328
" Imp"
A/1/3850
" Ref"
A/1/1202
" Ev"
A/1/1673
" cent"
A/0/77
This feature seems to identify words that are commonly used as suffixes in English. Some examples of suffixes it highlights include: -tion, -ment, -phy, -ing, -ed, -er, -ive, -ous, -ful, -ly, -ible, -able, -ure, -ship, -hood, -ness, -ity, -ence, -ant, -ial, -ary, -ery, -ory, -ion, -ity, -ward, -wise, -less, -ful, -ic, -ist, -ive, -ous. So to summarize, this feature seems to fire on English suffixes.
A/1/2596
"Rep", esp. right after EOT and short for (U.S.) Representative
Cluster #388
A/0/423
The feature seems to fire for tokens containing the word "hop" (and related words like "hip"), specifically in the context of hip-hop music.
A/1/319
" bur"
A/1/599
" wrink"
A/1/601
" scal"
A/1/1091
" hel"
A/1/1360
Mostly " pot" with some " bull"
A/1/1913
" bount"/" mas"/" bal"
A/1/3356
" bug"
A/1/3512
" pop"
Cluster #397
A/1/1554
" Int"
A/1/1571
" def"
A/1/2339
" Def"
Cluster #386
A/1/208
" compress", "uggest", and similar penultimate word tokens, predicting suffixes like "ive"/"or"
A/1/496
" cult"
A/1/2458
" apost"
A/1/2828
Mostly " gam" ("gamers"/"gamblers"/"gamification")
A/1/883
" Cont"/"Cont"
A/1/1906
" cat"
Cluster #398
A/1/808
" Ret"
A/1/3043
" downt"
A/1/3974
" unt"
A/1/3637
" post"
A/1/3144
Start-of-word "elect", sometimes capitalized and/or with leading space
Cluster #288
A/1/671
" rest"
A/1/949
" press"
A/1/1138
" cross"
A/1/1428
" camp"
A/1/2114
" trip"
A/1/1064
" ent"
A/1/1159
" disc"
A/1/1872
" refract"
Cluster #395
A/0/9
This feature seems to fire for words that start with 'sc' and 'acc' like scenery, scuff, scuttle, accents, accelerate, accidents, etc.
A/1/6
" rec"
A/1/534
" enc"
A/1/538
" inc"
A/1/882
"oc"
A/1/1347
" Ac"/" ac", sometimes in health/science
A/1/1625
" Occ"
A/1/1977
" dec"
A/1/2055
" prof"
A/1/2795
" dev"
A/1/4067
" prov"/" inv"
A/1/520
"sk"
Cluster #316
A/1/2872
" inst"
A/1/3290
" Sc"
A/1/4070
" Ph"
A/1/3846
" Ins"
Cluster #309
A/0/194
This feature fires for words ending with "st" such as past, post, pist, wast, etc..
A/0/399
The feature fires when it sees words beginning with pl, bl, fl or tr.
A/0/452
This feature seems to fire for examples of words or names that start with "Sh".
A/1/36
" Sk" and few other Title Case word starts
A/1/197
" cl"
A/1/205
" fl"
A/1/435
" Sh"
A/1/550
" Str"
A/1/660
" ch"
A/1/688
" Fl"
A/1/708
" Cr"
A/1/844
" pl"
A/1/859
" Wh"
A/1/889
" sk"
A/1/1014
" sw"
A/1/1185
" Pl"
A/1/1187
" wh"
A/1/1199
" cr"
A/1/1398
" Th"
A/1/1694
" sn"
A/1/1974
" Sm"/"Sm"
A/1/2017
" sl"
A/1/2091
" sh"
A/1/2442
" Ch"
A/1/2503
" Sw"
A/1/2579
" th"
A/1/2613
" bl"
A/1/2638
" Fr"/" fr"
A/1/2898
" str"
A/1/2902
" gr"
A/1/3191
" st"
A/1/3327
" sp"
A/1/3346
" Sch"
A/1/3546
" Sp"
A/1/3598
" Br"?
A/1/3806
" Cl"
A/1/3860
" tr"
A/1/3886
" Sl"
A/1/4056
" gl"
A/1/3208
" sc"
A/1/2939
" ev"
A/1/1239
The feature appears to fire for words containing the phonetic "arc" such as "arcades", "autoradiography", "choreocutaneous", "echocardiography", "hydrocephalus", "microautoradiography", "photocoagulation" and "ultrasonocardiography".
Cluster #387
A/1/681
" vag"
A/1/1825
" fib"
A/1/3363
" mac"
A/1/3500
" vis"
A/1/1614
" pent"
A/1/516
"act" as part of a word
A/1/3414
" hall"
A/1/3736
Mostly " meth", some " phen" and other chemical prefixes
Cluster #159
A/1/219
This feature fires for words containing the letter combination "it"
A/1/1737
" gly"
A/1/2997
" glut"
A/1/3432
" coll"
A/1/1920
" ax"
Cluster #379
A/0/91
The feature seems to attends to words containing "qu", particularly at the beginning of the word. This includes English words like "quite", "quote", and "quibble", as well as non-English words like "Québec" and "quels". The feature also fires for some words containing "qu" in the middle, like "accommodating", but more weakly.
A/1/196
" qu"
A/1/826
" gu"
A/1/1478
"Qu"
A/1/1970
" Vo"/" vo"
A/1/3169
" equ"
A/1/3670
" Mult"/" mult"
A/1/4016
"qu"
A/1/1485
" poll"
A/1/3979
" ir"
A/1/3249
" semin"/" sme", predicting "ar" and forms
A/1/2083
" bul"
A/1/2756
Start-of-word "Imag"/"fasc", predicting "ination" or forms
A/1/1547
" cond"
A/1/1482
" spir"
A/1/2155
" sem"
A/1/503
"cre"
A/1/821
" Domin"
A/1/2478
" ple"
A/1/1396
" pen"
A/1/3418
" clim"
A/1/2944
" solic"
A/1/1226
" comm"
A/1/860
"aph"
A/1/2887
First halves of "-ulate" words
A/1/2678
" ins"
A/1/921
Word beginnings " Adv"/" suff"/" coinc"
A/1/2330
" att"
A/1/2111
" sin"
A/1/858
" stabil"
A/1/1810
" fin"
A/1/2236
" hom"
A/1/3759
" throm" in medical contexts, predicting "bo-" tokens
A/1/1596
[Ultralow density cluster]
A/1/3038
" organ"
A/1/305
" prima" (in "prima facie")
A/1/700
"nih"
A/1/3068
" syn"
Cluster #401
A/0/241
The feature fires when it sees words related to photography and imaging such as photo, micrograph, videotape, photogenic, etc.
A/1/120
" mon"
A/1/624
Prefixes like " aut" and " Phot", predicting "ograph" and similar suffixes
A/1/772
" immun"
A/1/1025
" hyp"
A/1/1743
" psych"
A/1/1973
" bi"
A/1/2668
" hist"
A/1/3603
" cert"
A/1/3934
" radi"
Cluster #402
A/0/225
This feature attends to words with the beginning prefix "macro" and stem "mol" such as macromolecule, macromolecules, and macromolecular.
A/1/378
" phosph"
A/1/669
"ozyg"
A/1/923
" gran"
A/1/1363
" ur"
A/1/1624
" lip"
A/1/1739
" cere"
A/1/1891
" lymph"
A/1/3642
This feature fires when it sees medical terms ending in -itis, -emia, and -osis, as well as the word mellitus.
A/1/2724
"oph"
Cluster #403
A/1/445
[Ultralow density cluster]
A/1/510
" flu", in the context of bio or medical sciencey stuff, e.g. "fluorescence". Also fires on some other bio sciencey prefixes
A/1/656
Medical terms for cancers and diseases of the liver
A/1/2659
"orph"
A/1/3357
"roph" mostly in "spectrophoto-"
A/1/2848
"om"/"rim" largely in cell biology
A/1/3325
"olog"
A/1/854
" patri"
A/1/1924
" tyr"/" handic"
A/1/1578
" polit"
A/1/1307
" ser" prefix in seroepidemiological words
A/1/2487
[Ultralow density cluster]
Cluster #394
A/1/535
" electroly" or "analy", predicting "tic" or similar, as in "analytically" or "electrolysis"
A/1/1562
" parame"
A/1/3353
" plas"
Cluster #400
A/0/43
This feature appears to fire for words containing the substring "min" such as dominant, dominik, domin, dominates. This includes variations like domination, mination, domination as well as words like minimum, minimums.
A/0/408
This feature attends to words that are often found past tense or past participle verb forms.
A/1/84
" altern"
A/1/897
The feature attends to words ending with "ation" such as evaporation, proportional, optical or transportation.
A/1/1032
Word prefixes like " incompet" and " independ" that predict endings like "ent" or "ence"
A/1/1044
" expl"
A/1/1107
" ped"
A/1/1139
" domin"
A/1/1154
" illumin"
A/1/1774
" Contin"
A/1/1967
" interle"
A/1/2456
" Rel"
A/1/2457
"rom" in the middle of scientific/mechanical words, esp. forms of "macromolecule"
A/1/2780
" lum"
A/1/2928
" reass"
A/1/3097
"sor" in word beginning " adsorb"
A/1/3323
Mostly " quant", predicting forms of "itate"
A/1/3508
" neg" / "eg" etc.
A/1/3742
" substit" and similar word prefixes, predicting "ute" and word forms
A/1/3997
Word beginnings like " indul" and " diver" predicting suffixes like "-ge"
A/1/4019
Mid-word tokens after the letter "p", esp. "lic" in "complicate"/"implicate"
A/1/4058
" cran"
A/1/2469
" dispar"
A/1/721
The feature fires on words containing "ant" and forms of that root such as anti, anterior, antisera, etc. In particular it attends to medical and biological terms containing these roots.
A/1/2415
" ann"
A/1/2808
" tend"
A/1/3055
" contra"
A/1/571
[Ultralow density cluster]
Cluster #399
A/1/60
" mot"
A/1/957
Mostly " tub" with some " tum", " trib"
A/1/1823
"ag"
A/1/2158
"ult"
A/1/2505
This feature responds to words related to light and vision, like "photoluminescence", "photoreceptor", and "photodetector".
A/1/2436
Mid-word tokens after "un-"
A/1/1675
[Ultralow density cluster]
A/1/316
" judic"
A/1/1549
" conce"
A/1/2710
Mid-word tokens after "dis"
A/1/754
Usually "pe" in "Appeal"
A/1/3309
"Abbrev" in Abbreviations
A/1/2137
Mid-word tokens esp. after "emb"
A/1/3859
" lit"
A/1/3882
" Prom"/"Prom"
A/1/2170
Letters after "qu" in a word
A/1/4073
" mill"
Cluster #373
A/1/392
" ban"
A/1/606
" par"
A/1/912
" tor"
A/1/1314
" gar"
A/1/467
" dem"
Cluster #370
A/0/92
This feature attends to words ending in an 'b' sound.
A/1/164
Tokens after a single lowercase letter in a word
A/1/2162
Mid-word tokens in words starting with "s"
A/1/2874
"ocal"
A/1/2588
" gle"
A/1/3771
This feature attends to words related to introducing, producing or inducing something.
A/1/114
" ther"
A/1/295
" revol"
A/1/2815
" gro"
A/1/106
" cro"
A/1/2429
" cra"/" scra"
A/1/76
" den"
A/1/1073
" che"
A/1/3021
" fil"
A/1/2348
" gra"
A/1/3688
" mo"
A/1/1777
" pe"
A/1/2195
" ro"
Cluster #377
A/1/81
" cre"
A/1/83
" ve"
A/1/523
" ra"
A/1/2413
" ce"
Cluster #378
A/0/355
This feature seems to attend to words that contain "se" or "le". This pattern holds across multiple languages, including English, Spanish, French, German, Italian, Portuguese, and Romanian. The feature fires most strongly for words where "se" or "le" is in the middle of the word.
A/1/140
" lo"
A/1/1188
" bo"
A/1/1278
" te"
A/1/2866
" le"
A/1/3419
" se"
A/1/3787
" ne"
A/1/1993
" ma"
A/1/3911
" bu"
A/1/2476
" une" or " sa"?
A/1/2271
" Los"/"Los" in [context]
Cluster #182
A/1/155
" su"
A/1/1358
" Je"
A/1/3883
" du"
A/1/479
" de"
A/1/1477
" av"
Cluster #193
A/0/76
This feature seems to attend to Spanish and other Romance languages. Many of the high activations contain words like "de", "la", "el", etc. which are common in Spanish, as well as characters like á, é, í, etc. The feature also seems to fire on French, Italian, and Portuguese words.
A/0/206
"la" in french and spanish
A/1/597
Italian or Spanish: Feminine articles " una"/" la" in Spanish and maybe Italian / other languages?
A/1/1251
Spanish: " de"
A/1/1965
Spanish: Word beginnings?
A/1/379
" di"
Cluster #208
A/0/61
This feature seems to fire when it sees text in Portuguese or Brazilian Portuguese.
A/0/487
The feature seems to fire for Spanish words ending in -aban, -aban, -aron, -aban, or -aban. This indicates it is picking up on the Spanish imperfect past tense endings for -ar verbs.
A/1/30
Spanish: " que", usually Spanish?
A/1/434
" no" in [context]
A/1/542
Spanish: " el"
A/1/725
Spanish: word endings
A/1/944
Spanish or Portuguese: ?
A/1/1337
Spanish: " en"
A/1/1997
Spanish: Punctuation, esp. colons
A/1/2109
Portuguese tokens, esp. " prepar", predicting "ão" and endings
A/1/2244
Spanish: " los" and other articles?
A/1/2331
Spanish: ?
A/1/2502
Portuguese: Prepositions/articles
A/1/2989
Portuguese: Strong tokens with accented and distinguishing Portuguese characters
A/1/3024
Portuguese: ?
A/1/3140
Spanish or Portuguese: " a"
A/1/3656
Spanish: "reg" and other mid-word tokens?
A/1/3716
Spanish: "Yo" / "adie" / ... ?
A/1/3836
Short word fragments esp. just one capital letter
A/1/833
Spanish: Space+letter tokens?
Cluster #199
A/0/14
This feature appears to fire when French is present.
A/0/44
This feature fires when it sees accented characters, like é, ã, ö, â, etc.
A/0/182
This feature seems to fire on apostrophes in French text.
A/0/255
This feature fires when it sees Spanish words related to government, legal, and business topics such as politics, legislation, petitions, solicitation, requests, and registration.
A/1/150
" est"/" Est"
A/1/436
French: " les"/" des" (the/some)
A/1/544
French: Apostrophe as contraction
A/1/956
Spanish or Portuguese: Some -du word beginnings?
A/1/981
French: é in [context]
A/1/991
French: " qui" but only in the that sense, not who
A/1/1050
French: predicts é|e|è
A/1/1168
French: " é"
A/1/1288
French: "è" in [context]
A/1/1892
French: Prepositions?
A/1/2016
French: " :", and other punctuation
A/1/2042
French: " dé"/" ré" prefix only
A/1/2146
French: Names / proper nouns, perhaps in a bibliography?
A/1/3002
French: Prepositions about containing/linking
A/1/3008
French: Predicts prepositions
A/1/3207
"\u00e2"
A/1/3596
French: " à"
Cluster #209
A/0/184
This feature seems to be activating on words that have diphthongs (vowel sounds that glide from one position in the mouth to another) in Italian and other Romance languages.
A/1/2034
Italian: Formal governmental (Not sure why it it's matching on a subset of the Italian words/characters though)
A/1/2156
Spanish: ?
A/1/2250
Italian: elements of verb conjugation
Cluster #200
A/0/455
This feature attends to text in Romanian.
A/1/9
Romanian: ?
A/1/632
Romanian: " \u00ee"
A/1/846
Accented, esp. acute, characters
A/1/3923
Latin: ?
A/1/734
Indonesian/Malaysian: ?
Cluster #211
A/0/159
This feature appears to fire on words ending in ng (and similar sounds like nk, nd, etc.), with a preference for words of three or more syllables.
A/1/1657
Turkish, Basque, Hawaiian, Maori, and other Polynesian languages: ?
A/1/1912
Icelandic: ?
A/1/623
Vietnamese & Romance Languages: Text near UTF-8 as latin1 mojibake
Cluster #212
A/0/36
This feature is attending to Slavic languages, mainly Polish and Czech.
A/0/294
This feature attends to Hungarian words and some other Hungarian-like examples from nearby languages.
A/1/1141
"\u00e1" in [context]
A/1/1200
"," in [context]
A/1/1627
Turkish: ?
A/1/1772
Estonian: ?
A/1/2201
Lithuanian: ?
A/1/2360
Slovenian: Text or names?
A/1/2651
Slovenian: ?
A/1/2676
Hungarian: ?
A/1/2792
Slovenian: ?
A/1/2801
Slavic: "," in [context]
A/1/3348
Polish: ?
A/1/3361
Latvian: ?
A/1/3537
Vietnamese: ?
A/1/3648
Czech: ?
A/0/180
This feature attends to punctuation such as periods, quotation marks, newlines.
A/1/3329
Finnish: Word beginnings?
A/0/162
This feature fires when it sees German words that start with an "s" followed by a consonant.
Cluster #205
A/0/339
This feature seems to be firing for Finnish words and sentences.
A/1/1052
Finnish: ?
A/1/2570
Finnish and German?: Consonants after umlauts
A/1/2897
Finnish: Umlauts (and some surrounding text?)
A/1/3413
Finnish: tokens esp. with "-ist" or "-ll"
A/1/2153
Swedish?: Letters or tokens with diacritics
A/1/1112
[Ultralow density cluster]
A/1/917
Dutch: ?
Cluster #213
A/0/469
This feature seems to fire on Germanic languages - specifically Scandinavian languages like Swedish, Danish, and Norwegian. It is attuned to grammatical features and common words in those languages.
A/1/321
Swedish: Punctuation
A/1/1373
Danish: ?
A/1/2464
Swedish: ?
A/1/3046
Danish: Politics
Cluster #196
A/0/493
This feature seems to activate for German text, particularly German grammar such as noun declensions.
A/1/177
German: " der", plus other articles/prepositions weakly
A/1/334
German: Text tokens ending in "e"
A/1/358
German: Consonants at start of word
A/1/838
German: Prepositions
A/1/1695
" und"
A/1/2410
German:?
A/1/2601
German: Commas
A/1/2637
German: ?
A/1/2652
German: Pronouns ("ihre", "die", "dieser")
A/1/2661
German: Modal verbs
Cluster #214
A/1/1951
Dutch: " wa"/" na"?
A/1/2065
Dutch: Some word beginnings?
A/1/2899
Danish: ?
A/1/2032
Swedish: ?
A/1/103
LaTeX length names, particularly "side" in "oddsidemargin"
A/1/2576
"ensuremath" in LaTeX
Cluster #116
A/0/35
This feature fires for math expressions, in particular fractions displayed using LaTeX.
A/0/82
This feature attends to mathematical symbols.
A/1/267
LaTeX mathbb font
A/1/427
LaTeX operators with a number over another (frac, binom, dfrac)
A/1/914
LaTeX macro names esp. accepting one short argument
A/1/1136
LaTeX environment "begin"
A/1/1889
LaTeX text modifiers in equations (inc. mathfrak, sqrt, overline, boldsymbol)
A/1/2226
This feature seems to fire on LaTeX formatting and math markup.
A/1/2590
This feature fires on TeX/LaTeX formatting such as equations, labels, sections, tables, etc.
Cluster #3
A/0/175
This feature attends to mathematical symbols and expressions.
A/0/368
This feature fires when it encounters LaTeX commands and math symbols.
A/1/112
Code: "\" in paths or string escapes
A/1/193
LaTeX: Inline modifiers
A/1/222
LaTeX: Predicting an operator?
A/1/262
Code: "\" in RTF source code?
A/1/363
LaTeX: Predicting command with alternate tokens
A/1/848
LaTeX: Punctuation clusters ending in \, predicting a command
A/1/1123
LaTeX: " $\", predicting greek letters in formulae
A/1/1277
LaTeX: Predicting command
A/1/1506
LaTeX: Bracket and "\", predicting command (often formatting)
A/1/1911
LaTeX: "(\"
A/1/1917
LaTeX: Superscripted or subscripted backslash, predicting macros in superscripts/subscripts'
A/1/2313
LaTeX: Predicting escaped characters
A/1/2737
LaTeX: "\" and " \" - left/right delimiters, etc.
A/1/3205
LaTeX: " $$\\"
A/1/3471
LaTeX: "^{\" and "_{\", introducing superscripted/subscripted commands
A/1/3517
Back Slash in LaTeX
A/1/3853
LaTeX: Predicting equation symbols
A/1/3891
LaTeX: "{\"
A/1/3894
LaTeX: Subscript with escape
Cluster #109
A/0/173
This feature fires for LaTeX markup, specifically for commands that start an environment or matrix.
A/1/529
LaTeX font setting, primarily \rm
A/1/3789
"usepackage"
Cluster #77
A/0/158
The feature attends to mathematical notation and formulas.
A/0/205
This feature fires on mathematical notation.
A/1/108
LaTeX contents of curly-braced superscripts or subscripts
A/1/356
LaTeX measurements (probably units)
A/1/455
"aligned"
A/1/2452
Contents, esp. digits, of curly braces in superscripts or subscripts, predicting close braces
A/1/3340
LaTeX letters in \mathcal and a few other fonts
A/1/3552
Numbers after open curly braces in LaTeX
A/1/1798
Code: Letters in backslash escapes (n in \n etc.)
A/1/2927
LaTeX fraction numerators
A/1/3384
"69"
A/1/723
LaTeX: macro names for spacing like "qquad" or dots like "ldots"
Cluster #88
A/0/93
This feature attends to text written in LaTeX.
A/1/633
"}" in [context]
A/1/840
" \\\\"/"\\\\"
A/1/1186
Hexadecimal digits in \x escapes
Cluster #98
A/0/185
This feature appears to attend to mathematical equations with numbers, particularly those with the value of 1 or 2.
A/1/788
Numbers after "=" or similar operators, e.g. \approx
A/1/2401
LaTeX exponents (after "^"), esp. 2
A/1/2501
Numbers after "_" (subscripts), often predicting superscripts
Cluster #68
A/1/1285
Physics notation, esp. near square brackets (hints of relativity)
A/1/2221
LaTeX: numbers in matrices/tables/etc.
A/1/2785
This feature attends to mathematical notation with formulas and equations.
Cluster #117
A/0/111
This feature fires when it sees mathematical notation, specifically symbols used in calculus and algebra.
A/0/161
This feature attends to mathematical formulas written in LaTeX.
A/0/499
This feature fires for mathematical variables.
A/1/137
LaTeX big delimiters
A/1/189
LaTeX: "right" command
A/1/1035
Greek letters in LaTeX
A/1/1576
Greek letter macros in LaTeX math mode
A/1/1829
LaTeX: short, mostly single-letter variables
A/1/1929
This feature fires when it sees mathematical equations and LaTeX-style notation.
A/1/2056
LaTeX trigonometric functions
A/1/2512
LaTeX left delimiter opening
A/1/2750
\to in LaTeX
A/1/2800
LaTeX operators (sum, prod, int, etc.)
A/1/2920
LaTeX Math (general)
A/1/3057
Feature for predicting "_{" and similar in LaTeX
A/1/3333
Single-letter tokens/variables in LaTeX math mode
A/1/3574
Greek letters in LaTeX
A/1/3752
One-letter variables, often subscripts, in LaTeX math (predicting superscripts)
A/1/3940
Space + capital letter in LaTeX often a variable after a macro like "\in"
A/1/4044
"partial" in the LaTeX macro "\partial"
A/1/697
Numbers in parentheses, often comma-separated tuples, in LaTeX math mode
A/1/2295
Single-digit numbers in LaTeX math
A/0/144
This feature attends to mathematical formulas. It activates on variables, mathematical constants and operators, as well as formatting and punctuation commonly used in equations.
A/1/2864
This feature fires in response to mathematical formulae, particularly those containing mathematical symbols such as Greek letters.
A/1/596
Latin-alphabet identifiers in LaTeX
A/1/3258
LaTeX: variables in parenthesized expressions or pairs in inline math mode
A/0/167
This feature appears to fire for code examples and programming language keywords, especially in C/C++.
Cluster #75
A/0/275
This feature attends to capital letter A at the beginning of sentences.
A/0/325
This feature fires when it sees a single capital letter followed by a period, most often at the beginning of a sentence.
A/1/654
"S"
A/1/806
This feature attends to acronyms, initialisms and abbreviations in the text.
A/1/1207
'g' in 'e.g.', predicts the next period
A/1/1773
" A" in [context]
A/1/1998
" A" in [context]
A/1/2243
This feature attends to US Court case citations.
A/1/3625
"A"
A/1/4014
"L"
Cluster #121
A/1/784
" X"
A/1/1429
" N"
A/1/2125
" D"
A/1/2776
" V"
Cluster #70
A/0/213
This feature attends to words that contain the letter "b".
A/1/300
" d"
A/1/438
" e"
A/1/476
" w" in [context]
A/1/530
" k"
A/1/738
" m"
A/1/825
" h"
A/1/910
" p"
A/1/1079
" r"
A/1/1779
" j"
A/1/1790
"f"
A/1/2108
" b"
A/1/2265
" t" in [context]
A/1/2332
" s" in [context]
A/1/2335
" l"
A/1/2667
" g"
A/1/3371
" c"
A/1/3579
" v" usually beginning a word
A/1/3730
" n"
A/1/3993
" o"
Cluster #120
A/0/26
This feature fires when it sees initials, especially two initials with a period between them. It attends to proper nouns and names in abbreviation format.
A/0/137
This feature fires on the letter D in text.
A/0/296
This feature fires in response to proper nouns that start with "F" or some other capital letters.
A/0/395
This feature is firing for instances of "X.P" and "P.X" where "X" represents uppercase Roman numerals and "P" represents single digit page numbers.
A/0/479
The feature fires when it sees capital M followed by a capital letter.
A/1/11
" M" esp. shortly after <EOT>
A/1/323
" Z"
A/1/1057
" P"
A/1/1871
" T" in titles/names/English
A/1/3052
" C" in [context]
A/1/3107
" Y"
A/1/3159
" J"
A/1/3289
" G"
A/1/3406
" W"
A/1/3408
" F"
A/1/3428
" L"
A/1/3939
" R"
A/1/732
" H"
A/1/1740
" K"
A/1/1786
" B" in [context]
A/1/748
This feature fires for character names that start with a soft C, D, F, J, K, M, N, P, R, T, V, W, Z sound.
A/1/1010
" S" in [context]
A/1/1718
"C" in [context]
A/0/350
This feature attends to capital S followed by 4-9 capital letters.
Cluster #93
A/0/278
This feature fires for names and words starting with the letter E.
A/1/1377
" O"
A/1/3351
" U"
A/1/4008
" E" in [context]
A/1/3528
"B" esp. after "-" or "/"
Cluster #1
A/0/397
This feature attends to Chinese sentences and phrases.
A/1/374
Chinese: Technical
A/1/900
Chinese: Simplified
A/1/2000
Chinese: Obscure characters / dictionaries / transliterated names
A/1/2030
Japanese: Numbers (dates and laws?)
Cluster #2
A/0/304
This feature fires on Japanese text.
A/1/698
Japanese: Period, end-of-sentence character
A/1/1260
Chinese: Negations / "but"
A/1/2730
Japanese: ?
Cluster #8
A/1/1671
Japanese and Chinese: ?
A/1/2239
Japanese: "そ"?
A/1/3655
Japanese, Chinese, and Korean: ?
Cluster #5
A/1/2602
Japanese: The prefix the "suru" verb that means "to do", when used in a bunch of contexts, but _without_ the conjugation (ie, positive/negative, future/past, volitional/not, want-to/not, etc)
A/1/2986
Japanese: The "です desu" copula that means "is". It represents "X is Y" in "X Y desu"
A/1/3112
Japanese: Suffix that noun-ize a verb, similar to "ing" suffix in english
A/1/3605
Japanese: A form of the "to do" verb prefix. Usually passive tense, occasionally "can/able to do", and sometimes "is doing"
A/1/3927
Japanese: Hiragana "e" character, predicting things likely to come after that
A/1/3965
Japanese: "な"
Cluster #9
A/0/343
This feature fires when it sees Japanese text.
A/1/63
Japanese: Hiragana particle that follows a noun, representing it was a direct or indirect object
A/1/380
Japanese: Seems to be the "to X" form of the verb, represented in an impolite form, sometimes used in conjunction with suffixes that "noun-ize" the verb
A/1/421
Japanese: Specialized - game/IRC chats
A/1/964
Japanese: Katakana characters representing foreign-loan words
A/1/2850
Japanese: Hiragana ha/wa character, that is particle representing the phrase before it is a "topic"
A/1/3829
Japanese: Hiragana "no" that represents possessive conjunction, similar to "apostrophe-s" in English
Cluster #22
A/0/267
This feature fires on words that indicate a transition in a sentence, such as prepositions like "to", "in", "for", etc. It seems to be particularly attentive to the word "to".
A/0/331
This feature attends to the use of the preposition ``to'' in a variety of functional contexts.
A/1/297
" to" in [context]
A/1/390
" to" in [context]
A/1/975
" to" in [context]
A/1/990
" to" in [context]
A/1/1541
" to" in [context]
A/1/2169
" To"
A/1/2439
" to" in [context]
A/1/2543
" to" in [context]
A/1/3613
" to" in [context]
A/1/3854
" to" in [context]
A/1/3484
"to" in [context]
A/1/2240
" to" in [context]
Cluster #23
A/0/226
The feature fires when the word "to" is used to indicate an infinitive verb.
A/1/1262
" to" in [context]
A/1/1364
" to" in [context]
A/1/1802
" to" in [context]
A/1/1832
" to" in [context]
A/1/2063
" to" in [context]
A/1/2191
" to" in legal
A/1/2361
" to" in [context]
A/1/2653
[Ultralow density cluster]
A/1/2666
" to" in [context]
A/1/2688
" to" in [context]
A/1/2960
" to" in [context]
A/1/3018
" to" in [context]
A/1/3297
" to" in [context]
A/1/3778
" to" in [context]
A/1/3805
" to" in infinitive phrases (verb " to" verb)
A/1/4011
" to" in [context]
A/1/2172
" to" in [context]
A/1/3956
" to" in [context]
A/1/1047
" to" in [context]
A/1/905
" to" in [context]
scroll zoom
home
zoom in
zoom out
A/0/295
A/1/14
A/1/207
A/1/300
A/1/334
A/1/593
A/1/1015
A/1/1336
A/1/1393
A/1/1411
A/1/1506
A/1/1524
A/1/1648
A/1/1851
A/1/2000
A/1/2030
A/1/2095
A/1/2361
A/1/2482
A/1/2625
A/1/2645
A/1/2782
A/1/3041
A/1/3042
A/1/3205
A/1/3302
A/1/3333
A/1/3471
A/1/3474
A/1/3568
A/1/3573
A/1/3579
A/1/3875
A/1/3951
A/1/3965
A/1/3967
A/1/4082
Summary of Results
Sparse Autoencoders extract relatively monosemantic features. We provide four different lines of evidence: detailed investigations for a few features firing in specific contexts for which we can construct computational proxies, human analysis for a large random sample of features, automated interpretability analysis of activations for all the features learned by the autoencoder, and finally automated interpretability analysis of logit weights for all the features. Moreover, the last three analyses show that most learned features are interpretable. While we do not claim that our interpretations catch all aspects of features' behaviors, by constructing metrics of interpretability consistently for features and neurons, we quantitatively show their relative interpretability.
Sparse autoencoders produce interpretable features that are effectively invisible in the neuron basis. We find features (e.g., one firing on Hebrew script) which are not active in any of the top dataset examples for any of the neurons.
Sparse autoencoder features can be used to intervene on and steer transformer generation. For example, activating the base64 feature we study causes the model to generate base64 text, and activating the Arabic script feature we study produces Arabic text. (See discussion of pinned feature sampling in Global Analysis.)
Sparse autoencoders produce relatively universal features. Sparse autoencoders applied to different transformer language models produce mostly similar features, more similar to one another than they are to their own model's neurons. (See Universality)
Features appear to "split" as we increase autoencoder size. When we gradually increase the width of the autoencoder from 512 (the number of neurons) to over 131,000 (256×), we find features which naturally fit together into families. For example, one base64 feature in a small dictionary splits into three, with more subtle and yet still interpretable roles, in a larger dictionary. The different size autoencoders offer different "resolutions" for understanding the same object. (See Feature Splitting.)
Just 512 neurons can represent tens of thousands of features. Despite the MLP layer being very small, we continue to find new features as we scale the sparse autoencoder.
Features connect in "finite-state automata"-like systems that implement complex behaviors. For example, we find features that work together to generate valid HTML. (See "Finite State Automata".)





Problem Setup
A key challenge to our agenda of reverse engineering neural networks is the curse of dimensionality: as we study ever-larger models, the volume of the latent space representing the model's internal state that we need to interpret grows exponentially. We do not currently see a way to understand, search or enumerate such a space unless it can be decomposed into independent components, each of which we can understand on its own.

In certain limited cases, it is possible to side step these issues by rewriting neural networks in ways that don't make reference to certain hidden states. For example, in A Mathematical Framework for Transformer Circuits 
[18]
, we were able to analyze a one-layer attention-only network without addressing this problem. But this approach becomes impossible if we consider even a simple standard one-layer transformer with an MLP layer with a ReLU activation function. Understanding such a model requires us to have a way to decompose the MLP layer.

In some sense, this is the simplest language model we profoundly don't understand. And so it makes a natural target for our paper. We aim to take its MLP activations – the activations we can't avoid needing to decompose – and decompose them into "features":


Crucially, we decompose into more features than there are neurons. This is because we believe that the MLP layer likely uses superposition 
[2, 3, 4, 5]
 to represent more features than it has neurons (and correspondingly do more useful non-linear work!). In fact, in our largest experiments we'll expand to have 256 times more features than neurons, although we'll primarily focus on a more modest 8× expansion.

Transformer

Sparse Autoencoder

Layers

1 Attention Block
1 MLP Block (ReLU)

1 ReLU (up)
1 Linear (down)

MLP Size

512

512 (1×) – 131,072 (256×)

Dataset

The Pile
[19]

(100 billion tokens)

Transformer MLP Activations
(8 billion samples)

Loss

Autoregressive Log-Likelihood

L2 reconstruction
+ L1 on hidden layer activation

In the following subsections, we will motivate this setup at more length. Additionally, a more detailed discussion of the architectural details and training of these models can be found in the appendix.

Features as a Decomposition
There is significant empirical evidence suggesting that neural networks have interpretable linear directions in activation space. This includes classic work by Mikolov et al. 
[20]
 investigating "vector arithmetic" in word embeddings (but see 
[21]
) and similar results in other latent spaces (e.g. 
[22]
). There is also a large body of work investigating interpretable neurons, which are just basis dimensions (e.g. in RNNs 
[23, 24]
; in CNNs 
[25, 26, 27]
; in GANs 
[28]
; but see 
[29, 30]
). A longer review and discussion of this work can be found in the Motivation section of Toy Models 
[5]
, although it doesn't cover more recent work (e.g. 
[31, 32, 33]
). 2

If linear directions are interpretable, it's natural to think there's some "basic set" of meaningful directions which more complex directions can be created from. We call these directions features, and they're what we'd like to decompose models into. Sometimes, by happy circumstances, individual neurons appear to be these basic interpretable units (see examples above). But quite often, this isn't the case.

Instead, we decompose the activation vector 
x
j
x 
j
  as a combination of more general features which can be any direction:

x
j
≈
b
+
∑
i
f
i
(
x
j
)
d
i
x 
j
 ≈b+ 
i
∑
​
 f 
i
​
 (x 
j
 )d 
i
​
 
(1)
where 
x
j
x 
j
  is the activation vector of length 
d
M
L
P
d 
MLP
​
  for datapoint 
j
j, 
f
i
(
x
j
)
f 
i
​
 (x 
j
 ) is the activation of feature 
i
i, each 
d
i
d 
i
​
  is a unit vector in activation space we call the direction of feature 
i
i, and 
b
b is a bias. 3 Note that this decomposition is not new: it is just a linear matrix factorization of the kind commonly employed in dictionary learning.

In our sparse autoencoder setup, the feature activations are the output of the encoder

f
i
(
x
)
=
ReLU
(
W
e
(
x
−
b
d
)
+
b
e
)
i
,
f 
i
​
 (x)=ReLU(W 
e
​
 (x−b 
d
​
 )+b 
e
​
 ) 
i
​
 ,

where 
W
e
W 
e
​
  is the weight matrix of the encoder and 
b
d
b 
d
​
 , 
b
e
b 
e
​
  are a pre-encoder and an encoder bias. The feature directions are the columns of the decoder weight matrix 
W
d
W 
d
​
 . (See appendix for full notation.)

If such a sparse decomposition exists, it raises an important question: are models in some fundamental sense composed of features or are features just a convenient post-hoc description? In this paper, we take an agnostic position, though our results on feature universality suggest that features have some existence beyond individual models.

Superposition Hypothesis
To see how this decomposition relates to superposition, recall that the superposition hypothesis postulates that neural networks “want to represent more features than they have neurons”. We think this happens via a kind of “noisy simulation”, where small neural networks exploit feature sparsity and properties of high-dimensional spaces to approximately simulate much larger much sparser neural networks 
[5]
.


A consequence of this is that we should expect the feature directions to form an overcomplete basis. That is, our decomposition should have more directions 
d
i
d 
i
​
  than neurons. Moreover, the feature activations should be sparse, because sparsity is what enables this kind of noisy simulation. This is mathematically identical to the classic problem of dictionary learning.

What makes a good decomposition?
Suppose that a dictionary exists such that the MLP activation of each datapoint is in fact well approximated by a sparse weighted sum of features as in equation 1. That decomposition will be useful for interpreting the neural network if:

We can interpret the conditions under which each feature is active. That is, we have a description of which datapoints 
j
j cause the feature to activate (i.e. for which 
f
i
(
x
j
)
f 
i
​
 (x 
j
 ) is large) that makes sense on a diverse dataset (including potentially synthetic or off-distribution examples meant to test the hypothesis). 4
We can interpret the downstream effects of each feature, i.e. the effect of changing the value of 
f
i
f 
i
​
  on subsequent layers. This should be consistent with the interpretation in (1).
The features explain a significant portion of the functionality of the MLP layer (as measured by the loss; see How much of the model does our interpretation explain?).
A feature decomposition satisfying these criteria would allow us to:

Determine the contribution of a feature to the layer’s output, and the next layer’s activations, on a specific example.
Monitor the network for the activation of a specific feature (see e.g. speculation about safety-relevant features).
Change the behavior of the network in predictable ways by changing the values of some features. In multilayer models this could look like predictably influencing one layer by changing the feature activations in an earlier layer.
Demonstrate that the network has learned certain properties of the data.
Demonstrate that the network is using a given property of the data in producing its output on a specific example. 5
Design inputs meant to activate a given feature and elicit certain outputs.
Of course, decomposing models into components is just the beginning of the work of mechanistic interpretability! It provides a foothold on the inner workings of models, allowing us to start in earnest on the task of unraveling circuits and building a larger-scale understanding of models.

Why not use architectural approaches?
In Toy Models of Superposition 
[5]
, we highlighted several different approaches to solving superposition. One of those approaches was to engineer models to simply not have superposition in the first place.

Initially, we thought that this might be possible but come with a large performance hit (i.e. produce models with greater loss). Even if this performance hit had been too large to use in practice for real models, we felt that success at creating monosemantic models would have been very useful for research, and in a lot of ways this felt like the "cleanest" approach for downstream analysis.

Unfortunately, having spent a significant amount of time investigating this approach, we have ultimately concluded that it is more fundamentally non-viable.

In particular, we made several attempts to induce activation sparsity during training to produce models without superposition, even to the point of training models with 1-hot activations. This indeed eliminates superposition, but it fails to result in cleanly-interpretable neurons! Specifically, we found that individual neurons can be polysemantic even in the absence of superposition. This is because in many cases models achieve lower loss by representing multiple features ambiguously (in a polysemantic neuron) than by representing a single feature unambiguously and ignoring the others.

To understand this, consider a toy model with a single neuron trained on a dataset with four mutually-exclusive features (A/B/C/D), each of which makes a distinct (correct) prediction for the next token, labeled in the same fashion. Further suppose that this neuron’s output is binary: it either fires or it doesn’t. When it fires, it produces an output vector representing the probabilities of the different possible next tokens.

We can calculate the cross-entropy loss achieved by this model in a few cases:

Suppose the neuron only fires on feature A, and correctly predicts token A when it does. The model ignores all of the other features, predicting a uniform distribution over tokens B/C/D when feature A is not present. In this case the loss is 
3
4
ln
3
≈
0
.
8
4
3
​
 ln3≈0.8.
Instead suppose that the neuron fires on both features A and B, predicting a uniform distribution over the A and B tokens. When the A and B features are not present, the model predicts a uniform distribution over the C and D tokens. In this case the loss is 
ln
2
≈
0
.
7
ln2≈0.7.
Because the loss is lower in case (2) than in case (1), the model achieves better performance by making its sole neuron polysemantic, even though there is no superposition.

This example might initially seem uninteresting because it only involves one neuron, but it actually points at a general issue with highly sparse networks. If we push activation sparsity to its limit, only a single neuron will activate at a time. We can now consider that single neuron and the cases where it fires. As seen earlier, it can still be advantageous for that neuron to be polysemantic.  

Based on this reasoning, and the results of our experiments, we believe that models trained on cross-entropy loss will generally prefer to represent more features polysemantically than to represent fewer "true features" monosemantically, even in cases where sparsity constraints make superposition impossible.

Models trained on other loss functions do not necessarily suffer this problem. For instance, models trained under mean squared error loss (MSE) may achieve the same loss for both polysemantic and monosemantic representations (e.g. 
[35]
), and for some feature importance curves they may actively prefer the monosemantic solution 
[36]
. But language models are not trained with MSE, and so we do not believe architectural changes can be used to create fully monosemantic language models.

Note, however, that in learning to decompose models post-training we do use an MSE loss (between the activations and their representation in terms of the dictionary), so sparsity can inhibit superposition from forming in the learned dictionary. (Otherwise, we might have superposition "all the way down.")

Using Sparse Autoencoders To Find Good Decompositions
There is a long-standing hypothesis that many natural latent variables in the world are sparse (see 
[37]
, section "Why Sparseness?"). Although we have a limited understanding of the features that exist in language models, the examples we do have (e.g. 
[38]
) are suggestive of highly sparse variables. Our work on Toy Models of Superposition 
[5]
 shows that a large set of sparse features could be represented in terms of directions in a lower dimensional space.

For this reason, we seek a decomposition which is sparse and overcomplete. This is essentially the problem of sparse dictionary learning
[39, 37]
. (Note that, since we're only asking for a sparse decomposition of the activations, we make no use of the downstream effects of 
d
i
d 
i
​
  on the model's output. This means we'll be able to use those downstream effects in later sections as a kind of validation on the features found.)

It's important to understand why making the problem overcomplete – which might initially sound like a trivial change – actually makes this setting very different from similar approaches seeking sparse disentanglement in the literature. It's closely connected to why dictionary learning is such a non-trivial operation; in fact, as we'll see, it's actually kind of miraculous that this is possible at all. At the heart of dictionary learning is an inner problem of computing the feature activations 
f
i
(
x
)
f 
i
​
 (x) for each datapoint 
x
x, given the feature directions 
d
i
d 
i
​
 . On its surface, this problem may seem impossible: we're asking to determine a high-dimensional vector from a low-dimensional projection. Put another way, we're trying to invert a very rectangular matrix. The only thing which makes it possible is that we are looking for a high-dimensional vector that is sparse! This is the famous and well-studied problem of compressed sensing, which is NP-hard in its exact form. It is possible to store high-dimensional sparse structure in lower-dimensional spaces, but recovering it is hard.

Despite its difficulty, there are a host of sophisticated methods for dictionary learning (e.g. 
[40, 41]
). These methods typically involve optimizing a relaxed problem or doing a greedy search. We tried several of these classic methods, but ultimately decided to focus on a simpler sparse autoencoder approximation of dictionary learning (similar to Sharkey, et al. 
[11]
). This was for two reasons. First, a sparse autoencoder can readily scale to very large datasets, which we believe is necessary to characterize the features present in a model trained on a large and diverse corpus. 6 Secondly, we have a concern that iterative dictionary learning methods might be "too strong", in the sense of being able to recover features from the activations which the model itself cannot access. Exact compressed sensing is NP-hard, which the neural network certainly isn't doing. By contrast, a sparse autoencoder is very similar in architecture to the MLP layers in language models, and so should be similarly powerful in its ability to recover features from superposition.

Sparse Autoencoder Setup
We briefly overview the architecture and training of our sparse autoencoder here, and provide further details in Basic Autoencoder Training. Our sparse autoencoder is a model with a bias at the input, a linear layer with bias and ReLU for the encoder, and then another linear layer and bias for the decoder. In toy models we found that the bias terms were quite important to the autoencoder’s performance. 7

We train this autoencoder using the Adam optimizer to reconstruct the MLP activations of our transformer model, with an MSE 8 loss plus an L1 penalty to encourage sparsity.

In training the autoencoder, we found a couple of principles to be quite important. First, scale really matters. We found that training the autoencoder on more data made features subjectively “sharper” and more interpretable. In the end, we decided to use 8 billion training points for the autoencoder (see Autoencoder Dataset).

Second, we found that over the course of training some neurons cease to activate, even across a large number of datapoints. We found that “resampling” these dead neurons during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension. Our resampling procedure is detailed in Neuron Resampling, but in brief we periodically check for neurons which have not fired in a significant number of steps and reset the encoder weights on the dead neurons to match data points that the autoencoder does not currently represent well.

For readers looking to apply this approach, we supply an appendix with Advice for Training Sparse Autoencoders.

How can we tell if the autoencoder is working?
Usually in machine learning we can quite easily tell if a method is working by looking at an easily-measured quantity like the test loss. We spent quite some time searching for an equivalent metric to guide our efforts here, and unfortunately have yet to find anything satisfactory.

We began by looking for an information-based metric, so that we could say in some sense that the best factorization is the one that minimizes the total information of the autoencoder and the data. Unfortunately, this total information did not generally correlate with subjective feature interpretability or activation sparsity. (Runs whose feature activations had an average L0 norm in the hundreds but low reconstruction error could have lower total information than those with smaller average L0 norm and higher reconstruction error.)

Thus we ended up using a combination of several additional metrics to guide our investigations:

Manual inspection: Do the features seem interpretable?
Feature density: we found that the number of “live” features and the percentage of tokens on which they fire to be an extremely useful guide. (See appendix for details.)
Reconstruction loss: How well does the autoencoder reconstruct the MLP activations? Our goal is ultimately to explain the function of the MLP layer, so the MSE loss should be low.
Toy models: Having toy models where we know the ground truth and so can cleanly evaluate the autoencoder’s performance was crucial to our early progress.
Interpreting or measuring some of these signals can be difficult, though. For instance, at various points we thought we saw features which at first didn’t make any sense, but with deeper inspection we could understand. Likewise, while we have identified some desiderata for the distribution of feature densities, there is much that we still do not understand and which prevents this from providing a clear signal of progress.

We think it would be very helpful if we could identify better metrics for dictionary learning solutions from sparse autoencoders trained on transformers.

The (one-layer) model we're studying
We chose to study a one-layer transformer model. We view this model as a testbed for dictionary learning, and in that role it brings three key advantages:

Because one-layer models are small they likely have fewer "true features" than larger models, meaning features learned by smaller dictionaries might cover all their "true features". Smaller dictionaries are cheaper to train, allowing for fast hyperparameter optimization and experimentation.
We can highly overtrain a one-layer transformer quite cheaply. We hypothesize that a very high number of training tokens may allow our model to learn cleaner representations in superposition.
We can easily analyze the effects features have on the logit outputs, because these are approximately linear in the feature activations. 9 This provides helpful corroboration that the features we have found are not just telling us about the data distribution, and actually reflect the functionality of the model.
We trained two one-layer transformers with the same hyperparameters and datasets, differing only in the random seed used for initialization. We then learned dictionaries of many different sizes on both transformers, using the same hyperparameters for each matched pair of dictionaries but training on the activations of different tokens for each transformer.

We refer to the main transformer we study in this paper as the “A” transformer. We primarily use the other transformer (“B”) to study feature universality, as we can e.g. compare features learned from the “A” and “B” transformers and see how similar they are.

Notation for Features
Throughout this draft, we'll use strings like "A/1/2357" to denote features. The first portion "A" or "B" denote which model the features come from. The second part (e.g. the "1" in "A/1") denotes the dictionary learning run. These vary in the number of learned factors and the L1 coefficient used. A table of all of our runs is available here. Notably, A/0…A/5 form a sequence with fixed L1 coefficients and increasing dictionary sizes. The final portion (e.g. the "2357" in "A/1/2357") corresponds to the specific feature in the run.

Sometimes, we want to denote neurons from the transformer rather than features learned by the sparse autoencoder. In this case, we use the notation "A/neurons/32".

Interface for Exploring Features
We provide an interface for exploring all the features in all our dictionary learning runs. Links to the visualizations for each run can be found here. We suggest beginning with the interface for A/1, which we discuss the most.

These interfaces provide extensive information on each feature. This includes examples of when they activate, what effect they have on the logits when they do, examples of how they affect the probability of tokens if the feature is ablated, and much more:


Our interface also allows users to search through features:


Additionally, we provide a second interface displaying all features active on a given dataset example. This is available for a set of example texts.







Detailed Investigations of Individual Features
The most important claim of our paper is that dictionary learning can extract features that are significantly more monosemantic than neurons. In this section, we give a detailed demonstration of this claim for a small number of features which activate in highly specific contexts.

The features we study respond to

Text written in Arabic script (like "الكتاب المختصر في حساب الجبر والمقابلة")
DNA sequences (like "CCTGGTACTGTACGAACGAACGAACGTAGCCTTGG")
base64 strings (like the final characters in "https://www.youtube.com/watch?v=dQw4w9WgXcQ")
Text written in Hebrew script (like "בראשית ברא אלהים את השמים ואת הארץ")
For each learned feature, we attempt to establish the following claims:

The learned feature activates with high specificity for the hypothesized context. (When the feature is on the context is usually present.)
The learned feature activates with high sensitivity for the hypothesized context. (When the context is present, the feature is usually on.)
The learned feature causes appropriate downstream behavior.
The learned feature does not correspond to any neuron.
The learned feature is universal – a similar feature is found by dictionary learning applied to a different model.
To demonstrate claims 1–3, we devise computational proxies for each context, numerical scores estimating the (log-)likelihood that a string (or token) is from the specific context. The contexts chosen above are easy to model based on the defined sets of unicode characters involved. We model DNA sequences as random strings of characters from [ATCG] and we model base64 strings as random sequences of characters from [a-zA-Z0-9+/]. For Arabic script and Hebrew features, we exploit the fact that each language is written in a script consisting of well-defined Unicode blocks. Each computational proxy is then an estimate of the log-likelihood ratio of a string under the hypothesis versus under the full empirical distribution of the dataset. The full description of how we estimate 
log
(
P
(
s
∣
context
)
/
P
(
s
)
)
log(P(s∣context)/P(s)) for each feature hypothesis is given in the appendix section on proxies.

In this section we primarily study the learned feature which is most active in each context. There are typically other features that also model that context, and we find that rare “gaps” in the sensitivity of a main feature are often explained by the activity of another. We discuss this phenomenon in detail in sections on Activation Sensitivity and Feature Splitting.

We take pains to demonstrate the specificity of each feature, as we believe that to be more important for ruling out polysemanticity. Polysemanticity typically involves neurons activating for clearly unrelated concepts. 10 If our proxy shows that a feature only activates in some relatively rare and specific context, that would exclude the typical form of polysemanticity.


We finally note that the features in this section are cherry-picked to be easier to analyze. Defining simple computational proxies for most features we find, such as text concerning fantasy games, would be difficult, and we analyze them in other ways in the following section.

Arabic Script Feature
The first feature we'll consider is an Arabic Script feature, A/1/3450. It activates in response to text in Arabic, Farsi, Urdu (and possibly other languages), which use the Arabic script. This feature is quite specific and relatively sensitive to Arabic script, and effectively invisible if we view the model in terms of individual neurons.

Activation Specificity
Our first step is to show that this feature fires almost exclusively on text in Arabic script. We give each token an "Arabic script" score using an estimated likelihood ratio 
log
(
P
(
s
∣
Arabic Script
)
/
P
(
s
)
)
log(P(s∣Arabic Script)/P(s)), and break down the histogram of feature activations by that score. Arabic text is quite rare in our overall data distribution –  just 0.13% of training tokens — but it makes up 81% of the tokens on which our feature is active. That percentage varies significantly by feature activity level, from 25% when the feature is barely active to 98% when the feature activation is above 5.


We also show dataset examples demonstrating different levels of feature activity. In interpreting them, it's important to note that Arabic Unicode characters are often split into multiple tokens. For example, the character ث (U+062B) is tokenized as \xd8 followed by \xab. 11  When this happens, A/1/3450 typically only fires on the last token of the Unicode character.

The upper parts of the activation spectrum, above an activity of ~5, clearly respond with high specificity to Arabic script. What should we make of the lower portions? We have three hypotheses:

The proxy is imperfect. It has false negatives due to common characters which are in other Unicode blocks (e.g., whitespace, punctuation), and also due to places where tokenization has created strange behavior. 12
The model may be imperfect (but still calibrated). If features activate proportional to their "confidence" that some property is present, then we should expect them to sometimes be wrong in their weak activations. Although it might seem silly to fail to recognize which language a piece of text is, this is a very weak one layer transformer model, and the fact that characters are often split into multiple tokens adds additional difficulty.
The autoencoder may be imperfect: If the width of our autoencoder is less than the number of "true features" being used by the model, then unrecovered features may show up as low activations across many of our learned features.
Regardless, large feature activations have larger impacts on model predictions, 13 so getting their interpretation right matters most. One useful tool for assessing the impact of false positives at low activation levels is the "expected value plot" of Cammarata et al. 
[25]
. We plot the distribution of feature activations weighted by activation level. Most of the magnitude of activation provided by this feature comes from dataset examples which are in Arabic script.


Activation Sensitivity
In the Feature Activation Distribution above, it's clear that A/1/3450 is not sensitive to all tokens in Arabic script. In the random dataset examples, it fails to fire on five examples of the prefix "ال", transliterated as "al-", which is the equivalent of the definite article "the" in English. However, in exactly those places, another feature which is specific to Arabic script, A/1/3134, fires. There are several additional features that fire on Arabic and related scripts (e.g. A/1/1466, A/1/3134, A/1/3399) which contribute to representing Arabic script. Another example deals with Unicode tokenization: when Arabic characters are split into multiple tokens, the feature we analyze here only activates at the final token comprising the character, while A/1/3399 activates on the first token comprising the character.  To see how these features collaborate, we provide an alternative visualization showing all the features active on a snippet of Arabic text. We consider such interactions more in the Phenomenology section below.

Nevertheless, we find a Pearson correlation of 0.74 between the activity of our feature and the activity of the Arabic script proxy (thresholded at 0), over a dataset of 40 million tokens. Correlation provides a joint measure of sensitivity and specificity that takes magnitude into account, and 0.74 is a substantial correlation.

Feature Downstream Effects
Because the autoencoder is trained on model activations, the features it learns could in theory represent structure in the training data alone, without any relevance to the network’s function. We show instead that the learned features have interpretable causal effects on model outputs which make sense in light of the features’ activations. Note that these downstream effects are not inputs to the dictionary learning process, which only sees the activations of the MLP layer. If the resulting features also mediate important downstream behavioral effects then we can be confident that the feature is truly connected to the MLP’s functional role in the network and not just a property of the underlying data.

We begin with a linear approximation to the effect of each feature on the model logits. We compute the logit weight following the path expansion approach of 
[18]
, 14 multiplying each feature direction by the MLP output weights, an approximation of the layer norm operation combining a projection to remove the mean and a diagonal matrix approximating the scaling terms, and the unembedding matrix (
d
i
W
d
o
w
n
π
L
W
u
n
e
m
b
e
d
d 
i
​
 W 
down
​
 πLW 
unembed
​
 ). Because the softmax function is shift-invariant there is no absolute scale for the logit weights; we shift these so that the median logit weight for each feature is zero.

Each feature, when active, makes some output tokens more likely and some output tokens less likely. We plot that distribution of logit weights. 15 There is a large primary mode at zero, and a second much smaller mode on the far right, the tokens whose likelihood most increases when our feature is on. This second mode appears to correspond to Arabic characters, and tokens which help represent Arabic script characters (especially \xd8 and \xd9, which are often the first half of the UTF-8 encodings of Arabic Unicode characters in the basic Arabic Unicode block).


This suggests that activating this feature increases the probability the network predicts Arabic script tokens. 16

To visualize these effects on actual data, we causally ablate the feature. For a given dataset example, we run the context through the model until the MLP layer, decode the activations into features, then subtract off the activation of A/1/3450, artificially setting it to zero on the whole context, before applying the rest of the model. We visualize the effect of ablating the feature using underlines in the visualization; tokens whose predictions were helped by the feature (ablation decreased likelihood) are underlined in blue and tokens whose predictions were hurt by the feature (ablation increased likelihood) are underlined in red.

In the example on the right below we see that the A/1/3450 was active on every token in a short context (orange background). Ablating it hurt the predictions of all the tokens in Arabic script (purple underlines), but helped the prediction of the period . (orange underline). The rest of the figure displays contexts from two different ranges of feature activation levels. (The feature activation on the middle token of examples on the right ("subsample interval 5") is about half that of the middle token of examples on the left ("subsample interval 0")). We see that the feature was causally helping the model predictions on Arabic script through that full range, and the only tokens made less likely by the feature are punctuation shared with other scripts. The magnitudes of the impact are larger when the feature is more active.


We encourage interested readers to view the feature visualization for A/1 to review this and other effects.

We also validate that the feature's downstream effect is in line with our interpretation as an Arabic script feature by sampling from the model with the feature activity "pinned" at a high value. To do this, we start with a prefix 1,2,3,4,5,6,7,8,9,10 where the model has an expected continuation (keep in mind that this is a one layer model that is very weak!). We then instead set A/1/3450 to its maximum observed value and see how that changes the samples:


The feature is not a neuron
This feature seems rather monosemantic, but some models have relatively monosemantic neurons, and we want to check that dictionary learning didn't merely hand us a particularly nice neuron. 17 We first note that when we search for neurons with Arabic script in their top dataset 20 examples, only one neuron turns up, with a single example in Arabic script. (Eighteen of the remaining examples are in English, and one is in Cyrillic.)

We then look at the coefficients of the feature in the neuron basis, and find that the three largest coefficients by magnitude are all negative (!) and there are a full 27 neurons whose coefficients are at least 0.1 in magnitude.


It is of course possible that these neurons engage in a delicate game of cancellation, resulting in one particular neuron's primary activations being sharpened. To check for this, we find the neuron whose activations are most correlated to the feature's activations over a set of ~40 million dataset examples. 18 The most correlated neuron (A/neurons/489) responds to a mixture of different non-English languages. 19 We can see this by visualizing the activation distribution of the neuron – the dataset examples are all other languages, with Arabic script present only as a small sliver.


Logit weight analysis is also consistent with this neuron responding to a mixture of languages. For example, in the figure below many of the top logit weights appear to include Russian and Korean tokens. Careful readers will observe a thin red sliver corresponding to rare Arabic script tokens in the distribution. These Arabic script tokens have weight values that are very slightly positive leaning overall, but some are negative.


Finally, scatter plots and correlations suggest the similarities between A/1/3450 and the neuron are non-zero, but quite minimal. 20 In particular, note how the y-axis of the logit weights scatter plot (corresponding to the feature) cleanly separates the Arabic script token logits, while the x-axis does not. More generally, note how the y-marginals both clearly exhibit specificity, but the x-marginals do not.


We conclude that the features we study do not trivially correspond to a single neuron. The Arabic script feature would be effectively invisible if we only analyzed the model in terms of neurons.

Universality
We will now ask whether A/1/3450 is a universal feature that forms in other models and can be consistently discovered by dictionary learning. This would indicate we are discovering something more general about how one-layer transformers learn representations of the dataset.

We search for a similar feature in B/1, a dictionary learning run on a transformer trained on the same dataset but with a different random seed. We search for the feature with the highest activation correlation 21  and find B/1/1334 (corr=0.91), which is strikingly similar:


This feature clearly responds to Arabic script as well. If anything, it's nicer than our original feature – it's more specific in the 0–1 range. The logit weights tell a similar story:


The effects of ablating this feature are also consistent with this (see the visualization for B/1/1334).

To more systematically analyze the similarities between A and B, we look at scatter plots comparing the activations or logit weights:


The activations are strongly correlated (Pearson correlation of 0.91), especially in the main Arabic mode.

The logit weights reveal a two-dimensional version of the bimodality we saw in the histogram for A/1, with logit weights for Arabic tokens clustering at the top right. The correlation is more modest than that of the activations because the distribution is dominated by a relatively uncorrelated mode in the center. We hypothesize this central mode corresponds to "weight interference" and that the shared outlier mode is the important observation – that is, the model may ideally prefer to have all those weights be zero, but due to superposition with other features and their weights, this isn't possible.

DNA Feature
We now consider a DNA feature, A/1/2937. It activates in response to long uppercase strings consisting of A, T, C, and G, typically used to represent nucleotide sequences. We closely follow the analysis of the Arabic script feature above to show activation specificity and sensitivity for the feature, sensible downstream effects, a lack of neuron alignment, and universality between models. The main differences will be that (1)  A/1/2937 is the only feature devoted to modeling the DNA context, and (2) our proxy is less sensitive to DNA than our feature is, missing strings containing punctuation, spaces, and missing bases.

Activation Specificity and Sensitivity
We begin with the computational proxy for "is a DNA sequence", 
log
(
P
(
s
∣
DNA
)
/
P
(
s
)
)
log(P(s∣DNA)/P(s)). Because there are some vocabulary tokens, such as CAT, which could occur in DNA but also occur in other contexts, we always look at groups of at least two tokens when evaluating the proxy. The log-probabilities turn out to be quite bimodal, so we binarize the proxy (based on its sign). This binarized proxy then has a Pearson correlation of 0.8 with the feature activations.


While the feature appears to be quite monosemantic in the feature's higher registers (all 10 random dataset examples about activation of 6.0 are DNA sequences), there is significant blue indicating the DNA proxy not firing in the lower registers. Below we show a grid of random examples at four activation levels (including feature off) where the proxy does and doesn't fire.


We note that in all but two cases where the feature and proxy disagree, the feature does indicate a DNA sequence, just one outside our proxy's strict ATCG vocabulary. For example, the space present in the triplets  TGG AGT makes the proxy fail to fire. The feature also fires productively on '- in the string 5'-TCT, because what follows that prefix should be DNA, even though the prefix is not itself DNA. (The causal ablation reveals that turning off the DNA feature hurts the prediction of the strings that follow.) We thus believe that A/1/2937 is quite sensitive and specific for DNA. The case where the proxy fires and the feature does not is indeed a DNA sequence, though the feature begins firing on the very next token. We observe that the DNA feature may not fire on the first few tokens of a DNA sequence, but by the end of a long DNA sequence, it is the only feature active.

Feature Downstream Effect
The downstream effect of the DNA feature being active, as measured by logit weights, make sense, with all the top tokens being combinations of nucleotides like AGT and GCC.


The Feature Is Not A Neuron
The most similar neuron to A/1/2937, as measured by activation correlation, is A/neurons/67. DNA contexts form a tiny sliver of that neuron's activating examples. The neuron whose coefficient is the highest in our feature's vector, A/neurons/227, also has no DNA sequences in its top activating examples.


Universality
A/1/2937 has a correlated feature (corr=0.92) in run B/1, B/1/3680. Their top logit weights agree, and are DNA tokens (e.g. AGT) forming a separate mode (circled on the right) than the bulk of the logit weights for both.


Base64 Feature
We now consider a base64 feature, A/1/2357. We're particularly excited by this feature because we discovered a base64 neuron in our SoLU paper 
[38]
, suggesting that base64 might be quite universal – even across models trained on somewhat different datasets and with different architectures. We model base64 strings as random sequences of characters from [a-zA-Z0-9+/]. The activation distribution colored by the corresponding computational proxy, together with the random dataset examples from each activation level, shows that this feature is quite specific to base64.


This is not the only feature active in base64 contexts, and in a section below we discuss the two others, one of which fires on single digits in base64 contexts (like the 2, 4, 7, and 9 on which A/1/2357 doesn't activate in the figure above), exploiting a property of the BPE tokenizer to make a better prediction.

Turning to the logit weights, they have a second mode consisting of highly base64-specific tokens. The main mode seems to primarily be interference, but the right side is skewed towards base64-neutral or slightly base64-leaning tokens. (If we look at the conditional below, we see a more continuous transition to base64-specific tokens.)


There is a more continuous transition between non-base64 and base64 tokens than we saw in the Arabic script example. This difference likely arises because whether a token occurs more in Arabic script than in other text is a relatively binary distinction, whereas whether a token occurs more in base64 or other text varies more continuously. For instance, fr is both a common abbreviation for the French language and also a base64 token, so it makes sense for the model to be cautious in up-weighting fr because it might already have a higher prior due to use in French. Indeed, any token consisting of letters from the English alphabet will have some nontrivial probability of appearing in base64 strings.

The Pearson correlation between the computational proxy and the activity of A/1/2357 is just 0.38. We believe that is mostly because the proxy is too broad. For example hexadecimal strings (those made of [0-9A-F]) activate the proxy, as they are quite different from the overall data distribution, but are actually predicted by a feature of their own, A/1/3817.

Universality
A/1/2357 has a correlated feature (corr=0.85) in run B/1, B/1/2165. It also has high activation specificity for base64 strings:


Like A/1/2357, B/1/2165's logit weights have a second mode corresponding to base64 token:


Correlations and scatter plots are also consistent with them being very similar features:


Note that we expect the overlap between the interference and base64 token logit weights to be from the aforementioned usage of base64 token across many other contexts.

The Feature Is Not A Neuron
Looking at the neuron in model A that most correlates with this feature: A/neurons/470 (corr=0.18), we find that while it does notably respond to base64 strings, it also activates for lots of other things, including code, HTML labels, parts of URLs, etc.:


The logit weights suggest it somewhat increases base64 tokens, but is much more focused on upweighting other tokens, e.g. filename endings.


The activation and logit correlations are consistent with this neuron helping represent the same feature, but largely doing other things.


Hebrew Feature
Another interesting example is the Hebrew feature A/1/416. Like the Arabic feature, it's easy to computationally identify Hebrew text based on Unicode blocks.

A/1/416 has high activation specificity in the upper spectrum. It does weakly activate for other things (especially other languages with Unicode scripts). There is also some blue in strong activations; this appears to significantly be on "common characters", such as whitespace or punctuation, which are from other unicode blocks (see more discussion of similar issues in the Arabic feature section).


Its logit weights have a notable second mode, corresponding to Hebrew characters and relevant incomplete Unicode characters. Note that \xd7 is the first token in the UTF-8 encoding of most characters in the basic Hebrew Unicode block.


The Pearson correlation of the Hebrew script proxy with A/1/416 is 0.55. Some of the failure of sensitivity may be due to a complementary feature A/1/1016 that fires on \xd7 and predicts the bytes that complete Hebrew characters' codepoints.

The Feature Is Not A Neuron
There doesn't appear to be a similar neuron. The most correlated neuron in model A is A/neurons/489 (corr=0.1), which has low activation and logit specificity. Consider the following activation and logit correlation plots:


To cross-validate this, we also searched for any neuron where the main Hebrew Unicode block appeared in the top dataset examples. We found none.

Universality
A/1/416 has a correlated feature in the B/1 run, B/1/1901 (corr=0.92) that has significant activation specificity:


Logit weights have a second mode, as before:


Activation and logit weight correlations are again consistent:







Global Analysis
If the previous section has persuaded you that at least some of the features are genuinely interpretable and reflect the underlying model mechanics, it's natural to wonder how broadly this holds outside of those cherry-picked features. The primary focus of this section will be to answer the question, "how interpretable are the rest of the features?" We show that both humans and large language models find our features to be significantly more interpretable than neurons, and quite interpretable in absolute terms.

There are a number of other questions one might also ask. To what extent is our dictionary learning method discovering all the features necessary to understand the MLP layer? Holistically, how much of the MLP layer's mechanics have been made interpretable? We are not yet able to fully answer these questions to our satisfaction, but will provide some preliminary speculation towards the end of this section.

We note that of the 4,096 learned features in the A/1 autoencoder, 168 of them are "dead" (active on none of the 100 million dataset) and 292 of them are "ultralow density", active on less than 1 in a million dataset examples and exhibiting other atypical properties. We exclude both these groups of features from further analyses.

How Interpretable is the Typical Feature?
In this section, we use three different methods to analyze how interpretable the typical feature is, and how that compares to neurons: human analysis, and two forms of automated interpretability. All three approaches find that features are much more interpretable than neurons.

Manual Human Analysis
At present, we do not have any metric we trust more than human judgment of interpretability. Thus, we had a blinded annotator (one of the authors, Adam Jermyn) score features and neurons based on how interpretable they are. The scoring rubric can be found in the appendix and accounts for confidence in an explanation, consistency of the activations with that explanation, consistency of the logit output weights with that explanation, and specificity.

In doing this evaluation, we wanted to avoid a weakness we perceived in our prior work (e.g., 
[38]
) of focusing evaluation predominantly on maximal dataset examples, and paying less attention to the rest of the activation spectrum. Many polysemantic neurons appear monosemantic if you only look at top dataset examples, but are revealed to be polysemantic if you look at lower parts of the activation spectrum. To avoid this, we draw samples uniformly across the spectrum of feature activations, 22 and score each interval separately in light of the overall hypothesis suggested by the feature.

Unfortunately, this approach is labor intensive and so the number of scored samples is small. In total, 412 feature activation intervals were scored across 162 features and neurons.


We see that features are substantially more interpretable than neurons. Very subjectively, we found features to be quite interpretable if their rubric value was above 8. The median neuron scored 0 on our rubric, indicating that our annotator could not even form a hypothesis of what the neuron could represent! Whereas the median feature interval scored a 12, indicating that the annotator had a confident, specific, consistent hypothesis that made sense in terms of the logit output weights.  


Automated Interpretability – Activations
To analyze features at a larger scale, we turned to automated interpretability 
[45, 46]
. Following the approach of Bills et al. 
[45]
, we have a large language model, Anthropic’s Claude, generate explanations of features using examples of tokens where they activate. Next, we have the model use that explanation to predict new activations on previously unseen tokens. 23

Like with the human analysis, we used samples across the full range of activation intervals to evaluate monosemanticity. 24 Concretely, for each feature, we computed the Spearman correlation coefficient between the predicted activation and the true activations for 60 dataset examples made up of nine tokens each, resulting in 540 predictions per feature. While only using completely random sequences would be the most principled approach to scoring, half of the examples are from across the feature intervals to get a more accurate correlation. See the appendix for additional information including the use of importance scoring to precisely counter-weight the bias of providing tokens the feature fires for.

In agreement with the human analysis, Claude is able to explain and predict activations for features significantly better than for neurons. 25


Automated Interpretability – Logit Weights
In our earlier analysis of individual features, we found that looking at the logits is a powerful tool for cross-validating the interpretability of features. We can take this approach in automated interpretability as well. Using the explanations of features generated in the previous analysis, we ask a language model to predict if a previously unseen logit token is something the feature should predict as likely to come next. This is then scored against a 50/50 mix of top positive logit tokens and random other logit tokens. Randomly guessing would give a 50% accuracy, but the model instead achieves a 74% average across features, compared to a 58% average across neurons. Failures here refer to instances where Claude failed to reply in the correct format for scoring.


Activation Interval Analysis
In addition to studying features as a whole, in our manual analysis we can zoom in on portions of the feature activation spectrum using the feature intervals. As before, a feature interval is the set of examples with activations closest to a specific evenly-spaced fraction of the max activation. So, rather than asking if a feature seems interpretable, we ask whether a range of activations is consistent with the overall hypothesis suggested by the full spectrum of the feature’s activation. This allows us to ask how interpretability changes with feature activation strength.


Higher-activating feature intervals were more consistent with our interpretations than lower-activating ones. In particular:

Many features show consistent activations across the entire activation spectrum.
Some features show consistent activations across the top ~60% of the activation spectrum, and then quickly become less interpretable as we look to smaller and smaller activations.
It is possible that this is a sign that our features are not quite right. For instance, if one of our features is at a slight angle to the feature we’d really like to have learned, that can show up as inconsistent behavior in the lower activation intervals.

Caveats
Our manual and automated interpretability experiments have a few caveats:

Feature activations are skewed towards the lower intervals. Most feature activations are quite small, and so fall in the lower (and less-interpretable) feature intervals. That said, as we found in our detailed analysis, most of the effect of a feature 26 is due to its higher activations, which are quite interpretable.
The evaluated features are sampled uniformly from among the set of all features, and interpretability might be correlated with importance. One could imagine evaluating the features with the largest magnitude of activations, or the largest effect when ablated. Our sense is that these would be more interpretable than the randomly sampled features we considered. One could imagine a situation where the most important features in some sense were systematically less interpretable, though inspection of the most active features suggests the opposite.
Based on our inspection of many features in the visualization, we believe these caveats do not affect the experimental results. We encourage interested readers to open the visualization for A/1 and the corresponding neurons. You can sort by ‘random’ to get an unbiased sample and do your own version of the above experiment, or sort features by importance metrics such as max activation and max density to evaluate the final caveat above.

How much of the model does our interpretation explain?
We now turn to the question we're least able to answer – to what extent do these seemingly interpretable features represent the "full story" of the MLP? One could imagine posing this question in a variety of ways. What fraction of the MLP loss contribution have we made interpretable? How much model behavior can we understand? If there really are some discrete set of "true features", what fraction have we discovered?

One way to partly get at this question is to ask how much of the loss is explained by our features. For A/1, the run we've focused most on in this paper, 79% of the log-likelihood loss reduction provided by the MLP layer is recovered by our features. That is, the additional loss incurred by replacing the MLP activations with the autoencoder's output is just 21% of the loss that would be incurred by zero ablating the MLP. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. As an extreme example, A/5 (n_learned_sparse=131,072, l1_coefficient=0.004) recovers 94.5% of log-likelihood loss.

These numbers should be taken with a significant grain of salt. The biggest issue is that framing this question in terms of fraction of loss may be misleading – we expect there to be a long-tail of features such that as the fraction of loss explained increases, more and more features are needed to explain the residual. Another issue is that we don't believe our features are completely monosemantic (some polysemanticity may be hiding in low activations), nor are all of them necessarily cleanly interpretable. With all of that said, our earlier analyses of individual features (e.g. the Arabic feature, base64 feature, etc.) do show that specific interpretable features are used by the model in interpretable ways – ablating them decreases probabilities in the appropriate way, and artificially activating them causes a corresponding behavior. This seems to confirm that the 79% of loss recovered is measuring something real, despite these caveats.

In principle, one could use automated interpretability to produce a better measure here: replacing activations with those predicted from explanations. (We believe others in the community have recently been considering this!) The naive versions of this would be quite computationally expensive, 27 although there may be approximations. More generally, there is a much broader space of possibilities here. Perhaps there's a principled way to do this analysis in terms of single features – there are significant conceptual issues, but one might be able to formalize earlier notions of "feature importance" as an independent loss contribution a feature makes, and then analyze how much of that specific feature can be recovered.

Overall, we view the problem of measuring the degree to which a feature-based interpretation explains a model to be an important open question, where significant work is necessary on both defining metrics and finding efficient ways to compute them.

Do features tell us about the model or the data?
A model's activations reflect two things: the distribution of the dataset and the way that distribution is transformed by the model. Dictionary learning on activations thus mixes data and model properties, and intriguing properties of learned features may be attributed to either or both sources. Correlations in the data can persist after application of the first part of the model (up to the MLP), and it is in theory possible that the intriguing features we see are merely artifacts of dataset correlations projected into a different space. However, the use of those features by the second half of the model (MLP downprojection and unembedding) are not an input to dictionary learning, so the interpretability of the downstream effects of those features must be a property of the model.

To assess the effect of dataset correlations on the interpretability of feature activations, we run dictionary learning on a version of our one-layer model with random weights. 28 The resulting features are here, and contain many single-token features (such as "span", "file", ".", and "nature") and some other features firing on seemingly arbitrary subsets of different broadly recognizable contexts (such as LaTeX or code). However, we are unable to construct interpretations for the non-single-token features that make much sense and invite the reader to examine feature visualizations from the model with randomized weights to confirm this for themselves. We conclude that the learning process for the model creates a richer structure in its activations than the distribution of tokens in the dataset alone.

To assess the interpretability of the downstream feature effects, we again use the three main approaches of the previous section:

Logit weight inspection. The logit weights represent the effect of each feature on the logits, and, as demonstrated in our earlier investigations of individual features, they are consistent with the feature activations for the base64, Arabic, and Hebrew features. The reader is invited to inspect logit weights for all features in the visualization.
Feature ablation. We set the value of a feature to zero throughout a context, and record how the loss on each token changes. Ablations are available for all features in the visualization.
Pinned feature sampling. We artificially pinned the value of a feature to a fixed high number and then sample from the model. We find that the generated text matches the interpretation of the feature.

The empirical consistency of feature activations with their downstream effects across all these metrics provides evidence that the features found are being used by the model.






Phenomenology
Ultimately, the goal of our work is to understand neural networks. Decomposition of models into features is simply a means to this end, and one might very reasonably wonder if it's genuinely advancing our overall goal. So, in this section, we'll turn attention to the lessons these features can teach us about neural networks. (We've taken to calling this work of leveraging our theoretical understanding to reason about model properties phenomenology by analogy to phenomenology in physics, and the 2019 ICML workshop on phenomena in deep learning.)

One way to do this would be to give a detailed discussion of the features we've found (similar to 
[47, 48]
), but we believe the best way to get a sense of the features we discovered is simply to browse the interface for exploring features which we've published alongside this paper. The features we find vary enormously, and no concise summary will capture their breadth. Instead, we will largely focus on more abstract properties and patterns that we notice. These abstract properties will be able to inform – although by no means answer – questions like "Are these the real features?" and "What is actually going on in a one-layer model?"

We begin by discussing some basic motifs and observations about features. We'll then discuss how the features we relate compare to features in other dictionary learning runs and in other models. This will suggest that features are universal and that dictionary learning can be understood as a process of feature splitting that reflects something deep about the geometry of superposition. Finally, we'll explore how features connect together into "finite state automata" as systems that implement more complex behaviors.

Feature Motifs
What kinds of features do we find in our model?

One strong theme is the prevalence of context features (e.g. DNA, base64) and token-in-context features (e.g. the in mathematics – A/0/341, < in HTML – A/0/20). 29 These have been observed in prior work (context features e.g. 
[38, 49, 45]
; token-in-context features e.g.  
[38, 15]
; preceding observations 
[50]
), but the sheer volume of token-in-context features has been striking to us. For example, in A/4, there are over a hundred features which primarily respond to the token "the" in different contexts. 30 Often these features are connected by feature splitting (discussed in the next section), presenting as pure context features or token features in dictionaries with few learned features, but then splitting into token-in-context features as more features are learned.

Another interesting pattern is the implementation of what seem to be "trigram" features, such as a feature that predicts the 19 in COVID-19 (A/2/12310). Such features could in principle be implemented with attention alone, but in practice the model uses the MLP layer as well. We also see features which seem to respond to specific, longer sequences of tokens. These are particularly striking because they may implement "memorization" like behavior – we'll discuss this more later.

Finally, it's worth noting that all the features we find in a one-layer model can be interpreted as "action features" in addition to their role as "input features". For example, a base64 feature can be understood both as activating in response to base64 strings, and also as acting to increase the probability of base64 strings. The "action" view can clarify some of the token-in-context features: the feature A/0/341 predicts noun phrases in mathematical text, upweighting nouns like denominator and adjectives like latter. Consequently, while it activates most strongly on the, it also activates on adjectives like special and this which are also followed by noun phrases. This dual interpretation of features can be explored by browsing our interface. Several papers have previously explored interpreting neurons as actions (e.g. 
[51]
), and one-layer models are particularly suited to this, since it's a particularly principled way to understand the last MLP layer, and the only MLP in a one-layer model is the last layer.

Feature Splitting
One striking thing about the features we’ve found is that they appear in clusters. For instance, we observed above multiple base64 features, multiple Arabic script features, and so on. We see more of these features as we increase the total number of learned sparse features, a phenomenon we refer to as feature splitting. As we go from 512 features in A/0 to 4,096 features in A/1 and to 16,384 features in A/2, the number of features specific to base64 contexts goes from 1 to 3 to many more.

To understand how the geometry of the dictionary elements correspond to these qualitative clusters, we do a 2-D UMAP on the combined set of feature directions from A/0, A/1, and A/2.


We see clusters corresponding to the base64 and Arabic script features, together with many other tight clusters from specific contexts and a variety of other interesting geometric structures for other features. This confirms that the qualitative clusters are reflected in the geometry of the dictionary: similar features have small angles between their dictionary vectors.


We conjecture that there is some idealized set of features that dictionary learning would return if we provided it with an unlimited dictionary size. Often, these "true features" are clustered into sets of similar features, which the model puts in very tight superposition. Because the number of features is restricted, dictionary learning instead returns features which cover approximately the same territory as the idealized features, at the cost of being somewhat less specific.

In this picture, the reason the dictionary vectors of conceptually similar features are similar is that they are likely to produce similar behaviors in the model, and so should be responsible for similar effects in the neuron activations. For instance, it would be natural for a feature that fires on periods to predict tokens with a leading space followed by a capital letter. If there are multiple features that fire on periods, perhaps on periods in somewhat different contexts, these might all predict tokens with a leading space, and those predictions might well involve producing similar neuron activations. The combination of features being highly correlated and having similar "output actions", causes real models to have both denser and more structured superposition than what we observed in our previous toy models work 
[5]
. 31

If this picture is true, it would be important for a number of reasons. It suggests that determining the "correct number of features" for dictionary learning is less important than it might initially seem. It also suggests that dictionary learning with fewer features can provide a "summary" of model features, which might be very important in studying large models. Additionally, it would explain some of the stranger features we observe in the process of dictionary learning, suggesting that these are either "collapsed" features which would make sense if split further (see "Bug" 1: Single Token Features), or else highly-specific "split" features which do in fact make sense if analyzed closely (see "Bug" 2: Multiple Features for a Single Context). Finally, it suggests that our basic theory of superposition in toy models is missing an important dimension of the problem by not adequately studying highly correlated and "action sharing" features.

Example: Mathematics and Physics Features
In this example, our coarsest run (with 512 learned sparse features) has three features describing tokens in different technical settings. Using the masked cosine similarity 32 between feature activations, we are able to identify how these features refine and split in runs with more learned sparse features.

What we see is that the finer runs reveal more fine-grained distinctions between e.g. concepts in technical writing, and distinguish between the articles the and a, which are followed by slightly different sets of noun phrases. We also see that the structure of this refinement is more complex than a tree: rather, the features we find at one level may both split and merge to form refined features at the next. In general though, we see that runs with more learned sparse features tend to be more specific than those with fewer.


It's worth noting that these more precise features reflect differences in model predictions as well as activations. The general the in mathematical prose feature (A/0/341) has highly generic mathematical tokens for its top positive logits (e.g. supporting the denominator, the remainder, the theorem), whereas the more finely split machine learning version (A/2/15021) has much more specific topical predictions (e.g. the dataset, the classifier). Likewise, our abstract algebra and topology feature (A/2/4878) supports the quotient and the subgroup, and the gravitation and field theory feature (A/2/2609) supports the gauge, the Lagrangian, and the spacetime.

Features which seemed like Bugs
"Bug" 1: Single-Token Features
When we limit dictionary learning to use very few learned sparse features, the features that emerge sometimes look quite strange. In particular, there are a large number of high-activation magnitude features which each only fire on a single token, and which seem to fire on every instance of that token. Such features are strange because the model could achieve the same effect entirely by learning different bigram statistics, and so should have no reason to devote MLP capacity to these. Similar features were also recently observed in a report by Smith 
[15]
.

We believe that feature splitting explains this phenomenon: the model hasn’t learned a single feature firing on the letter P, 33 for instance. Rather, it’s learned many features which fire on P in different contexts 34 , with correspondingly different effects on the neuron activations and output logits (see below). At a sufficiently coarse level dictionary learning cannot tell the difference between these, but when we allow it to use more learned sparse features, the features split and refine into a zoo of different P features that fire in different contexts.


"Bug" 2: Multiple Features for a Single Context
We also observed the converse, where multiple features seemed to cover roughly the same concept or context. For example, there were three features in A/1 which fired on (subsets of) base64 strings, and predicted plausible base64 tokens like zf, mF, and Gp. One of these features was discussed in detail earlier, where we showed it fired for base64 strings. But we also observed that it didn't fire for all base64 strings – why? And what are the other two features doing? Why are there three?

In A/0 (with 512 features), the story is simple. There is only one base64-related feature, A/0/45, which seems to activate on all tokens of base64-encoded strings. But in A/1, that feature splits into three different features whose activations seem to jointly cover those of A/0/45:


Two of these features seem relatively straightforward. A/1/2357 seems to fire preferentially on letters in base64, while A/1/2364 seems to fire preferentially on digits.

Comparing the logit weights of these features reveals that they predict largely the same sets of tokens, with one significant difference: the feature that firing on digits has much lower logit weights for predicting digits. Put another way, if the present token is made of digits, the model will predict that the next token is a non-digit base64 token.


We believe this is likely an artifact of tokenization! If a single digit were followed by another digit, they would have been tokenized together as a single token; [Bq][8][9][mp] would never occur, as it would be tokenized instead as [Bq][89][mp]. Thus even in a random base64 string, the fact that the current token is a single digit gives information about the next token.

But what about the third feature, A/1/1544? At first glance, there isn't an obvious rule for when it fires. But if we look more closely, we notice that it seems to respond to base64 strings which encode ASCII text. 35 If we look at the top dataset examples for each feature, we find that examples for A/1/1544 contain substrings which decode as ASCII, while none of the top activating examples for A/1/2357 or A/1/2364 do: 36


This pattern of investigation, where one looks at coarser sets of features to understand categories of model behavior, and then at more refined sets of features to investigate the subtleties of that behavior, may prove well adapted to larger models where the feature set is expected to be quite large.

It's also worth noting how dictionary learning features were able to surprise us here. Many approaches to interpretability are top-down, and look for things we expect. But who would have known that models not only have a base64 feature, but that they distinguish between distinct kinds of base64 strings? This reminds us of cases like high-low frequency detectors 
[52]
 or multimodal neurons 
[48]
 where surprising and unexpected features were discovered in vision models.

Universality
One of the biggest "meta questions" about features is whether they're universal 
[53, 4]
– do the same features form across different models? This question is generally important because it bears on whether the hard-earned lessons from studying one model will generalize to others. But it's especially important in the context of attempting to extract features from superposition because universality could provide significant evidence that the features we're extracting are "real", or at least reproducible. 37

Earlier, we saw that all the features we performed detailed analyses of (e.g. the Arabic feature, or base64 feature) were universal between two one-layer models. But is this true for typical features in our model? And how broadly is it true – do we only observe the same feature if we train models of the same architectures on the same dataset, or do these features also occur in more divergent models? This section will seek to address these two questions. The first subsection will quantitatively analyze how widespread universality is between the two one-layer models we studied, while the second will compare the features we find to others reported in the literature in search of a stronger form of universality.

We observe substantial universality of both types. 38 At a high-level, this makes sense: if a feature is useful to one model in representing the dataset, it's likely useful to others, and if two models represent the same feature then a good dictionary learning algorithm should find it.

Comparing features between two one-layer transformers
To compare features from different models, we need model-independent ways to represent a feature.

One natural approach is to think of a feature as a function assigning values to datapoints; two features would be similar in this sense if they take similar values over a diverse set of data. This general approach has been explored by a number of prior papers (e.g. 
[54, 53, 55, 56]
). In practice, this can be approximated by representing the feature as a vector, with indices corresponding to a fixed set of data points. We call the correlations between these vectors the activation similarity between features.

A second natural approach is to think of a feature in terms of its downstream effects; two features would be similar in this sense if their activation changes their models' predictions in similar ways. In our one-layer model, a simple approximation to this is the logit weights. This approximation represents each feature as a vector with indices corresponding to vocabulary tokens. We call the correlations between these vectors the logit weight similarity between features.

These two notions of similarity correspond to the correlations of the points in the two scatter plots we used when analyzing individual features earlier. We've reproduced the plots for the Arabic feature below:


For each feature in run A/1, we find the closest feature by activation similarity in run B/1, which is a different dictionary learning run trained on different activations from a different transformer with different random seeds but otherwise identical hyperparameters. We find that many features are highly similar between models, with features in A/1 having a median activation correlation of 0.72 with the most similar feature from B/1. (We perform the same analysis finding the closest neurons between the transformers, and find significantly less similarity, with median activation correlation 0.46.) The features with low activation correlation between models may represent different "feature splittings" in the dictionaries learned or different "true features" learned by the base models.


A natural next question is whether features that fire on the same tokens also have the same logit effects. That is, how well do activation similarity and logit weight similarity agree?

Some gap between the two is visible for the Arabic feature above: the "important tokens" for the features' effects (the ones in Arabic script) are upweighted by features from both models, but there is a large cloud of tokens with smaller effects that appear to almost be isotropic noise, resulting in a logit weight correlation of just 0.23, significantly below the activation correlation of 0.91.

In the scatterplot below, we find that this kind of disagreement is widespread.


The most dramatic example of this disparity is for the features A/1/3949 and B/1/3321, with an activation correlation of 0.98 but a negative logit weight correlation. These features fire on pone (and occasionally on pgen and pcbi) as abbreviations for the journal name PLOSOne in citations, like @pone.0082392, and predict the . that follows. 39

Zooming in on the logit weight scatterplot (inset in the figure above), we see that only the . token has high logit weight in both models, and that every other token is in the 'interference' portion of the logit weight distribution. Indeed, the model may simply not care about what the feature does to tokens which were already implausible because they are suppressed by the direct path, attention layer, or other features of the MLP.

We want to measure something more like "the actual effect a feature has on token probabilities." One way to get at this would be to compute a vector of ablation effects for every feature on every data point; pairs of features whose ablations hurt the model's predictions on the same tokens must have been predicting the same thing. Unfortunately, this would be rather expensive computationally. Instead, we scale the activation vector of a feature by the logit weights of the tokens that empirically come next in the dataset to produce an attribution vector. 40 Correlations between those vectors provide an attribution similarity that combines both the activity of the feature with the effect it has on the loss. We find that the attribution similarity correlates quite highly with the activation similarity, meaning that features that were coactive between models were useful at predicting the same tokens.


In light of this, we feel that the activation correlation used throughout the paper is in fact a good proxy for both notions of universality in the context of our one-layer models.

Comparing features with the literature
So far, we've established that many of our features are universal in a limited sense. Features found in one of our transformers can also be found in an alternative version trained with a different random seed. But this second model has an identical architecture and was trained on identical data. This is the most minimal version of universality one could hope for. Despite this, we believe that many of the features we've found are universal in a deeper sense, because very similar features have been reported in the literature before.

The first comparison which struck us is that many features seem quite similar to neurons we previously found in one-layer SoLU models, which use an activation function designed to make neurons more monosemantic 
[38]
. In particular, we observed a base64 neuron, hexadecimal neuron, and all caps neuron in our SoLU investigations, and base64 (A/0/45), hexademical (A/0/119), and all caps (A/0/317) features here. Discussion of some of these neurons can be found in Section 6.3.1 of the SoLU paper.

We also find many features similar to Smith 
[15]
, who applies dictionary learning to the residual stream. In addition to us also observing preponderance of single token features they note (see our interpretation of this phenomenon), we find a similar German detector (e.g. A/0/493) and similar title case detectors (e.g. A/0/508). Likewise, we find a number of features similar to Gurnee et al. 
[49]
, including a "prime factors" feature  (A/4/22414) and a French feature (A/0/14).

At a more abstract level, many features we find seem similar to features reported in multimodal models by Goh et al. 
[48]
. For example, we find many similar features including an Australia feature (A/3/16085), Canada feature (A/3/13683), Africa feature (A/3/14490), and Israel-Palestine feature (A/3/739) which predict locations in those regions when grammatically appropriate. This vaguely mirrors "region neurons" reported by Goh et al.'s paper. For other families of features, the parallels are less clear. For example, one of the most striking results of Goh et al. was person detector neurons (similar to famous results in neuroscience). We find some features that are person detectors in very narrow contexts, such as responding to a person’s name and predicting appropriate next words, or predicting their name (e.g. A/1/3240  is somewhat similar to Goh et al.'s Trump neuron), but they seem quite narrow. We also don't find features that seem clearly analogous to Goh et al.'s emotion neurons.

"Finite State Automata"
One of the most striking phenomena we've observed in our study of the features in one-layer models is the existence of "finite state automata"-like assemblies of features. These assemblies aren't circuits in the conventional sense – they're formed by one feature increasing the probability of tokens, which in turn cause another feature to fire on the next step, and so on. 41

The simplest example of this is features which excite themselves on the next token, forming a single node loop. For example, a base64 feature increases the probability of tokens like Qg and zA – plausible continuations which would continue to activate it.


It's worth noting that these examples are from A/0, a dictionary learning run which is not overcomplete (the dictionary dimensionality is 512, equal to the transformer MLP dimension). As we move to runs with larger numbers of features, the central feature will experience feature splitting, and become a more complex system.

Let's now consider a two-node system for producing variables in "all caps snake case" (e.g. ARRAY_MAX_VALUE). One node (A/0/207) activates on the all caps text tokens, the other (A/0/358) on underscores:


This type of two-node system is quite common for languages where Unicode characters are sometimes split into two tokens. (Again, with more feature splitting, these would expand into more complex systems.)

For example, Tamil Unicode characters (block U+0B80–U+0BFF) are typically split into two tokens. For example, the character "ண" (U+0BA3) is tokenized as \xe0\xae followed by \xa3.  The first part (\xe0\xae or \xe0\xaf) roughly specifies the Unicode block, while the second component specifies the character within that block. Thus, it's natural for the model to alternate between two features, one for the Unicode prefix token, and one for the suffix token.

A more complex example is Chinese. While many common Chinese characters get dedicated tokens, many others are split. This is further complicated by Chinese characters being spread over many Unicode blocks, and those blocks being large and cutting across many logical blocks specified in terms of bytes. To understand the state machine the model implements to handle this, the key observation is that complete characters are similar to the "suffix" part of a split character: both can be followed by either a new complete character, or a new prefix. Thus, we observe two features, one of which fires on either complete characters or the suffix (predicting either a new complete character, or a prefix), while the other only fires on the prefixes and predicts suffixes.


Let's now consider a very simple four node system which models HTML. The "main path" through it is:

A/0/20 fires on open tags and predicts tag names
A/0/0 fires on tag names and predicts tag closes
A/0/30 fires on tag closes and predicts whitespace
A/0/494 fires on whitespace and predicts new tag opens.
A prototypical sample this might generate is something like <div>\n\t\t<span>.

The full system can be seen below:


Keep in mind that we're focusing on the A/0 features where this is very simple – if we looked at A/1, we'd find something much more complex! One particularly striking shortcoming of the A/0 features is that they don't describe what happens when A/0/0 emits a token like  href, which leads to a more complex state.

It's important to note that these features can be quite contextual. There are several features related to IRC transcripts which form a totally different finite state automata like system:


A prototypical sample this might generate is something like <nickonia_> lol ubuntu ;). Presumably the Pile dataset heavily represents IRC transcripts about linux.

One particularly interesting behavior is the apparent memorization of specific phrases. This can be observed only in runs with relatively large numbers of features (like A/4). In the following example, a sequence of features seem to functionally memorize the bolded part of the phrase MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. This is a relatively standard legal language, and notably occurs in the file headers for popular open source software licenses, meaning the model likely saw it many times during training.


This seems like an example of the mechanistic theory of memorization we described in Henighan et al. 
[57]
 – we observe features which appear to be relatively binary and respond to a very specific situation. This might also be seen as an instance of mechanistic anomaly detection 
[58]
: the model behaves differently in a specific, narrow case. It's somewhat surprising that something so narrow can be found in a model with only 512 neurons; from this perspective it's an interesting example of superpositions' ability to embed many things in few neurons. On the other hand, because these mechanisms are buried deep in superposition, they are likely very noisy.






Related Work
Superposition and attempts to resolve it have deep connections to many lines of research, including general investigations of interpretable features, linear probing, compressed sensing, dictionary learning and sparse coding, theories of neural coding, distributed representations, mathematical frames, vector symbolic architectures, and much more. Rather than attempt to do justice to all these connections here, we refer readers to the related work section of Toy Models of Superposition 
[5]
 where we discuss these topics in depth, and also to our essay Distributed Representations: Composition & Superposition 
[44]
. Instead, we'll focus our discussion on work connected to our attempts to solve superposition, and also more recent advancements in our understanding of superposition.

Superposition
Since we published Toy Models of Superposition there has been significant further work attempting to better understand superposition. We briefly summarize below.

Is superposition real? Gurnee et al. 
[49]
 demonstrate some compelling examples of features which may be in superposition, using sparse linear probes. Separately, an exchange between Li et al. 
[34]
 and Nanda et al. 
[31]
 seems like an update on whether the general picture of features as directions is correct; Li et al. seemed to show that it wasn't, putting the hypothesis in jeopardy, which was then resolved by Nanda et al.

When and why does superposition occur? Scherlis et al. 
[36]
 provide a mathematical framework for thinking about monosemanticity vs polysemanticity. lsgos 
[59]
 explored the effects of dropout on toy models of superposition.

Memorization – In Henighan et al. 
[57]
, we studied the same toy model as in Toy Models of Superposition, but this time trained on many repetitions of finite-sized datasets. We found that small datasets are memorized in superposition, instead of generalizing features in the case of large datasets. Hobbhahn 
[60]
 replicated some of these findings, and further showed extensions to other settings including bottlenecks between layers (analogous to the residual stream between MLP layers). Subsequently, in a monthly update we examined the boundary between the memorization and generalization regimes and found a sharp phase transition, as well as dataset clustering in superposition on the memorization side of the boundary.

Disentanglement and Architectural Approaches
There is also a rich and related literature on disentanglement, which seeks to find representations of data that separate out (disentangle) conceptually-distinct phenomena influencing the data. In contrast to superposition, this work typically seeks to find a number of factors of variation or features which are equal to the dimensionality of the space being represented, whereas superposition seeks to find more.

This is often approached as an architecture/training-time problem. For instance, Kim & Mnih 
[61]
 proposed a method that pushes variational autoencoders to disentangle factors by encouraging independence across dimensions. Similarly, Chen et al. 
[62]
 developed a Generative Adversarial Network approach that attempts to disentangle factors by maximizing the mutual information between a small subset of factors and the dataset. And Makhzani & Frey 
[63]
 use a TopK activation to encourage sparsity and hence disentanglement. See Bengio et al. 
[64]
 and Räuker et al. 
[65]
 for a discussion of other such approaches.

Framed this way, some architectural approaches to superposition may also be understood as attempts at disentanglement. For instance, in Elhage et al. 
[38]
, we proposed the SoLU activation function, which increases the number of interpretable neurons in transformers by encouraging features to align to the neuron basis. Unfortunately, it appears that training models with the SoLU activation function may make some neurons more interpretable at the cost of making others even less interpretable than before.

Similarly, Jermyn et al. 
[35]
 studied MLP layers trained on a compressed sensing task and found multiple equal-loss minima, with some strongly polysemantic and others strongly monosemantic. This suggested that training interventions could steer models towards more monosemantic minima, though subsequent investigations on more realistic tasks suggested that the equal-loss property was specific to the chosen task.

These two examples of attempts to tackle superposition through architecture, and the challenges they encountered, highlight a key distinction between the problems of disentanglement and that of superposition: disentanglement fundamentally seeks to ensure that the dimensions in the model’s latent space are disentangled, whereas superposition hypothesizes that this disentanglement typically hurts performance (since success would require throwing away many features), and that models will typically respond to disentangling interventions by making some features more strongly entangled (as was found by both Mahinpei et al. 
[66]
 and Elhage et al. 2022 
[38]
 in somewhat different contexts).

Dictionary Learning and Features
Our work builds on a longer tradition of using dictionary learning and sparse autoencoders to decompose neural network activations.

Early work in this space focused on word embeddings and other non-transformer neural networks. Faruqui et al. 
[6]
 and Arora et al. 
[2]
 both found linear structure in word embeddings using sparse coding approaches. Subramanian et al. 
[7]
 similarly found linear factors for word embeddings, in this case using a sparse autoencoder. Zhang et al. 
[8]
 solved a similar problem using methods from dictionary learning while Panigrahi et al.
[9]
 approached this with Latent Dirichlet Allocation.

More recently, a number of works have applied dictionary learning methods to transformer models. Yun et al. 
[10]
 applied dictionary learning to the residual stream of a 12-layer transformer to find an undercomplete basis of features.

At this point, our work in Toy Models 
[5]
 advocated for dictionary learning as a potential approach to superposition. This motivated a parallel investigation by our colleagues Cunningham et al., published as a series of interim reports 
[11, 12, 13, 14, 15, 16]
 with very similar themes to this paper, culminating in a manuscript 
[17]
. We've been excited to see so many corroborating findings between our work.

In their interim reports, Sharkey et al. 
[11]
 used sparse autoencoders to perform dictionary learning on a one-layer transformer, identifying a large (overcomplete) basis of features. (Sharkey et al. deserve credit for focusing on dictionary learning and especially the sparse autoencoder approach, while our investigation was only exploring it as one of several approaches in parallel.) This work was then partially replicated by Cunningham & Smith 
[12]
 and Huben 
[13]
. Next, Smith 
[14]
 used an autoencoder to find features in one MLP layer of a six-layer model. The resulting features appear interpretable, e.g. detecting ‘$’ in the context of LaTeX equations. In follow up work, Smith then extended this approach to the residual stream of the same model, identifying a number of interesting features (see earlier discussion). Building on these results, Cunningham
[16]
 applied autointerpretability techniques from Bills et al. 
[45]
 to features in the residual stream and an MLP layer of the same six-layer model, finding that the features discovered by the sparse autoencoder are substantially more interpretable than neurons.






Discussion
Theories of Superposition
Coming into this work, our understanding of superposition was mostly informed by Toy Models 
[5]
. This gave us a picture one might call the isotropic superposition model. Features are discrete, one-dimensional objects which repel from each other due to interference, creating a roughly evenly spaced organization of feature directions.

This work has persuaded us that our previous model was missing something crucial. At a minimum, features seem to clump together in higher density groups of related features. One explanation for this (considered briefly by Toy Models) is that the features may have correlated activations – firing together. Another – which we suspect to be more central – is that the features produce similar actions. The feature which fires on single digits in base64 predicts approximately the same set of tokens as the feature firing on other characters in base64, with the exception of other digits; these similar downstream effects manifest as geometrically close feature directions.

Moreover, it isn't clear that features need to be one-dimensional objects (encoding only some intensity). In principle, it seems possible to have higher-dimensional "feature manifolds" (see earlier discussion here).


These hypotheses are not mutually exclusive. The convex hull of several correlated features might be understood as a feature manifold. On the other hand, some manifolds would not admit a unique description in terms of a finite number of one-dimensional features. (Perhaps this accounts for the continued feature splitting observed above.)


Nevertheless, these experiments have left us more confident that some version of the superposition hypothesis (and the linear representation hypothesis) is true. The number of interpretable features found, the way activation level seems to correspond to "intensity" or "confidence," the fact that logit weights mostly make sense, and the observation of "interference weights": all of these observations are what you would expect from superposition.

Finally, we note that in some of these expanded theories of superposition, finding the "correct number of features" may not be well-posed. In others, there is a true number of features, but getting it exactly right is less essential because we "fail gracefully", observing the "true features" at resolutions of different granularity as we increase the number of learned features in the autoencoder.

Are "Token in Context" Features Real?
One of the most common motifs we found were "token-in-context" features. They also represent many of the features that emerge via feature splitting with increasing dictionary size. Some of these are intuitive – borrowing an example from 
[50]
, it makes sense to represent "die" in German (where it's the definite article) as distinct from "die" in English (where it means "death" or "dice").

But why do we see hundreds of different features for "the" (such as "the" in Physics, as distinct from "the" in mathematics)? We also observe this for other common words (e.g. "a", "of"), and for punctuation like periods. These features are not what we expected to find when we set out to investigate one-layer models!

To make the question a bit more precise, it is helpful to borrow the language and examples of local vs compositional representations 
[44, 67]
. Individually representing token-context pairs (such as "the" in Physics) is technically a "local code". The more intuitive way to represent this would instead be a "compositional code" – representing "the" as an independent feature from Physics. So the thing we really want to ask is why we're observing a local code, and whether it's really what's going on. There are two hypotheses:

The underlying transformer uses a compositional code, and a quirk of our dictionary learning scheme produces features using a local code.
The underlying transformer is genuinely using a local code (at least in part), and dictionary learning is correctly representing this.
If the former holds, then better dictionary learning schemes may help uncover a more compositional set of features from the same transformer. Local codes are sparser than compositional codes, and our L1 penalty may be pushing the model too far towards sparsity.

However, we believe the second hypothesis is likely to hold to some extent. Let's consider the example of "the" in Physics again, which predicts noun phrases in Physics: if the model represented "the" and Physics context independently, it would be forced to have logits be the sum of "upweight tokens which come after the" and "upweight tokens which occur in Physics". But the model might wish to have "sharper" predictions than this, which is only possible with a local code.

Future Work
Scaling Sparse Autoencoders. Scaling the application of sparse autoencoders to frontier models strikes us as one of the most important questions going forward. We're quite hopeful that these or similar methods will work – Cunningham et al.'s work 
[17]
 seems to suggest this approach can work on somewhat larger models, and we have preliminary results that point in the same direction. However, there are significant computational challenges to be overcome. Consider an autoencoder with a 100× expansion factor applied to the activations of a single MLP layer of width 10,000: it would have ~20 billion parameters. Additionally, many of these features are likely quite rare, potentially requiring the autoencoder to be trained on a substantial fraction of the large model's training corpus. So it seems plausible that training the autoencoder could become very expensive, potentially even more expensive than the original model. We remain optimistic, however, and there is a silver lining – it increasingly seems like a large chunk of the mechanistic interpretability agenda will now turn on succeeding at a difficult engineering and scaling problem, which frontier AI labs have significant expertise in.

Scaling Laws for Dictionary Learning. It's worth noting that there's enormous uncertainty about the dynamics of scaling dictionary learning and sparse autoencoders discussed above. As we make the subject model bigger, how does the ideal expansion factor change? (Does it stay constant?) How does the necessary amount of data change? The resolution of these questions will determine whether it's possible for this approach, if executed well, to scale up to frontier models. Ideally, we'd like to have scaling laws 
[68]
 which could answer this.

How Can We Recognize Good Features? One of the greatest challenges of this work is that we're "wandering in the dark" to some extent. We don't have a great, systematic way to know if we're successfully extracting high quality features. Automated interpretability 
[45]
 seems like a strong contender for solving this question. Alternatively, one might hope for some purely abstract definition (e.g. the information-based metric proposal), but we have not yet seen compelling signs of life for this on real data. It would also be helpful to have metrics beyond MMCS, activation similarity, and attribution similarity for comparing sets of features for the purposes of assessing consistency and universality.

Scalability of Analysis. Suppose that sparse autoencoders fully solve superposition. Do we have a home run to fully mechanistically understanding models? It seems clear that there would be at least one other fundamental barrier: scaling analysis of models, so that we can turn microscopic insights into a more macroscopic understanding. Again, one approach here could be automated interpretability. But delegating the understanding of AI to AI may not be fully satisfying, for various reasons. It is possible that there may be other paths based on discovering larger scale structure (see discussion here).

Algorithmic Improvements for Sparse Autoencoders. New algorithms refining the sparse autoencoder approach could be useful. One might explore the use of variational autoencoders (e.g., 
[69]
), or sparsity promoting priors regularization techniques beyond a simple L1 penalty on activations (e.g., 
[70, 71]
), for example encouraging sparsity in the interactions between learned features in different layers. Earlier research has shown that noise injection can also increase neuron interpretability separately from an L1 penalty 
[11]
[72]
.

Attentional Superposition? Many of the motivations for the presence of superposition in MLP layers 
[5]
 apply to self-attention layers as well. It seems conceivable that similar methods may extract useful structure from attention layers, although a clear example has not yet been established (e.g., see our May and July Updates). If this is true, addressing this may become a future bottleneck for the mechanistic interpretability agenda.

Theory of Superposition and Features. Many fundamental questions remain for our understanding of superposition, even if the hypothesis is right in some very broad sense. For example, as discussed above, this work suggests extensions of the superposition hypothesis covering clusters of features with similar effects, or continuous families of features. We believe there is important work to be done in exploring the theory of superposition further, perhaps through the use of toy models.






Comments & Replications
Inspired by the original Circuits Thread and Distill's Discussion Article experiment, the authors invited several external researchers who we had previously discussed our preliminary results with to comment on this work. Their comments are included below.

Replication & Tutorial
Neel Nanda is an external mechanistic interpretability researcher. This is a summary of a blog post replicating and extending this paper, with an accompanying tutorial to load some trained autoencoders and practice interpreting a feature.

The core results of this paper seem to replicate. I trained a sparse autoencoder on the MLP layer of an open source 1 layer GELU language model, and a significant fraction of the latent space features were interpretable.

I've open sourced two trained autoencoders and a tutorial for how to use them, and how to interpret a feature. I've also open sourced a (very!) rough training codebase, along with some of the implementation details that came up, and tips for training your own.

I investigated how sparse the decoder weights are in the neuron basis and find that they’re highly distributed, with 4% well explained by a single neuron, 4% well explained by 2 to 10, and the remaining 92% dense. I find this pretty surprising! Despite this, kurtosis shows the neuron basis is still privileged.


I also exhibit some case studies of the features I found, like a title case feature, and an "and I" feature

I didn’t find any dead features, but more than half of the features form an ultra-low frequency cluster (frequency less than 1e-4). Surprisingly, I find that this cluster is almost all the same feature (in terms of encoder weights, but not in terms of decoder weights). On one input 95% of these ultra rare features fired!

The same direction forms across random seeds, suggesting it's a true thing about the model and not just an autoencoder artifact
I failed to interpret what this shared direction was
I tried to fix the problem by training an autoencoder to be orthogonal to this direction but it still forms a ultra-low frequency cluster (which all cluster in a new direction)
One question from this work is whether the encoder and decoder should be tied. I find that, empirically, the decoder and encoder weights for each feature are moderately different, with median cosine similiarty of only 0.5, which is empirical evidence they're doing different things and should not be tied. Conceptually, the encoder and decoder are doing different things: the encoder is detecting, finding the optimal direction to project onto to detect the feature, minimising interference with other similar features, while the decoder is trying to represent the feature, and tries to approximate the “true” feature direction regardless of any interference.

Author Contributions Statement
Infrastructure, Tooling, and Core Algorithmic Work
General Infrastructure – The basic framework for our dictionary learning work was built and maintained by Adly Templeton, Trenton Bricken, Tom Henighan, and Tristan Hume. Adly Templeton, in collaboration with Trenton Bricken and Tom Conerly, performed the engineering to scale up our experiments to large numbers of tokens. Robert Lasenby created tooling that allowed us to train extremely small language models. Yifan Wu imported the "Pile" dataset so we could train models on a standard external dataset. Tom Conerly broadly assisted with our infrastructure and maintaining high code quality. Adly Templeton implemented automatic plots comparing different experiments in a scan.

Sparse Autoencoders (Algorithms / ML) – Trenton initially implemented and advocated for sparse autoencoders as an approach to dictionary learning. Trenton Bricken and Adly Templeton then collaborated on the research needed to achieve our results, including scanning hyperparameters, iterating on algorithms, introducing the "neuron resampling" method, and characterizing the importance of dataset scale. (It's hard for the other authors to communicate just how much work Trenton Bricken and Adly Templeton put into iterating on algorithms and hyperparameters.) Josh Batson assisted by analyzing runs and designing metrics with Chris Olah, Adly Templeton and Trenton Bricken to measure success.

Analysis Infrastructure – The tooling to collect data for behind the visualizations was created by Trenton Bricken and Adly Templeton. Tom Conerly greatly accelerated this. Adly Templeton implemented the tooling to perform large scale feature ablation analysis, as well as creating the shuffled weights models as baselines.

Interface – The interface for visualizing and exploring features was created by Brian Chen, with support from Shan Carter. It replaced a much earlier version by Trenton Bricken.

Analysis
Feature Deep Dives – The dataset examples and ablations came from Brian Chen's interface work and Adly Templeton's ablation infrastructure. Chris Olah developed the activation spectrum plots, broken down by proxy. Josh Batson developed the logit scatter plot visualization and the sensitivity analysis. Tom Henighan created the pinned sampling visualizations. Nick Turner, Josh Batson and Tom Henighan explored how best to analyze neuron alignment. Many people contributed to a push to systematically analyze many features in detail, including Tom Henighan, Josh Batson, Trenton Bricken, Adly Templeton, Nick Turner, Brian Chen, and Chris Olah.

Manual Analysis of Features – Nick Turner and Adam Jermyn created our rubric for evaluating if a feature interval is interpretable. Adam Jermyn manually scored a random set of features.

Automated Interpretability – Our automated interpretability pipeline was created by Trenton Bricken, Cem Anil, Carson Denison, and Amanda Askell. Trenton Bricken ran the experiments using it to evaluate our features. This built on earlier explorations of automated interpretability by Shauna Kravec, and was only possible due to infrastructure contributions by Tim Maxwell, Nicholas Schiefer, and Nicholas Joseph.

Feature Splitting – Josh Batson, Adam Jermyn, and Chris Olah analyzed feature splitting. Shan Carter produced the UMAPs of features.

Universality – Josh Batson analyzed universality, with engineering support from Trenton Bricken and Tom Henighan.

"Finite State Automata" – Chris Olah analyzed "Finite State Automata".

Paper Production
Writing – The manuscript was primarily drafted by Chris Olah, Josh Batson, and Adam Jermyn, with extensive feedback and editing from all other authors. The detailed appendix on dictionary learning was drafted by Adly Templeton and Trenton Bricken. Josh Batson and Chris Olah managed the revision process in response to internal and external feedback.

Illustration – Diagrams were created by Chris Olah, Josh Batson, Adam Jermyn, and Shan Carter, based on data generated by themselves and others.

Development of Intuition & Negative Results
Early Dictionary Learning Experiments – Many of the authors (including Trenton Bricken, Adly Templeton, Tristan Hume, Tom Henighan, Adam Jermyn, Josh Batson, and Chris Olah) did experiments applying both more traditional dictionary learning methods and sparse autoencoders to a variety of toy problems and small MNIST models. Much of this work focused on finding metrics which we hoped would tell us whether dictionary learning had succeeded in a simple context. These experiments, along with theoretical work, built valuable intuition and helped us discover several algorithmic improvements, and clarified challenges with traditional dictionary learning.

Sparse Architectures – Brian Chen ran the experiments and generated the counter-examples which persuaded us that training models for sparse activations was less promising. Crucially, he produced models where neurons typically activated in isolation, and showed they were still polysemantic. He then produced the counter-example described in the text. Robert Lasenby implemented infrastructure that made it easier to experiment with sparse activation architectures.

Other
Support - Alex Tamkin, Karina Nguyen, Brayden McLean, and Josiah E Burke made a variety of contributions through infrastructure, operational support, labeling features, and commenting on the draft.

Leadership - Tom Henighan led the dictionary learning project. Tristan Hume led an early version of the project, before handing it over to Tom Henighan. Shan Carter managed the overall team. Chris Olah provided general research guidance.

Acknowledgments
We are deeply grateful to Martin Wattenberg, Neel Nanda, Hoagy Cunningham, David Lindner, Logan Smith, Steven Bills, William Saunders, Jonathan Marcus, Daniel Mossing, Nick Cammarata, Robert Huben, Aidan Ewart, Nicholas Sofroniew, Anna Golubeva, Bruno Olshausen for their detailed comments and feedback, which greatly improved our work.

We are also grateful to our colleagues at Anthropic who generously provided feedback, and many of whom also helped us explore the features discovered by dictionary learning as part of a "Feature Party". We particularly thank Oliver Rausch, Pujaa Rajan, Anna Chen, Alexander Silverstein, Marat Freytsis, Maryam Mortazavi, Mike Lambert, Justin Spahr-Summers, Mike Lambert, Justin Spahr-Summers, Emmanuel Ameisen, Andre Callahan, Shannon Yang, Zachary Witten, Zac Hatfield-Dodds, Karina Nguyen, Avital Balwit, Amanda Askell, Brayden McLean, and Nicholas Scheifer.

Our work is only possible because of the extensive support of all our colleagues at Anthropic, from infrastructure, to engineering, to operations and more. It's impossible for us to list all the people whose work indirectly supported this paper, because there are so many, but we're deeply grateful for their support.

Citation Information
Please cite as:

Bricken, et al., "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning", Transformer Circuits Thread, 2023.
BibTeX Citation:

    @article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }
Feature Proxies
In order to analyze features, we construct "proxies" for each feature which can be easily computed. Our proxies are the log-likelihood ratio of a string under the feature hypothesis and under the full empirical distribution. For example,  
log
(
P
(
s
∣
base64
)
/
P
(
s
)
)
log(P(s∣base64)/P(s)) or 
log
(
P
(
s
∣
Arabic Script
)
/
P
(
s
)
)
log(P(s∣Arabic Script)/P(s)). We use log-likelihood proxies on the intuition that features, since they linearly interact with the logits, will be incentivized to track log-likelihoods.

Since the activation of a feature at a token may be in part due to the preceding tokens (for example the Arabic feature could fire more strongly towards the end of a long Arabic string), we maximize the log-likelihood ratio over different prefixes leading up to and including the final token.

But how can we estimate the different terms? Let's consider each part separately.

Estimating 
P
(
s
)
P(s)
To compute 
P
(
s
)
P(s), we use the unigram distribution over tokens and compute 
P
(
s
)
=
∏
t
∈
s
p
(
t
)
P(s)=∏ 
t∈s
​
 p(t).

Estimating 
P
(
s
∣
base64
)
P(s∣base64) (and other characters features)
For base64, we assume that we're modeling a random base64 string where the probability of a character 
c
c is 
P
(
c
)
=
1
/
6
4
P(c)=1/64 if it's in a base64 character, and 
1
0
−
1
0
10 
−10
  otherwise. We then compute 
P
(
s
)
=
∏
c
∈
s
P
(
c
)
P(s)=∏ 
c∈s
​
 P(c).

We model DNA as a random string of [ATCG], each with probability 1/4.

(Although we don't use them in this draft, we also found this approach helpful for hexadecimal, binary, and similar features.)

Estimating 
P
(
s
∣
Arabic
)
P(s∣Arabic) (and other scripts features)
We also construct proxies for languages written in scripts that use distinctive unicode blocks. We'll use Arabic as an example of this.

For 
P
(
s
∣
Arabic
)
P(s∣Arabic), we use Bayes rule (i.e. 
P
(
s
∣
Arabic
)
=
P
(
Arabic
∣
s
)
⋅
P
(
s
)
/
P
(
Arabic
)
P(s∣Arabic)=P(Arabic∣s)⋅P(s)/P(Arabic)). For 
P
(
s
)
P(s), we use the method discussed above. For the other terms, we exploit the fact that it is easy to detect whether a character is in a Unicode block associated with Arabic script (including e.g. U+0600–U+06FF, U+0750–U+077F). Note this will count common characters shared between languages, like punctuation, as non-Arabic characters. If a string s consists entirely of Arabic characters, 
P
(
A
r
a
b
i
c
∣
s
)
P(Arabic∣s) is 1; if it contains non-Arabic characters, 
P
(
Arabic
∣
s
)
P(Arabic∣s) is assigned a tiny probability of 1e−10. (Keep in mind that we will be maximizing over multiple prefix strings.) If 
s
s consists of a single token which isn't a complete unicode character, we sample random occurrences of it and observe the fraction of the time it is used in a given script. We also estimate 
P
(
Arabic
)
P(Arabic) by applying this heuristic to a random sample of the dataset.

Feature Ablations
We perform feature ablations by running the model on an entire context up through the MLP layer, running the autoencoder to compute feature activations, subtracting the feature direction times its activation from the MLP activation on each token in the context (replacing 
x
j
x 
j
  with 
x
j
−
f
i
(
x
j
)
d
j
x 
j
 −f 
i
​
 (x 
j
 )d 
j
​
 ) and then completing the forward pass. We record the resulting change in the predicted log-likelihood of each token in the context in the color of an underline of that token. Thus if a feature were active on token [B] in the sequence [A][B][C], and ablating that feature reduced the odds placed on the prediction of C, then there would be an orange background on [B] (the activation) and a blue underline on [C] (the ablation effect), indicating that ablating that feature increased the model’s loss on the prediction of [C] and hence that feature is responsible for improving the model’s ability to predict [C] in that context.

Feature Interpretability Rubric
Our rubric for scoring how interpretable features are has the following instructions:

Form an interpretation of the feature based on the positive logits and feature activations across all intervals.
On a scale of 0–3, rate your confidence in this interpretation.
On a scale of 0–5, rate how consistent the high-activation (bolded) tokens are with your interpretation from (1).
On a scale of 0–3, rate how consistent the positive logit effects are with your interpretation from (1).
If some of the positive logit effects were inconsistent with your interpretation, was there a separation in effect size between the consistent and inconsistent ones? If so, score as 1, otherwise or if not applicable, score as 0.
On a scale of 0–3, how specific is your interpretation of this feature?
The total score for a feature’s interpretability is the sum of ratings across these steps. Note that the maximum score is 14, because a perfect score on item #4 implies that item #5 is not applicable.

Feature Density Histograms
While iterating on different techniques, one important proxy for autoencoder performance is feature density. Each feature in our autoencoder only activates on a very small percentage of the total tokens in the training set. We define the feature density of each feature as the fraction of tokens on which the feature has a nonzero value.

We hypothesize that language models contain a large number of features across a distribution of feature densities, and that lower-density features are harder for our autoencoder to discover because they appear less often in the training dataset. Using large training datasets was an attempt to recover such low-density features.

As one way to measure the performance of an autoencoder, we take all the feature densities and plot a histogram of their distribution on a log scale. Roughly, we look for two histogram metrics: the number of features we’ve recovered, and the minimum feature density among features that we’ve recovered. Anecdotally, we find that these metrics are a decent proxy for subjective autoencoder performance and useful for quick iteration cycles.

Ideally, the autoencoder neurons would be either meaningful features or completely dead. Indeed, training does completely leave some autoencoder neurons unused. Many neurons, however, seem to fall into a third category, which we tentatively name the ultralow density cluster.

Many of the feature density histograms are bimodal. For example, consider the feature density histogram for A/16:


This histogram has two main modes: a left mode around 
1
0
−
7
10 
−7
  (corresponding to the ultralow density cluster) and a right mode around 
1
0
−
5
10 
−5
 . 42 We call the right mode the “high density cluster”, although the absolute magnitude of the density is very low. Anecdotally, almost all of the features in the high density cluster are interpretable, but almost none of the features in the ultralow density cluster are. As an aside, we suspect that this bimodality in feature density might partially explain the bimodality in cross-dictionary MCS observed by Huben 
[13]
.

In many preliminary small-scale experiments, the two clusters are often perfectly separable. However, for runs with a large number of autoencoder neurons and a large L1 coefficient these two clusters often overlap.


Even in these cases, the two clusters can be separated by combining other statistics. For each feature, in addition to density, we plot the bias and the dot product of the vectors in the decoder and the encoder corresponding to each feature. Along these three dimensions, the high density cluster is clearly separable.


The ultralow density cluster appears to be an artifact of the autoencoder training process and not a real property of the underlying transformer. We are continuing to investigate the source of this phenomenon and other ways to either mitigate these features or to robustly automatically detect and filter them.

Below, we present feature density histograms for all Seed A runs, grouped by L1 coefficient. As we increase the number of autoencoder neurons (or “N learned sparse”), we continue to discover not only more features but also rarer features, suggesting that we have not exhausted the features in our one-layer transformer.


Transformer Training and Preprocessing
The one-layer transformers we study are trained on the Pile 
[19]
. We chose to train these models on the Pile over Anthropic's internal dataset in order to make our experiments more reproducible. While we think many features are universal, others are very likely idiosyncratic to the dataset.

We train the transformers on 100 billion tokens using the Adam optimizer
[73]
 . We hypothesize that a very high number of training tokens may allow our model to learn cleaner representations in superposition. These transformers have a residual stream dimension of 128, and an inner MLP dimension of 512. The MLP activation is a ReLU.

Advice for Training Sparse Autoencoders: Autoencoder Dataset and Basic Training
To create the dataset for autoencoder training, we evaluate the transformers on 40 million contexts from the Pile and collect the MLP activation vectors after the ReLU for each token within each context. We then sample activation vectors from 250 tokens in each context and shuffle these together so that samples within a batch come from diverse contexts.

For training, we sample MLP activation vectors without replacement using a batch size of 8192. We’ve found that sampling without replacement (i.e., not repeating data) is important for optimal results. We perform 1 million update steps, using just over 8 billion activation vectors out of the total dataset size of 10 billion. We use an Adam optimizer 
[73]
 to minimize the sum of mean squared error loss and an L1 regularization penalty on the hidden layer activations of the autoencoder.

Advice for Training Sparse Autoencoders: Autoencoder Architecture
Our dictionary learning model is a one hidden layer MLP. It is trained as an autoencoder, using the input weights as an encoder and output weights as the decoder. The hidden layer is much wider than the inputs and applies a ReLU non-linearity. We use the default Pytorch Kaiming Uniform initialization 
[74]
.

Formally, let 
n
n be the input and output dimension and 
m
m be the autoencoder hidden layer dimension. Given encoder weights 
W
e
∈
R
m
×
n
W 
e
​
 ∈R 
m×n
 , decoder weights 
W
d
∈
R
n
×
m
W 
d
​
 ∈R 
n×m
  with columns of unit norm, and biases 
b
e
∈
R
m
,
b
d
∈
R
n
b 
e
​
 ∈R 
m
 ,b 
d
​
 ∈R 
n
 , the operations and loss function over a dataset 
X
X are:

x
¯
=
x
−
b
d
f
=
ReLU
(
W
e
x
¯
+
b
e
)
x
^
=
W
d
f
+
b
d
L
=
1
∣
X
∣
∑
x
∈
X
∣
∣
x
−
x
^
∣
∣
2
2
+
λ
∣
∣
f
∣
∣
1
x
¯
 
f
x
^
 
L
​
  
=x−b 
d
​
 
=ReLU(W 
e
​
  
x
¯
 +b 
e
​
 )
=W 
d
​
 f+b 
d
​
 
= 
∣X∣
1
​
  
x∈X
∑
​
 ∣∣x− 
x
^
 ∣∣ 
2
2
​
 +λ∣∣f∣∣ 
1
​
 
​
 

Note that our hidden layer is overcomplete 
m
≥
n
m≥n, and the MSE is a mean over each vector element while the L1 penalty is a sum (with 
λ
λ being the L1 coefficient). The hidden layer activations 
f
f are our learned features. As shown, we subtract the decoder bias from the inputs, and call this a pre-encoder bias. Recall that the decoder is also referred to as the “dictionary” while the encoder can be thought of as a linear, amortized approximation to more powerful sparse coding algorithms.

Our modifications to a standard autoencoder are backed by theory and empirical ablations. However, because these modifications appeared at different periods during our research and can interact with each other, we do not provide rigorous ablation experiments and instead provide intuition and anecdotal evidence for why each is justified. There are likely many important modifications that could further improve performance.

Here is a summary of the modifications sorted both between and within sections in rough order of importance

Pre encoder bias – this boosted performance in toy models.
Decoder weights not tied – the learned encoder weights are often far from the corresponding decoder weights and this is important for increasing model capacity.
Neuron resampling – helps finding more features and achieve a lower total loss.
Many training steps – beyond when the training loss looks to have plateaued.
Learning rate sweep – lower learning rate results in more real features.
Interaction Between Adam and Decoder Normalization – a principled way to account for how the dictionary vector L2 normalization interacts with Adam, resulting in lower total loss.
Pre-Encoder Bias
Testing the autoencoder on toy problems with a bias, we found that it would fail to produce the “bounce plots” we described previously unless we added a learned bias term to both the decoder output and the encoder input. We constrain the pre-encoder bias to equal the negative of the post-decoder bias and initialize it to the geometric median of the dataset. These investigations were motivated by the observations by Hobbhahn 
[60]
.

Decoder Weights Not Tied
It is common to tie the encoder and decoder weights of single hidden layer autoencoders 
[17, 63]
. However, we find that in our trained models the learned encoder weights are not the transpose of the decoder weights and are cleverly offset to increase representational capacity. Specifically, we find that similar features which have closely related dictionary vectors have encoder weights that are offset so that they prevent crosstalk between the noisy feature inputs and confusion between the distinct features.

For example, in "Bug" 2: Multiple Features for a Single Context we described 3 features handling different types of base64 strings. These all have similar dictionary vectors, but it turns out that the encoder weight vectors, while still similar, are more tilted away to help distinguish them.

One way of viewing this is that our autoencoder is learning an amortized, linear approximation to a multi-step, non linear sparse coding algorithm. Allowing the weights of the encoder to be independent from the decoder enables more representational capacity as we empirically observe.

Neuron Resampling
Over the course of training, a subset of autoencoder neurons will have zero activity across a large number of datapoints. We find that “resampling” these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster, see Feature Density Histograms) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis 
[75]
 and increase the number of chances the network has to find promising feature directions.

An interesting nuance around dead neurons involves the ultralow density cluster. We find that if we increase the number of training steps then networks will kill off more of these ultralow density neurons. This reinforces the use of the high density cluster as a useful metric because there can exist neurons that are de facto dead but will not appear to be when looking at the number of dead neurons alone.

The number of dead neurons that appear over training appears to depend upon a number of factors including but not limited to: learning rate (too high); batch size (too low); dataset redundancy (too many tokens per context or repeated epochs over the same dataset); number of training steps (too many); optimizer used. Our results here agree with those of Bricken et al. 
[72]
.

Better resampling strategies is an active research area of ours. The approach used in this work is as follows:

At training steps 25,000, 50,000, 75,000 and 100,000, identify which neurons have not fired in any of the previous 12,500 training steps.
Compute the loss for the current model on a random subset of 819,200 inputs.
Assign each input vector a probability of being picked that is proportional to the square of the autoencoder’s loss on that input.
For each dead neuron sample an input according to these probabilities. Renormalize the input vector to have unit L2 norm and set this to be the dictionary vector for the dead autoencoder neuron.
For the corresponding encoder vector, renormalize the input vector to equal the average norm of the encoder weights for alive neurons × 0.2. Set the corresponding encoder bias element to zero.
Reset the Adam optimizer parameters for every modified weight and bias term.
This resampling procedure is designed to seed new features to fit inputs where the current autoencoder performs worst. Resetting the encoder norm and bias are crucial to ensuring this resampled neuron will only fire weakly for inputs similar to the one used for its reinitialization. We do this to minimize interference with the rest of the network.

This resampling approach outperforms baselines including no resampling and reinitializing the relevant encoder and decoder weights using a default Kaiming Uniform initialization. However, there are likely better resampling methods to be developed – this approach still causes sudden loss spikes, and resampling too frequently causes training to diverge.

Learning Rate Sweep
We performed several training runs using multiple learning rates while also varying the number of training steps and found that lower learning rates, when given sufficient training steps, result in lower total loss and more “real” features discovered (as indicated by the Feature Density Histograms). We also tried annealing our learning rate over the course of training but found that this did not further increase performance.

Interaction Between Adam and Decoder Normalization
Recall that we constrain our dictionary vectors to have unit norm. Our first naive implementation simply reset all vectors to unit norm after each gradient step. This means any gradient updates modifying the length of our vector are removed, creating a discrepancy between the gradient used by the Adam optimizer and the true gradient. We find that instead removing any gradient information parallel to our dictionary vectors before applying the gradient step results in a small but real reduction in total loss.

Advice for Training Sparse Autoencoders: Hyperparameter Selection
The ultimate goal of dictionary learning is to produce features that are interpretable and also accurately represent the underlying model. Quantitatively measuring interpretability is currently difficult and slow, so we frequently look at proxy metrics when testing changes or optimizing hyperparameters. Our methods for optimizing hyperparameters are far from perfect, but we want to share insights that may be useful for other researchers doing similar work.

We most frequently look at the following proxy metrics:        

Training loss: This is the simplest metric, and useful for testing changes such as optimizers or learning rates. Unfortunately, it isn’t meaningful to compare training loss across hyperparameters that change the loss function, such as L1 coefficients.
Feature Density Histograms: Specific metrics from these histograms include:
The number of alive features outside of the ultralow density cluster
The minimum feature density at which we see a significant number of non-ultralow-density-cluster features.
The number of features with density above 1%. A significant number of features above this level seems to correspond to an L1 coefficient that is too low.
L
0
L 
0
  norm: The average number of nonzero entries in a sparse representation. We don’t yet know of a principled way to determine a target level of sparsity, but we generally target a 
L
0
L 
0
  norm that is less than 10 or 20. We especially distrust solutions where the 
L
0
L 
0
  norm is a significant fraction of the transformer’s activation dimensionality.
Reconstructed Transformer NLL: We would like the features we discover to explain almost all of the behavior of the underlying transformer. One way to measure this is to take a transformer, run the MLP activations through our autoencoder, replace the MLP activations with the autoencoder predictions, measure the loss on the training dataset, and calculate the difference in loss.
We often normalize this by dividing by the difference in loss between the baseline transformer’s performance and its performance after ablating the MLP layer. This gives us a fraction of the MLP’s loss contribution that is explained by our transformer. However, the performance with an abated MLP may be an especially bad baseline, so this percentage is considered an overestimate. Ad hoc experiments (data not shown) show that similarly high percentages can be reached by training a transformer from scratch with a small MLP.

The 
L
0
L 
0
  norm and Reconstructed Transformer NLL metrics, taken together, display a human-interpretable tradeoff between sparsity and explained behavior:


Each line on this plot shows data for a single L1 coefficient and varying autoencoder sizes. The lowest L1 coefficient is excluded to create a reasonable scale.

Activation Visualization
Visualizations were produced using a 100 million token subset of our training dataset where only 10 tokens are taken from every context. We display a window of four tokens on each side of the selected token for visualization.


Feature UMAPs
We construct two UMAP embeddings of learned features from our transformer. We represent each feature as a vector of dimension 512 (the number of neurons), its column in the decoder matrix 
W
d
W 
d
​
 . The first UMAP embeds the features from A/0 and A/1 jointly into 2 dimensions, and uses hyperparameters n_neighbors=15, metric="cosine", min_dist=0.05. The second UMAP uses the same hyperparameters, but with min_dist=0.01, and also includes the features from A/2. We additionally organize the points in the first UMAP into clusters by applying HDBSCAN with min_cluster_size=3 to a 10-dimensional UMAP embedding of the same features, fit with hyperparameters n_neighbors=15, metric="cosine", min_dist=0.1. The list of features is sorted by a one-dimensional UMAP fit to the unclustered features and the cluster mediods.

Automated Interpretability
Here we provide more details on the implementation of our automated interpretability before explaining importance scoring and providing additional results.

Experimental Setup
In order to get feature explanations Claude 2 is provided with a total of 49 examples: ten examples from the top activations interval; two from the other 12 intervals; five completely random examples; and ten examples where the top activating tokens appear in different contexts. 43 For example, in the main text we discuss a feature that fires for token "the" in the context of machine learning, the ten examples here would contain the word "the" but across different contexts. We also provide the top and bottom logits and labels for what interval each example came from. In addition, we give eight human demonstrations on held out features that include chain of thought reasoning. Following Bills et al. 
[45]
 all of our activations are quantized to be between 0-9 inclusive. Finally, we ask the model to be succinct in its answer and not provide specific examples of tokens it activates for.

Using the explanation generated, in a new interaction Claude is asked to predict activations for sixty examples: six from the top activations; two from the other 12 intervals; ten completely random; and twenty top activating tokens out of context. The same eight held out demonstration features show how each token should elicit a predicted activation. All of the examples are shuffled, the logits and interval each came from is removed, and blanks are left where the model should fill in its activation predictions. For the sake of computational efficiency, Claude scores all sixty examples in a single shot, repeating each token followed by its predicted activation. In an ideal setting, each example would be given independently as its own prompt.

Note that there are instances where a feature has less than 49 explanation examples and 60 prediction examples because we remove any repeated examples that appear in the feature visualization intervals. However, this occurs for a negligible number of features and we drop any of those with fewer than 20 examples in total.

For the sake of comparison to Bills et al. 
[45]
 the figure below includes Pearson correlation in addition to Spearman that is otherwise used throughout the text. The distributions and overall conclusions are almost identical to Spearman.


We also systematically catalog the feature output weight logits with Claude. We use the same explanation Claude generated to predict activations and provide 100 logits, 50 taken randomly and 50 which are the 10th–60th largest positive logits (the top 10 were used to generate the original explanations and thus held out). Claude was asked to output a binary label for if this logit would plausibly come after the feature fired, given the explanation. Note that the chance performance is 50%. The A/1 features are quite a bit better than the neurons at a median of 0.74 versus 0.53 for the neurons and 0.5 for the randomized transformer.

Randomized Transformer
As a further ablation, we compare our human and automated interpretability approaches to the randomized transformer. It is unsurprising that the logit weight predictions are centered around random chance for the randomized transformer by the very nature of its random weights. Manual interpretability also favors the A/1 features. However, for automated interpretability the randomized features have a higher median score.

Iit turns out that the randomized transformer features are distinctly bimodal, consisting of single token and polysemantic clusters which are separated out in the inset at the bottom of the following figure.


Features are labeled as being "single token" if for all 20 examples in the top activation interval, the token with the largest activation is the same. The single token features of the randomized transformer are not only more numerous but also more truly single token, for example, firing for the same token even in the weakest activation intervals. This makes them more trivial to score as evidenced by the high scoring yellow peak (bottom row left side of the figure).

Meanwhile, for the not "single token" cluster, the A/1 features score higher while the randomized features in fact have a similar distribution to that of the polysemantic neurons.

Note that this bimodal clustering of the randomized transformer features is also supported by the manual human analysis where there is both a cluster at 0, the same score given to the polysemantic neurons in the main text, and a non zero cluster. The non-zero cluster scores worse than the A/1 Features here (unlike for the automated approach) because part of the rubric accounts for the logit interpretability, which scores zero in the shuffled case.

The fact that the randomized transformer learns more highly single token features fits with the hypothesis that dictionary learning will learn the features of the dataset rather than the representations of the transformer.

Importance Scoring
Importance scoring is the principled way to correct for the fact that we did not randomly sample all of the examples our model is asked to score. Formally, we assign to all tokens from random examples a weight of 1 and examples from each feature interval a weight of feature_density*interval_probability. feature_density is the fraction of times this feature fires over a large portion of the dataset and interval_probability converts the distribution of non-zero activations into a categorical distribution corresponding to each interval and uses the corresponding probability. We then use these weights in our Spearman correlation as before. 44

The issue with importance scoring is that because our features have densities typically in the range of 1e-3 to 1e-6  (see next Appendix section), almost all of the weight is assigned to the random examples for which features almost never fire. As a result, it is possible for features to score well by almost always predicting zero and in turn favors explanations that are overly restrictive that say what the feature does not fire for, instead of what it does fire for. Conversely, explanations are highly penalized for even a small false positive rate, since the true rate of occurrences is so low.

The figure below shows importance scores for features and neurons. Note that many of the features now have scores very close to zero while the neurons are relatively less affected. We hypothesize this is because the neurons, in being more polysemantic, have more non-zero activations that test their explanations on both things it does and doesn't fire for. Empirically, taking an average over features, 88% of their true activations from the random examples are zero versus 54% for the neurons (the medians are ~91% and 58%, respectively).


Explanation Lengths Caveat
Claude can at times fail to wrap its explanation in the <answer></answer> explanation tags resulting in the full chain of thought being used instead. This can be much longer and leak more specific words that the feature fires for.

Here we plot the number of characters in each explanation where the second mode to the right highlights these instances. However, not only does this happen infrequently, it also occurs across the features, neurons, and randomized transformer results. In fact, it happens the most often for neurons, occurring 19% of the time versus 15% for the randomized transformer and 13% for the features. Most importantly, it does result in significantly different scores when we split out the performance of those with long explanations from short ones.


To conclude, while automated interpretability is a difficult task for models that we have only begun working on, it has already been very useful for quickly understanding dictionary learning features in a scalable fashion. We encourage readers to explore the visualizations with automated interpretability scores: A/1, A/0, randomized model features, neurons.

Footnotes
For more discussion of this point, see Distributed Representations: Composition and Superposition.[↩]
We'd particularly highlight an exciting exchange between Li et al.  
[34]
 and Nanda et al. 
[31]
: Li et al. found an apparent counterexample where features were not represented as directions, which was the resolved by Nanda finding an alternative interpretation in which features were directions.[↩]
While not the focus of this paper, we could also imagine decomposing other portions of the model’s latent state or activations in this way. For instance, we could apply this decomposition to the residual stream (see e.g. 
[10, 15]
), or to the keys and queries of attention heads, or to their outputs.[↩]
This property is closely related to the desiderata of Causality, Generality, and Purity discussed in Cammarata et al. 
[25]
, and those provide an example of how we might make this property concrete in a specific instance.[↩]
This is similar in spirit to the evidence provided by influence functions.[↩]
For the model discussed in this paper, we trained the autoencoder on 8 billion datapoints.[↩]
We tie the biases applied in the input and output, so the result is equivalent to subtracting a fixed bias from all activations and then using an autoencoder whose only bias is before the encoder activation.[↩]
Note that using an MSE loss avoids the challenges with polysemanticity that we discussed above in Why Not Architectural Approaches?[↩]
Note that this linear structure makes it even more likely that features should be linear. On the one hand, this means that the linear representation hypothesis is more likely to hold for this model. On the other hand, it potentially means that our results are less likely to generalize to multilayer models. Fortunately, others have studied multilayer transformers with sparse autoencoders and found interpretable linear features, which gives us more confidence that what we see in the one-layer model indeed generalizes 
[14, 15, 16]
.[↩]
For example, one neuron in our transformer model responds to a mix of academic citations, English dialogue, HTTP requests, and Korean text. In vision models, there is a classic example of a neuron which responds to cat faces and fronts of cars 
[1]
.[↩]
This kind of split tokenization is common for Unicode characters outside the Latin script and the most common characters from other Unicode blocks.[↩]
For example, in the figure above, there is a newline character ⏎ that our feature fires on but our proxy assigns a low score to because it is outside the Unicode block.[↩]
Why do we believe large activations have larger effects? In the case of a one-layer transformer like the one we consider in this paper, we can make a strong case for this: features have a linear effect on the logits (modulo rescaling by layer norm), and so a larger activation of the feature has a larger effect on the logits. In the case of larger models, this follows from a lot of conjectures and heuristic arguments (e.g. the abundance of linear pathways in the model and the idea of linear features at each layer), and must be true for sufficiently small activations by continuity, but doesn't have a watertight argument.[↩]
(also called “logit attribution”, see similar work e.g. 
[42]
)[↩]
We exclude weights corresponding to extremely rare or never used vocabulary elements. These are perhaps similar to the "anomalous tokens" (e.g., "SolidGoldMagikarp") of Rumbelow & Watkins 
[43]
.[↩]
 There are in theory several ways the logit weights could overestimate the model's actual use of a feature:
1. It could be that these output weights are small enough that, when multiplied by activations, they don't have an appreciable effect on the model’s output.
2. The feature might only activate in situations where other features make these tokens extremely unlikely, such the feature in fact has little effect.
3. It is possible that our approximation of linearizing the layer norm (see Framework 
[18]
) is poor.
Based on the subsequent analysis, which confirms the logit weight effects, we do not believe these issues arise in practice.[↩]
Of course, some features might genuinely be neuron aligned. But we'd like to know that at least some of the features dictionary learning discovers were not trivial.[↩]
We also tried looking at the neurons which have the largest contribution to the dictionary vector for the feature. However, we found looking at the most correlated neuron to be more reliable – in earlier experiments, we found rare cases where the correlated method found a seemingly similar neuron, while the dictionary method did not. This may be because neurons can have different scales of activations.[↩]
This makes sense as a superposition strategy: since languages are essentially mutually exclusive, they're natural to put in superposition with each other
[44]
[↩]
By analogy, this also applies to B/1/1334.[↩]
"Activation correlation" is defined as the feature whose activations across 40,960,000 tokens has the highest Pearson correlation with those of A/1/3450.[↩]
In order to sample uniformly across the spectrum of feature activations, we divide the activation spectrum into 11 "activation intervals" evenly spaced between 0 activation and the maximum activation. We sample uniformly from these intervals.[↩]
It's worth explicitly stating that our automated interpretability setup was designed to ensure that there's no leak of information about activation patterns, except for the explanation. For example, when predicting new activations, the model cannot see any true activations of that feature.[↩]
This is distinct from the evaluation strategy of Bills et al., who calculated their correlation of predicted and true activations on a mixture of maximal dataset examples and random samples. For sparse features, which don't fire on most random samples, this effectively tests the model's ability to distinguish a feature's large activations from zero.[↩]
In instances where Claude predicts a constant score, most often all 0s, a correlation can't be computed and we assign a score of zero which explains the uptick there.[↩]
with respect to the effect it has on the model outputs see e.g. the activation expected value plot in Arabic Feature's Activations Specificity Analysis.[↩]
Naively, simulating a layer with 100k features would be 100,000 times more expensive than sampling a large language model such as Claude 2 or GPT-4 (we'd need to sample the explaining large model once for every feature at every token). At current prices, this would suggest simulating 100k features on a single 4096 token context would cost $12,500–$25,000, and one would presumably need to evaluate over many contexts.[↩]
We generate a model with random weights by randomly shuffling the entries of each weight matrix of the trained transformer used in Run A. This guarantees that the distributions of individual weights match, and differences are due to structure.[↩]
From a purely theoretical lens, attention heads can largely implement "three point functions" (with two inputs, and an output). MLP layers are well positioned to instead implement N-token conjunctions, perhaps the most extreme of which are context features or token-in-context features. Thus, it is perhaps natural that we see many of these.[↩]
Token-in-context features may offer a significant opportunity for simplifying analysis of the model – as Elhage et al. 
[38]
 note, it may be possible to understand these features as two-dimensional family of features parameterized by a context and a token.[↩]
Toy Models considered correlated features (see in particular organization of correlated features and collapsing of correlated features), but only features which were correlated in whether they were active (and not their value if active), and had nothing analogous to the similar "output actions" described here. Nonetheless, Toy Models' experiments may be a useful intuition pump, especially in noticing the distinction between similar features in superposition vs features collapsing into a single broader feature.[↩]
For each pair of features, we compute the cosine similarity between their activations on the subset of tokens for which one of them fires. We repeat this, restricting to the subset on which the other fires, and take the greater of the two. We draw a connection between the features if this measure exceeds ~0.4. This threshold was chosen to balance producing a small enough graph to visualize while also showing some of the richness of feature splitting. We omit the ultralow density features from this analysis.[↩]
The features fire on the token " P" of words where it appears as the first token, such as [ P][attern].[↩]
In this instance we found the refined P features by manual inspection rather than by cosine similarity.[↩]
Our initial clue that A/1/1544 might fire on base64 strings encoding ASCII text was the token ICAgICAg which this feature particularly responds to, and corresponds to six spaces in a row.[↩]
Determining when a dataset example encodes ASCII text is somewhat subtle because base64 can only be decoded to ASCII in groups of four characters, since four base64 characters encode triples of ASCII characters. Thus, we select substrings which – when decoded with the python base64 library – contain the maximal number of printable ASCII characters.[↩]
In what sense does universality suggest features are "real"? One basic observation is that they suggest the features we're finding are not just artifacts of the dictionary learning process – or at least that if they come from the dictionary learning process, it's in some consistent way. But there are also several deeper ways in which it's suggestive. It means that, whatever the source of the features, we can talk about features as replicable, reliable, recurring units of analysis. It's also just a surprising observation that one would expect if a strong version of the features in superposition hypothesis was true and models were literally representing some finite, discrete set of features in superposition.[↩]
In fact, we found some features so universal that we began to take it for granted as a basic tool in our workflow of evaluating dictionary learning runs. For example, the base64 feature – which we previously observed in SoLU models – was so consistently universal that its presence was a useful debugging heuristic.[↩]
The feature is bimodal, monosemantic in the larger of the modes, and fires on 0.02% of tokens. This means that at least 1 in 10,000 tokens in the Pile dataset are abbreviations for PLoS journals in citations! This is an example of how inspecting features can reveal properties of the dataset, in this case the strong bias of the Pile towards scientific content.[↩]
Suppose feature 
f
i
f 
i
​
  has logit weights 
v
i
k
v 
ik
​
  for 
k
∈
{
1
,
…
,
n
vocab
}
k∈{1,…,n 
vocab
​
 }. At a given token 
t
j
t 
j
​
 , we compute the activation of the feature 
f
i
(
t
j
)
f 
i
​
 (t 
j
​
 ) and multiply it by the logit weight 
v
i
t
j
+
1
v 
it 
j+1
​
 
​
  of the token 
t
j
+
1
t 
j+1
​
  that comes next to get an attribution score of 
f
i
(
t
j
)
v
i
t
j
+
1
f 
i
​
 (t 
j
​
 )v 
it 
j+1
​
 
​
 . The attribution vector is given by stacking the attribution scores for a random sampling of datapoints. This approximates the classic attribution method of multiplying the gradient by the activation, differing in that we ignore the denominators of the softmax and the layer norm.[↩]
The "finite state automata"-esque feature assemblies are also different from circuits in that the model didn't learn them to work together. Rather, in the course of learning to autoregressively model text, it learned features that interact via the token stream because of patterns in the real datasets. In contrast, a language model trained with reinforcement learning might have systems like this – circuits whose feature components interact via generated tokens – which co-evolved and adapted to work together during RL training.[↩]
The right mode in this example may actually be split into two modes, one around 
1
0
−
5
10 
−5
  and another around 
1
0
−
3
10 
−3
 . We are still investigating the nature of this split.[↩]
We are explicit in keeping all of these examples separate from the ones used when we test on activation predictions.[↩]
The top activating tokens out of context examples inherit the weighting of the top activating examples quantile.[↩]
References
Feature Visualization  [link]
Olah, C., Mordvintsev, A. and Schubert, L., 2017. Distill. DOI: 10.23915/distill.00007
Linear algebraic structure of word senses, with applications to polysemy
Arora, S., Li, Y., Liang, Y., Ma, T. and Risteski, A., 2018. Transactions of the Association for Computational Linguistics, Vol 6, pp. 483--495. MIT Press.
Decoding The Thought Vector  [link]
Goh, G., 2016.
Zoom In: An Introduction to Circuits
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S., 2020. Distill. DOI: 10.23915/distill.00024.001
Toy Models of Superposition
Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M. and Olah, C., 2022. Transformer Circuits Thread.
Sparse overcomplete word vector representations
Faruqui, M., Tsvetkov, Y., Yogatama, D., Dyer, C. and Smith, N., 2015. arXiv preprint arXiv:1506.02004.
Spine: Sparse interpretable neural embeddings
Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T. and Hovy, E., 2018. Proceedings of the AAAI Conference on Artificial Intelligence, Vol 32(1).
Word embedding visualization via dictionary learning
Zhang, J., Chen, Y., Cheung, B. and Olshausen, B.A., 2019. arXiv preprint arXiv:1910.03833.
Word2Sense: sparse interpretable word embeddings
Panigrahi, A., Simhadri, H.V. and Bhattacharyya, C., 2019. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5692--5705.
Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors
Yun, Z., Chen, Y., Olshausen, B.A. and LeCun, Y., 2021. arXiv preprint arXiv:2103.15949.
[Interim research report] Taking features out of superposition with sparse autoencoders  [link]
Sharkey, L., Braun, D. and Millidge, B., 2022.
[Replication] Conjecture's Sparse Coding in Toy Models  [link]
Cunningham, H. and Smith, L., 2023.
[Research Update] Sparse Autoencoder features are bimodal  [link]
Huben, R., 2023.
(tentatively) Found 600+ Monosemantic Features in a Small LM Using Sparse Autoencoders  [link]
Smith, L., 2023.
Really Strong Features Found in Residual Stream  [link]
Smith, L., 2023.
AutoInterpretation Finds Sparse Coding Beats Alternatives  [link]
Cunningham, H., 2023.
Sparse Autoencoders Find Highly Interpretable Model Directions  [PDF]
Cunningham, H., Ewart, A., Smith, L., Huben, R. and Sharkey, L., 2023. arXiv preprint arXiv:2309.08600.
A Mathematical Framework for Transformer Circuits  [HTML]
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S. and Olah, C., 2021. Transformer Circuits Thread.
The Pile: An 800GB Dataset of Diverse Text for Language Modeling
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S. and Leahy, C., 2020.
Linguistic regularities in continuous space word representations  [PDF]
Mikolov, T., Yih, W. and Zweig, G., 2013. Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pp. 746--751.
Linguistic regularities in sparse and explicit word representations
Levy, O. and Goldberg, Y., 2014. Proceedings of the eighteenth conference on computational natural language learning, pp. 171--180.
Unsupervised representation learning with deep convolutional generative adversarial networks
Radford, A., Metz, L. and Chintala, S., 2015. arXiv preprint arXiv:1511.06434.
Visualizing and understanding recurrent networks  [PDF]
Karpathy, A., Johnson, J. and Fei-Fei, L., 2015. arXiv preprint arXiv:1506.02078.
Learning to generate reviews and discovering sentiment  [PDF]
Radford, A., Jozefowicz, R. and Sutskever, I., 2017. arXiv preprint arXiv:1704.01444.
Curve Detectors  [link]
Cammarata, N., Goh, G., Carter, S., Schubert, L., Petrov, M. and Olah, C., 2020. Distill.
Object detectors emerge in deep scene cnns  [PDF]
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. and Torralba, A., 2014. arXiv preprint arXiv:1412.6856.
Network Dissection: Quantifying Interpretability of Deep Visual Representations  [PDF]
Bau, D., Zhou, B., Khosla, A., Oliva, A. and Torralba, A., 2017. Computer Vision and Pattern Recognition.
Understanding the role of individual units in a deep neural network
Bau, D., Zhu, J., Strobelt, H., Lapedriza, A., Zhou, B. and Torralba, A., 2020. Proceedings of the National Academy of Sciences, Vol 117(48), pp. 30071--30078. National Acad Sciences.
On the importance of single directions for generalization  [PDF]
Morcos, A.S., Barrett, D.G., Rabinowitz, N.C. and Botvinick, M., 2018. arXiv preprint arXiv:1803.06959.
On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron
Donnelly, J. and Roegiest, A., 2019. European Conference on Information Retrieval, pp. 795--802.
Emergent Linear Representations in World Models of Self-Supervised Sequence Models
Nanda, N., Lee, A. and Wattenberg, M., 2023. arXiv preprint arXiv:2309.00941.
Discovering latent knowledge in language models without supervision
Burns, C., Ye, H., Klein, D. and Steinhardt, J., 2022. arXiv preprint arXiv:2212.03827.
Acquisition of chess knowledge in alphazero
McGrath, T., Kapishnikov, A., Toma{\v{s}}ev, N., Pearce, A., Wattenberg, M., Hassabis, D., Kim, B., Paquet, U. and Kramnik, V., 2022. Proceedings of the National Academy of Sciences, Vol 119(47), pp. e2206625119. National Acad Sciences.
Emergent world representations: Exploring a sequence model trained on a synthetic task
Li, K., Hopkins, A.K., Bau, D., Viégas, F., Pfister, H. and Wattenberg, M., 2022. arXiv preprint arXiv:2210.13382.
Engineering monosemanticity in toy models
Jermyn, A.S., Schiefer, N. and Hubinger, E., 2022. arXiv preprint arXiv:2211.09169.
Polysemanticity and capacity in neural networks
Scherlis, A., Sachan, K., Jermyn, A.S., Benton, J. and Shlegeris, B., 2022. arXiv preprint arXiv:2210.01892.
Sparse coding with an overcomplete basis set: A strategy employed by V1?
Olshausen, B.A. and Field, D.J., 1997. Vision research, Vol 37(23), pp. 3311--3325. Elsevier.
Softmax Linear Units
Elhage, N., Hume, T., Olsson, C., Nanda, N., Henighan, T., Johnston, S., ElShowk, S., Joseph, N., DasSarma, N., Mann, B., Hernandez, D., Askell, A., Ndousse, K., Jones, A., Drain, D., Chen, A., Bai, Y., Ganguli, D., Lovitt, L., Hatfield-Dodds, Z., Kernion, J., Conerly, T., Kravec, S., Fort, S., Kadavath, S., Jacobson, J., Tran-Johnson, E., Kaplan, J., Clark, J., Brown, T., McCandlish, S., Amodei, D. and Olah, C., 2022. Transformer Circuits Thread.
Sparse and redundant representations: from theory to applications in signal and image processing
Elad, M., 2010. , Vol 2(1). Springer.
Method of optimal directions for frame design
Engan, K., Aase, S.O. and Husoy, J.H., 1999. 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No. 99CH36258), Vol 5, pp. 2443--2446.
K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation
Aharon, M., Elad, M. and Bruckstein, A., 2006. IEEE Transactions on signal processing, Vol 54(11), pp. 4311--4322. IEEE.
Attribution Patching: Activation Patching At Industrial Scale  [link]
Nanda, N., 2023.
SolidGoldMagikarp (plus, prompt generation)  [link]
Rumbelow, J. and Watkins, M., 2023.
Distributed Representations: Composition & Superposition  [HTML]
Olah, C., 2023.
Language models can explain neurons in language models  [HTML]
Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J. and Saunders, W., 2023.
Natural language descriptions of deep visual features
Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A. and Andreas, J., 2021. International Conference on Learning Representations.
An Overview of Early Vision in InceptionV1
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S., 2020. Distill. DOI: 10.23915/distill.00024.002
Multimodal Neurons in Artificial Neural Networks
Goh, G., Cammarata, N., Voss, C., Carter, S., Petrov, M., Schubert, L., Radford, A. and Olah, C., 2021. Distill. DOI: 10.23915/distill.00030
Finding Neurons in a Haystack: Case Studies with Sparse Probing
Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D. and Bertsimas, D., 2023. arXiv preprint arXiv:2305.01610.
Visualizing and measuring the geometry of BERT
Coenen, A., Reif, E., Yuan, A., Kim, B., Pearce, A., Viégas, F. and Wattenberg, M., 2019. Advances in Neural Information Processing Systems, Vol 32.
Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space
Geva, M., Caciularu, A., Wang, K.R. and Goldberg, Y., 2022. arXiv preprint arXiv:2203.14680.
High-Low Frequency Detectors
Schubert, L., Voss, C., Cammarata, N., Goh, G. and Olah, C., 2021. Distill. DOI: 10.23915/distill.00024.005
Convergent learning: Do different neural networks learn the same representations?
Li, Y., Yosinski, J., Clune, J., Lipson, H., Hopcroft, J.E. and others,, 2015. FE@ NIPS, pp. 196--212.
Why does unsupervised pre-training help deep learning?  [link]
Erhan, D., Courville, A., Bengio, Y. and Vincent, P., 2010. Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 201--208.
Visualizing Representations: Deep Learning and Human Beings  [link]
Olah, C., 2015.
SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability  [PDF]
Raghu, M., Gilmer, J., Yosinski, J. and Sohl-Dickstein, J., 2017. Advances in Neural Information Processing Systems 30, pp. 6078--6087. Curran Associates, Inc.
Superposition, Memorization, and Double Descent
Henighan, T.A.C., 2023. Transformer Circuits Thread.
Mechanistic anomaly detection and ELK  [link]
Christiano, P., 2022.
Dropout can create a privileged basis in the ReLU output model  [link]
lsgos,, 2023.
More findings on Memorization and double descent  [link]
Hobbhahn, M., 2023.
Disentangling by factorising
Kim, H. and Mnih, A., 2018. International Conference on Machine Learning, pp. 2649--2658.
Infogan: Interpretable representation learning by information maximizing generative adversarial nets
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I. and Abbeel, P., 2016. Advances in neural information processing systems, Vol 29.
k-Sparse Autoencoders  [link]
Makhzani, A. and Frey, B.J., 2013. CoRR, Vol abs/1312.5663.
Representation learning: A review and new perspectives
Bengio, Y., Courville, A. and Vincent, P., 2013. IEEE transactions on pattern analysis and machine intelligence, Vol 35(8), pp. 1798--1828. IEEE.
Toward transparent ai: A survey on interpreting the inner structures of deep neural networks
Räuker, T., Ho, A., Casper, S. and Hadfield-Menell, D., 2023. 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pp. 464--483.
Promises and pitfalls of black-box concept learning models
Mahinpei, A., Clark, J., Lage, I., Doshi-Velez, F. and Pan, W., 2021. arXiv preprint arXiv:2106.13314.
Local vs. Distributed Coding
Thorpe, S.J., 1989. Intellectica, Vol 8, pp. 3--40.
Scaling laws for neural language models
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and Amodei, D., 2020. arXiv preprint arXiv:2001.08361.
Sparse-Coding Variational Auto-Encoders
Barello, G., Charles, A.S. and Pillow, J.W., 2018. bioRxiv.
On incorporating inductive biases into VAEs
Miao, N., Mathieu, E., Siddharth, N., Teh, Y.W. and Rainforth, T., 2021. arXiv preprint arXiv:2106.13746.
Beyond ℓ₁ sparse coding in V1
Rentzeperis, I., Calatroni, L., Perrinet, L. and Prandi, D., 2023. arXiv preprint arXiv:2301.10002.
Emergence of Sparse Representations from Noise
Bricken, T., Schaeffer, R., Olshausen, B. and Kreiman, G., 2023.
Adam: A method for stochastic optimization
Kingma, D.P. and Ba, J., 2014. arXiv preprint arXiv:1412.6980.
Delving deep into rectifiers: Surpassing human-level performance on imagenet classification
He, K., Zhang, X., Ren, S. and Sun, J., 2015. Proceedings of the IEEE international conference on computer vision, pp. 1026--1034.
The lottery ticket hypothesis: Finding sparse, trainable neural networks
Frankle, J. and Carbin, M., 2018. arXiv preprint arXiv:1803.03635.